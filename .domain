# DOMAIN MODEL ANALYSIS - FP8 K=64 MFMA Multi-Block Kernel

## CURRENT BLOCKER
Code model has created `fwd_fp8_k64_256t.s` with multi-block support, but the LDS addressing uses stride-128 which causes 16x bank conflicts.

**ACTION REQUIRED**: Implement bank-conflict-free LDS addressing using one of:
1. **Stride-132** (simplest) - see "RECOMMENDED IMPLEMENTATION" section below
2. **Diagonal padding** (matches Triton) - see "Triton Diagonal Padding Strategy" section below

The kernel structure is correct. Only the LDS addressing pattern needs fixing.

---

## PROVEN FACT: K=64 FP8 MFMA Thread-to-Data Mapping (ISA Doc)

**CLAIM**: The V_MFMA_F32_32x32x64_F8F6F4 instruction has a specific interleaved thread-to-data layout that differs from K=16.

**EVIDENCE**: CDNA4 ISA doc lines 3153-3175, section "Dense Matrix Layouts: 8-bit and Smaller"

**LAYOUT FOR FP8 32x32x64**:
```
A[32][64] and B[64][32] layout in 8 VGPRs:

         v0-v3 (first 4 VGPRs)           v4-v7 (last 4 VGPRs)
Thread   M/N dimension    k dimension    k dimension
------   -------------    -----------    -----------
0-15     M/N = [0-15]     k = [0-15]     k = [32-47]
16-31    M/N = [16-31]    k = [0-15]     k = [32-47]
32-47    M/N = [0-15]     k = [16-31]    k = [48-63]
48-63    M/N = [16-31]    k = [16-31]    k = [48-63]
```

**FORMULA FOR THREAD t (0-63)**:
- M/N row = (t % 16) + 16 * ((t / 32) % 2)
- k range depends on thread group and VGPR index

---

## PROVEN FACT: K=64 MFMA k-range mapping (verified on gfx950)

**CLAIM**: The actual k-range mapping for FP8 MFMA K=64 on gfx950 differs from the doc hypothesis. The observed mapping for lane groups is:
- Group 0 (lanes 0–15): low regs (v0–v3) → k16–31, high regs (v4–v7) → k0–15
- Group 1 (lanes 16–31): low regs → k48–63, high regs → k32–47
- Group 2 (lanes 32–47): low regs → k16–31, high regs → k0–15
- Group 3 (lanes 48–63): low regs → k48–63, high regs → k32–47

**EVIDENCE**: `hsa/gfx950/fmha_v3_fwd_fp8/test_mfma_map_debug.py` (search over k-range mappings) reports:
```
best mean_err: 0.0 ... mapping: {0: (16, 0), 1: (48, 32), 2: (16, 0), 3: (48, 32)}
```
where mapping value `(low_base, high_base)` corresponds to k ranges for reg0–3 and reg4–7.

---

## PROVEN FACT: Triton XOR Swizzle Pattern for LDS Addressing

**CLAIM**: Triton achieves 1294 TF/s using XOR-based swizzle for bank-conflict-free LDS access with K=64 MFMA.

**EVIDENCE**: triton_fp8_fmha.s lines 159-211

**PATTERN**:
```asm
; Base computation (lines 159-160)
v_bitop3_b32 v4, v5, v11, v4 bitop3:0x36    ; XOR of three values
v_xor_b32_e32 v5, 0x60, v4                  ; +96 offset

; Two ds_read_b128 per operand half (lines 164-165)
ds_read_b128 v[118:121], v5                 ; 16 bytes from addr v5
ds_read_b128 v[114:117], v6                 ; 16 bytes from addr v6

; Additional XOR offsets used (lines 200-211):
v_xor_b32_e32 v11, 32, v144                 ; 32
v_xor_b32_e32 v12, 64, v144                 ; 64
v_xor_b32_e32 v13, 0x60, v144               ; 96
v_xor_b32_e32 v15, 0x460, v10               ; 1120
v_xor_b32_e32 v16, 0x1020, v10              ; 4128
v_xor_b32_e32 v17, 0x1460, v10              ; 5216
```

**KEY INSIGHT**: Triton uses TWO DIFFERENT swizzled addresses for each ds_read_b128 pair that form the 32-byte MFMA operand. The bitop3:0x36 is XOR (A⊕B⊕C).

---

## PROVEN FACT: ds_read_b64_tr_b8 for Transposed V Matrix

**CLAIM**: Triton uses `ds_read_b64_tr_b8` for PV multiplication to read V matrix transposed directly from LDS.

**EVIDENCE**: triton_fp8_fmha.s lines 679-682, 697-700

```asm
ds_read_b64_tr_b8 v[74:75], v145
ds_read_b64_tr_b8 v[76:77], v145 offset:1088
ds_read_b64_tr_b8 v[78:79], v145 offset:4096
ds_read_b64_tr_b8 v[80:81], v145 offset:5184
```

**NOTE**: The `_tr_b8` suffix indicates transposed 8-bit read. This avoids explicit transpose operations for FP8 matrices.

---

## ISSUE: Current Kernel LDS Addressing

**CLAIM**: The code model's `fwd_fp8_k64_256t.s` uses simple linear LDS addressing that doesn't match K=64 MFMA requirements.

**EVIDENCE**: fwd_fp8_k64_256t.s lines 125-137

```asm
; Current pattern:
v_and_b32_e32 v203, 31, v202         // lane % 32
v_lshrrev_b32_e32 v204, 5, v202      // lane / 32
v_lshlrev_b32_e32 v205, 7, v203      // (lane % 32) * 128
v_lshlrev_b32_e32 v206, 5, v204      // (lane / 32) * 32
v_add_u32_e32 v207, v205, v206       // base offset

; This gives: addr = (lane % 32) * 128 + (lane / 32) * 32
; Lane 0: addr = 0
; Lane 1: addr = 128 (SAME BANK as lane 0!)
; Lane 32: addr = 32
```

**PROBLEM 1**: Stride-128 causes 16x bank conflicts (128 bytes = 4 banks × 32 = full conflict cycle)

**PROBLEM 2**: The linear layout doesn't match ISA's interleaved thread-to-k mapping

---

## ISSUE: K-tile Semantics Confusion

**OBSERVATION**: Current kernel loads K[32 rows, 128 cols] per tile (4096 bytes), but the QK^T computation needs:
- Q: [128 rows, 128 cols] loaded once per block
- K: [S rows, 128 cols] loaded in chunks of 32 rows

**CURRENT K-LOOP**: Iterates over 32-row K chunks, doing 2 MFMAs per chunk (first 64 cols, then second 64 cols of head_dim).

**ACCUMULATION**: Each wave's v[80:95] accumulates the dot products. This accumulates Q×K^T correctly as:
```
sum over k in [0,127]: Q[m,k] × K[n,k]  (for each m in wave's 32 rows, n in 32 K rows)
```

The K-loop semantics appear correct, but LDS addressing is wrong.

---

## RECOMMENDED FIX FOR CODE MODEL

### Step 1: Fix LDS Stride
Replace stride-128 with stride-132 or stride-136 to avoid bank conflicts:
```asm
; Write Q to LDS with stride-136:
; addr = row * 136 + col
v_mov_b32_e32 v250, 136
v_mul_lo_u32 v251, row, v250
v_add_u32_e32 addr, col, v251
```

### Step 2: Use Correct Thread-to-Data Mapping for ds_read
For K=64 MFMA, each thread needs specific k-range data based on ISA layout:
- Thread groups 0-15/32-47: Load columns for rows M=[0-15]
- Thread groups 16-31/48-63: Load columns for rows M=[16-31]

### Step 3: Or Follow Triton's XOR Swizzle Pattern
Replicate the bitop3:0x36 and XOR offset pattern from Triton assembly.

---

## PERFORMANCE BASELINE (verified)

| Kernel | TF/s | Notes |
|--------|------|-------|
| BF16 ASM | 1016 | Reference |
| Triton FP8 | 1294 | Target to beat |
| FP8 ASM (untested) | ??? | fwd_fp8_k64_256t.s |

Target: >1300 TF/s (30% above BF16)

---

## PROVEN FACT: BF16 Reference Kernel LDS Pattern

**CLAIM**: BF16 kernel uses m0-based swizzle with stride 0x2040 per load.

**EVIDENCE**: fwd_hd128_bf16.s lines 180-270

**PATTERN**:
```asm
; Base LDS offset per wave:
s_mul_i32 s63, 0x408, s5           ; 0x408 = 1032 per wave
s_add_u32 s63, 0x8200, s63         ; base = 33280 + wave_id * 1032
s_mov_b32 m0, s63                  ; set m0 for buffer_load...lds

; Each load increments m0:
buffer_load_dwordx4 v4, s[8:11], 0 offen lds
s_add_u32 m0, 0x2040, m0           ; m0 += 8256

; ds_read offsets (8-byte stride for BF16):
ds_read_b64 v[160:161], v2
ds_read_b64 v[162:163], v2 offset:8
ds_read_b64 v[164:165], v2 offset:32
ds_read_b64 v[166:167], v2 offset:40
; Pattern: 0, 8, 32, 40, 64, 72, 96, 104
```

**KEY CONSTANTS**:
- 0x8200 = 33280 (base LDS offset for Q storage)
- 0x408 = 1032 (per-wave stride ≈ 32 rows × 32 stride + padding)
- 0x2040 = 8256 (load stride ≈ 64 rows × 129 effective stride)

---

## NEXT STEP FOR CODE MODEL

1. **Test current kernel** - Run `test_multiblock.py` to see if it produces any output
2. **If wrong results**: Focus on LDS addressing fix (stride-136 or XOR swizzle)
3. **If crashes**: Check VGPR allocation, waitcnt placement

The kernel structure (multi-block, K-loop, 256T/4 waves) is correct. The main issue is likely LDS addressing pattern.

---

## QUICK FIX: Simplest Path to Correct LDS Addressing

Instead of implementing full XOR swizzle, try **stride-136** for FP8:

```asm
; Q LDS layout: 128 rows × 128 cols at stride-136
; Total: 128 × 136 = 17408 bytes (fits in 20KB LDS)

; Write: row * 136 + col
v_mov_b32_e32 v250, 136
v_mul_lo_u32 v251, row_idx, v250
v_add_u32_e32 lds_addr, col_idx, v251

; Read for MFMA: need to gather correct k-range per thread
; Thread t needs k-range based on ISA layout
```

**WHY 136?**: 
- 136 = 128 + 8 (padding of 8 bytes)
- 136 / 4 = 34 (not divisible by 32 banks)
- Bank(addr) = (addr / 4) % 32
- Row 0 col 0: bank 0
- Row 1 col 0: bank 34 % 32 = 2
- Row 2 col 0: bank 68 % 32 = 4
- No conflicts!

**ALTERNATIVE 132**:
- 132 = 128 + 4
- 132 / 4 = 33
- Bank pattern: 0, 1, 2, 3, ... (perfect distribution)

---

## PROVEN FACT: Triton Diagonal Padding Strategy (from triton_padding_analysis.py)

**CLAIM**: Triton uses diagonal padding to achieve zero bank conflicts, not simple stride padding.

**EVIDENCE**: `hsa/gfx950/fmha_v3_fwd_fp8/low_priority/triton_padding_analysis.py`

**KEY PARAMETERS**:
```
warp_size = 64
vec_size = 8 bytes (FP8 vector load)
padding_interval = warp_size × vec_size = 512 bytes
row_size = 128 bytes (head_dim=128, FP8=1 byte)
padding = kWidth = 8 bytes (for FP8 k-contiguous)
wrap = min(row_size, 128) / padding = 128 / 8 = 16
```

**DIAGONAL ROW LAYOUT IN LDS**:
```
Rows are NOT stored linearly. They're arranged diagonally:
r0,  r4,  r8,  r12, r16, r20, r24, r28
pad, r1,  r5,  r9,  r13, r17, r21, r25
r29, pad, r2,  r6,  r10, r14, r18, r22
r26, r30, pad, r3,  ...

This creates different bank offsets for each row group.
```

**ADDRESS FORMULA**:
```
group = row % wrap                    ; which diagonal group (0-15)
pos_in_group = row / wrap             ; position within group
diagonal_shift = group * padding      ; shift for this group
addr = pos_in_group * (row_size + padding) + diagonal_shift + col

Simplified: addr = row * pitch + col + (row % period) * shift
```

**ZERO-CONFLICT COMBINATIONS** (from analysis):
```
pitch=136, period=8, shift=64  → 0 bank conflicts
pitch=144, period=4, shift=32  → 0 bank conflicts
```

**ASM IMPLEMENTATION**:
```asm
; For pitch=136, period=8, shift=64:
; addr = row * 136 + col + (row % 8) * 64

v_mul_lo_u32 v_addr, v_row, 136       ; row * pitch
v_add_u32 v_addr, v_addr, v_col       ; + col
v_and_b32 v_tmp, v_row, 7             ; row % 8 (period-1)
v_lshlrev_b32 v_tmp, 6, v_tmp         ; * 64 (shift)
v_add_u32 v_addr, v_addr, v_tmp       ; + diagonal shift
```

**LDS USAGE**:
- With pitch=136: 128 rows × 136 bytes = 17,408 bytes
- Plus diagonal shifts: max additional ~512 bytes
- Total: ~18KB (fits in 64KB LDS)

---

## RECOMMENDED IMPLEMENTATION FOR CODE MODEL

**Option A: Simple Stride-132 (easiest)**
```asm
; addr = row * 132 + col
v_mov_b32 v250, 132
v_mul_lo_u32 v_addr, v_row, v250
v_add_u32 v_addr, v_addr, v_col
```
- LDS: 128 × 132 = 16,896 bytes
- Bank pattern: 0, 1, 2, 3, ... (perfect)

**Option B: Diagonal Padding (matches Triton)**
```asm
; addr = row * 136 + col + (row & 7) << 6
v_mul_lo_u32 v_addr, v_row, 136
v_add_u32 v_addr, v_addr, v_col
v_and_b32 v_tmp, v_row, 7
v_lshlrev_b32 v_tmp, 6, v_tmp
v_add_u32 v_addr, v_addr, v_tmp
```
- LDS: ~18KB
- Matches Triton's strategy exactly

**RECOMMENDATION**: Start with Option A (stride-132) for simplicity. If performance is insufficient, switch to Option B.

---

## VERIFIED: Bank Conflict Analysis (test_bank_conflicts.py)

**TEST RESULTS** (run on 2025-01-18):
```
| Pattern              | Max Bank Conflict |
|----------------------|-------------------|
| Current (stride-128) | 32x (TERRIBLE!)   |
| Stride-132           | 0x (PERFECT!)     |
| Stride-136           | 2x                |
| Diagonal patterns    | 2-4x              |
```

**STRIDE-132 IS THE CLEAR WINNER** - zero bank conflicts with simple implementation.

**CURRENT PERFORMANCE**:
- Kernel runs at 474 TF/s (with 32x bank conflicts)
- Target: 1294 TF/s (Triton)
- Expected improvement: ~2-3x from eliminating bank conflicts

---

## EXACT CODE CHANGES FOR CODE MODEL

**File**: `fwd_fp8_k64_256t.s`

### Change 1: Update LDS constants (line ~30-32)
```asm
; OLD:
.set Q_LDS, 0           // 128 rows × 128 cols = 16384 bytes
.set K_LDS, 16384       // 32 rows × 128 cols = 4096 bytes
.set LDS_SIZE, 20480    // 20KB total

; NEW:
.set Q_STRIDE, 132      // bytes per row (was 128)
.set Q_LDS, 0           // 128 rows × 132 cols = 16896 bytes
.set K_LDS, 16896       // 32 rows × 132 cols = 4224 bytes  
.set LDS_SIZE, 21120    // ~21KB total
```

### Change 2: Q write pattern (line ~88-108)
Current code writes 64 bytes per thread linearly. Need to write with stride-132:
```asm
; Thread tid writes row (tid*64/128) = tid/2, col_start = (tid*64)%128
; With stride-132: addr = (tid/2) * 132 + (tid%2) * 64

; Compute row and col
v_lshrrev_b32_e32 v_row, 1, v200      ; tid / 2 = row
v_and_b32_e32 v_col, 1, v200          ; tid % 2
v_lshlrev_b32_e32 v_col, 6, v_col     ; (tid % 2) * 64 = col_start

; LDS addr = row * 132 + col_start
v_mov_b32_e32 v250, 132
v_mul_lo_u32 v_addr, v_row, v250
v_add_u32_e32 v_addr, v_addr, v_col
v_add_u32_e32 v_addr, Q_LDS, v_addr
```

### Change 3: Q read pattern (line ~143)
```asm
; OLD: v_lshlrev_b32_e32 v206, 7, v205   ; row * 128
; NEW:
v_mov_b32_e32 v250, 132
v_mul_lo_u32 v206, v205, v250           ; row * 132
```

### Change 4: K read pattern (line ~162)
```asm
; OLD: row * 128 implicit
; NEW:
v_mov_b32_e32 v250, 132
v_mul_lo_u32 v_k_base, v_row, v250      ; row * 132
v_add_u32_e32 v214, K_LDS, v_k_base
```

### Change 5: K write pattern (wherever K is written to LDS)
Same stride-132 adjustment for K tile writes.

---

## ANALYSIS UPDATE: Triton XOR Swizzle (2025-01-18)

**SURPRISING FINDING**: Even Triton's XOR swizzle has 8x bank conflicts per read!

From `analyze_triton_swizzle.py`:
```
Max bank conflict for addr1: 8x
Max bank conflict for addr2: 8x
```

**HOW TRITON HANDLES THIS**:
1. Issues 4 separate `ds_read_b128` with different XOR offsets (0, 32, 64, 96)
2. The 8x conflicts are spread across 4 reads, so ~2x per read
3. Uses `ds_read_b128` (128-bit) which accesses 4 consecutive banks

**TRITON'S 4KB SECTION LAYOUT**:
```
Section 0: offset 0     → 32 rows (Q rows 0-31)
Section 1: offset 4096  → 32 rows (Q rows 32-63)
Section 2: offset 8192  → 32 rows (Q rows 64-95)
Section 3: offset 12288 → 32 rows (Q rows 96-127)
```

**KEY INSIGHT**: Triton uses power-of-2 (128-byte) rows with 4KB section boundaries.
The XOR swizzle reduces but doesn't eliminate bank conflicts.

---

## QUESTION FOR USER

Previous stride-132/136 attempts had issues with "full attention with 4 waves, multiple blocks, Q and K tiles."

**What specific issues occurred?**
1. Numerical errors (wrong output)?
2. Crashes/hangs?
3. Performance regression?
4. LDS size overflow?
5. Alignment issues at wave/block boundaries?

Knowing the specific failure mode will help identify the root cause.

---

## DEBUG ANALYSIS: "Thread 16 outputs col 15" Issue (from multiblock.path)

**VERIFIED CORRECT** (via debug_k_mapping.py):
1. K write: Thread 128 writes K[16, 0:15] to K_LDS + 2048 ✓
2. K read: Lane 16 computes row=16, reads from K_LDS + 2048 ✓
3. Row formula: `row = (lane & 15) + ((lane >> 4) & 1) << 4` is correct ✓
4. k_off values: lanes 0-31 use k_off=0,32; lanes 32-63 use k_off=16,48 ✓

**THE BUG MUST BE ELSEWHERE**. Possibilities:

1. **MFMA output layout mismatch**: The 16 VGPRs v[80:95] output mapping might differ from expected
   - Need to verify which VGPR holds which (row, col) of the 32x32 output

2. **Identity test input construction**: If the test creates Q/K with wrong layout:
   - Check if Q/K use [B,H,S,D] or [B,S,H,D] layout
   - Check if FP8 quantization shifts values

3. **ds_read_b128 byte order**: The 16 bytes read might be reordered
   - FP8 packing: 8 values per 32-bit VGPR
   - Check if byte 0 maps to lane's k=0 or k=7

4. **Q write pattern mismatch**: 
   - Q uses 64 bytes/thread (rows interleaved differently than K)
   - Row r written by threads 2r and 2r+1
   - Verify this matches MFMA Q operand expectations

**MFMA OUTPUT LAYOUT** (from .lessons):
```
Thread t outputs to column (t % 32).
Thread 16 → column 16.

Row determined by VGPR index (interleaved):
- Threads 0-31 write rows: 0-3, 8-11, 16-19, 24-27
- Threads 32-63 write rows: 4-7, 12-15, 20-23, 28-31
```

**IF THREAD 16 OUTPUTS TO COL 15**, the issue is likely:
1. Output storage indexing is off-by-one
2. Test verification reads wrong column
3. Or the Q/K identity data is constructed with offset

**RECOMMENDED DEBUG FOR CODE MODEL**:
```python
# In test_multiblock.py, add:
# 1. Print raw Q_fp8 and K_fp8 patterns:
print(f"Q_fp8[0,0,0:32,0:32] non-zero positions")
print(f"K_fp8[0,0,0:32,0:32] non-zero positions")

# 2. Test with S=32 (single K-tile) first

# 3. Store thread metadata to output for verification:
# Modify kernel to write: output[tid] = float(lane_id | (row << 8) | (col << 16))
```

**KEY INSIGHT**: The "col 15 vs col 16" error suggests **off-by-one in N dimension**, not M (row). Check how the test verifies output columns.

---

## VERIFIED BUG: Wave Offset Not Applied (test_identity_debug.py)

**TEST RESULT** (S=32, single K-tile):
```
Wave 0: Correct identity pattern (1.0 at expected positions)
Wave 1: SAME pattern as Wave 0 (BUG: should be zeros/different!)
Wave 2: Wrong values (1.875)
Wave 3: Empty
```

**ROOT CAUSE**: Waves 1-3 are reading from same Q data as Wave 0!

**SUSPECTED ISSUE** in `fwd_fp8_k64_256t.s` line 152-154:
```asm
// Q LDS base = Q_LDS + wave_id * 4096 (32 rows × 128 bytes)
v_lshlrev_b32_e32 v209, 12, v201     // wave_id * 4096
v_add_u32_e32 v209, Q_LDS, v209      // Q base for this wave
```

**CHECK**: Is `v201` correctly set to wave_id?
Look at line ~81: `v_lshrrev_b32_e32 v201, 6, v200  // wave_id = tid / 64`

**POSSIBLE FIXES**:
1. Verify v201 contains wave_id (0-3) not lane_id
2. Check if v201 is clobbered before use at line 153
3. Add debug: store wave_id to output to verify

**VERIFIED**: v201 is NOT modified between lines 81 and 153.

**ALTERNATIVE THEORY**: The test with S=32 is too small!
- Q global memory: only 32×128 = 4096 bytes
- Thread 64+ loads from offset 4096+ (out of bounds → garbage/zeros)
- LDS bytes 4096-16383 contain garbage, NOT valid Q data
- Wave 1 reading garbage should NOT produce same pattern as wave 0

**IF wave 1 shows same pattern as wave 0**, the wave offset (v201×4096) is NOT being applied.

**CODE MODEL TODO**: 
1. Add test with S=128 (full Q tile) to verify multi-wave works
2. Or add debug kernel that stores wave_id: `buffer_store_dword v201, ...`

---

## CRITICAL BUG FOUND: K-loop Accumulates Wrong Dimension! (test_full_qblock.py)

**TEST RESULT** (S=128, 4 K-tiles):
```
Wave 0, Lane 0 (Q row 0): [1, 0, 0, 0]... - 1.0 at acc[0]
Wave 1, Lane 0 (Q row 32): [1, 0, 0, 0]... - 1.0 at acc[0]!
Wave 2, Lane 0 (Q row 64): [1, 0, 0, 0]... - 1.0 at acc[0]!
Wave 3, Lane 0 (Q row 96): [1, 0, 0, 0]... - 1.0 at acc[0]!
```

**PROBLEM**: ALL waves show 1.0 at acc[0], but:
- Q row 32 should output 1.0 at col 32, not col 0!
- Q row 64 should output 1.0 at col 64!
- Q row 96 should output 1.0 at col 96!

**ROOT CAUSE**: The K-loop ACCUMULATES across K-tiles instead of outputting separately!

```asm
K_LOOP:
    // Load K-tile (different K rows each iteration)
    // ...
    // MFMA accumulates into v[80:95]
    v_mfma_f32_32x32x64_f8f6f4 v[80:95], ..., v[80:95]  // WRONG: keeps adding!
    // ...
    s_cbranch K_LOOP
```

**CORRECT DESIGN FOR QK^T**:
- K-loop iterates over K-tiles (different 32-row chunks of K)
- Each K-tile produces 32 OUTPUT COLUMNS of QK^T
- Accumulators should be RESET per K-tile
- Output should be STORED after each K-tile, not after all tiles

**FIX OPTIONS**:
1. **Single K-tile mode**: Only process one K-tile, output 32 cols. Grid includes K-tile dimension.
2. **Multi-output store**: Store output after each K-tile to different locations.
3. **Restructure**: Make K-loop iterate over head_dim chunks, not K-row chunks.

**CURRENT KERNEL IS FOR GEMM ACCUMULATION, NOT ATTENTION QK^T!**

---

## PROVEN FACT: TR8 V-read base mapping (bank-conflict-free)

**CLAIM**: For `ds_read_b64_tr_b8`, a bank-conflict-free base uses
`base = (lane % 32) * 128 + (lane / 32) * 32`, with fixed offsets per TR8 read.

**EVIDENCE**: `hsa/gfx950/fmha_v3_fwd_fp8/low_priority/fwd_fp8_k64_tr8.s`
lines 80–90 show:
```
v_lshlrev_b32_e32 v205, 7, v203      // (lane % 32) * 128
v_lshlrev_b32_e32 v206, 5, v204      // (lane / 32) * 32
v_add_u32_e32 v207, v205, v206
```

---

## PROVEN FACT: QK+PV scaffold throughput with K=16 PV

**CLAIM**: The 2‑Q‑tile scaffold (512 threads, ping‑pong K/V LDS) with
K=16 PV MFMA and TR8 base mapping runs in ~12.65 ms for the target shape.

**EVIDENCE**: `test_scaffold.py` run (2026‑01‑18) output:
```
avg_ms: 12.649
TF/s (equivalent full-attention): 1671.4
TF/s (executed MFMA): 1679.5
```

---

## PROVEN FACT: Scaffold PMC counters (rocprofv3)

**CLAIM**: PMC counters show low LDS bank conflicts and high L2 hit rate
for the K=16 scaffold kernel.

**EVIDENCE**: `hsa/gfx950/fmha_v3_fwd_fp8/nimrodmi350/3782199_results.db`
average over dispatch_id 12–19:
```
GRBM_GUI_ACTIVE=27613110.55
SQ_LDS_BANK_CONFLICT=126362.75  (~0.46% of GUI)
TCC_HIT=4915078.91
TCC_MISS=469143.96  (L2 hit ~91.29%)
SQ_INSTS_VALU_MFMA_F8=12663000
SQ_INSTS_VALU_MFMA_MOPS_F8=1296691200
```
