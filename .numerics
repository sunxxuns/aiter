# FP8 Flash Attention - Numerical Validation

## Test Command

```bash
cd /sgl-workspace/aiter/hsa/gfx950/fmha_v3_fwd_fp8
python test_rigorous.py
```

---

## Max Error Results

| Test | Description | Max Error | Threshold | Status |
|------|-------------|-----------|-----------|--------|
| Transformer Embeddings | Normalized LayerNorm inputs | 0.086 | 0.10 | ✅ |
| Peaked Attention | One dominant key per query | 0.004 | 0.05 | ✅ |
| Position Patterns | Recency bias, start token | 0.005 | 0.05 | ✅ |
| Numerical Edge Cases | Large magnitudes, alternating | 0.047 | 0.05 | ✅ |
| Row/Column Confusion | Softmax dimension check | 0.000 | - | ✅ |
| Accumulation Order | FP ordering sensitivity | 0.000 | 0.10 | ✅ |
| Specific Patterns | Diagonal, block, copy | 0.006 | 0.05 | ✅ |

---

## Error Analysis

### FP8 Quantization Error Sources

| Source | Typical Max Error |
|--------|-------------------|
| Q/K/V input quantization | ~0.12 |
| P (softmax) quantization | ~0.02 |
| P sum drift | ~0.02 |

### Why Transformer Embeddings Has 0.086 Error

Row 19 (seed 42) has peaked attention distribution:
- P[19] max = 0.266 (concentrated attention)
- After FP8: P sum = 1.016 (1.6% drift)
- This is inherent FP8 limitation, not kernel bug

---

## Quick Validation

```bash
# Full test (7 scenarios)
python test_rigorous.py

# Quick row-wise check
python test_softmax_check.py

# Row vs block softmax
python test_rowwise_softmax.py
```

---

## Test Files

| File | Purpose |
|------|---------|
| `test_rigorous.py` | Full 7-test suite |
| `test_softmax_check.py` | Quick sanity check |
| `test_rowwise_softmax.py` | Row-wise verification |
| `test_transpose_theory.py` | S^T math proof |
