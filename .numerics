# FP8 Flash Attention Numerical Validation Guide

## Overview

This document provides rigorous numerical validation procedures for the FP8 flash attention
assembly kernel. Each phase must pass these tests before proceeding to the next.

---

## Current Status: Phase 1 + Milestone 2.0 + HD=128 QK

| Component | Status | Max Error | Test Count |
|-----------|--------|-----------|------------|
| QK MFMA (flat_load, HD=32)   | ✅ PASS | < 1e-5    | 50+ seeds  |
| Softmax (HD=32)  | ✅ PASS | < 1e-5    | 50+ seeds  |
| PV MFMA (HD=32)  | ✅ PASS | < 0.02    | 50+ seeds  |
| Full Attn (HD=32)| ✅ PASS | < 0.02    | 54 tests   |
| QK MFMA (buffer_load, HD=32) | ✅ PASS | < 0.001   | `test_qk_fixed.py` |
| **QK MFMA (buffer_load, HD=128)** | ✅ PASS | < 0.0001  | `test_qk_hd128.py` |

---

## Test Procedures

### 1. QK MFMA Test (`test_qk_only.py`)

**Purpose**: Verify S = Q @ K^T computation

**Test Command**:
```bash
cd /sgl-workspace/aiter/hsa/gfx950/fmha_v3_fwd_fp8
python3 test_qk_rigorous.py
```

**Validation Criteria**:
- Max absolute error vs FP32 reference: < 1e-5
- No NaN or Inf values
- Deterministic across 20+ runs
- Correct output layout (interleaved rows per MFMA spec)

**Reference Computation**:
```python
Q_fp8 = Q.to(torch.float8_e4m3fn)
K_fp8 = K.to(torch.float8_e4m3fn)
S_ref = torch.matmul(Q_fp8.float(), K_fp8.float().T)
```

**Key Verification Points**:
1. FP8 format is e4m3fn (OCP), NOT e4m3fnuz
2. Output layout matches MFMA 32x32x16 interleaved pattern:
   - Threads 0-31: rows 0,1,2,3, 8,9,10,11, 16,17,18,19, 24,25,26,27
   - Threads 32-63: rows 4,5,6,7, 12,13,14,15, 20,21,22,23, 28,29,30,31
3. **MFMA Output Layout Formula** (discovered via `decode_mfma_layout.py`):
   ```
   col = tid % 32
   row = ((vreg-32) % 4) + (tid//32)*4 + ((vreg-32)//4)*8
   ```

---

### 2. Softmax Test (`test_softmax_only.py`)

**Purpose**: Verify P = softmax(S) row-wise computation

**Test Command**:
```bash
cd /sgl-workspace/aiter/hsa/gfx950/fmha_v3_fwd_fp8
python3 test_softmax_rigorous.py
```

**Validation Criteria**:
- Row sums = 1.0 (tolerance: 1e-5)
- All values positive (min > 0)
- Max absolute error vs reference: < 1e-5
- No NaN or Inf values
- Deterministic across 20+ runs

**Reference Computation**:
```python
P_ref = torch.softmax(S, dim=-1)
```

**Key Verification Points**:
1. Uses log2(e) = 0x3fb8aa3b for v_exp_f32 (which computes 2^x)
2. Cross-thread reduction via ds_swizzle SWAP patterns
3. **Critical**: s_nop 7 required after v_exp_f32 and v_rcp_f32

---

### 3. PV MFMA Test (`test_pv_only.py`)

**Purpose**: Verify O = P @ V computation with P redistribution

**Test Command**:
```bash
cd /sgl-workspace/aiter/hsa/gfx950/fmha_v3_fwd_fp8
python3 test_pv_rigorous.py
```

**Validation Criteria**:
- Max absolute error vs reference: < 0.02 (FP8 precision limited)
- No NaN or Inf values
- Deterministic across 20+ runs

**Reference Computation**:
```python
P_fp8 = P.to(torch.float8_e4m3fn).float()
V_fp8 = V.to(torch.float8_e4m3fn).float()
O_ref = torch.matmul(P_fp8, V_fp8)
```

**Key Verification Points**:
1. P redistribution via LDS (QK output layout ≠ PV input layout)
2. **Critical**: v_cvt_pk_fp8_f32 leaves garbage in high 16 bits - must mask!
3. V loading with stride 32 for transposed access

---

### 4. Full Attention Test (`test_integrated.py`)

**Purpose**: Verify O = softmax(Q @ K^T) @ V end-to-end

**Test Command**:
```bash
cd /sgl-workspace/aiter/hsa/gfx950/fmha_v3_fwd_fp8
python3 test_integrated.py
```

**Validation Criteria**:
- Max absolute error vs reference: < 0.05 (cumulative FP8 error)
- No NaN or Inf values
- Output range matches reference (±0.1)
- Deterministic across 20+ runs
- 50+ random seeds pass

**Reference Computation**:
```python
Q_q = Q.to(torch.float8_e4m3fn).float()
K_q = K.to(torch.float8_e4m3fn).float()
V_q = V.to(torch.float8_e4m3fn).float()
S = torch.matmul(Q_q, K_q.T)
P = torch.softmax(S, dim=-1)
O_ref = torch.matmul(P, V_q)  # Note: P not quantized to FP8 in reference
```

---

## Rigorous Test Script

Run this to validate entire Phase 1:

```bash
cd /sgl-workspace/aiter/hsa/gfx950/fmha_v3_fwd_fp8

python3 << 'EOF'
import torch
import subprocess
import ctypes
import numpy as np

print("=" * 70)
print("PHASE 1 NUMERICAL VALIDATION")
print("=" * 70)

# Build kernel
subprocess.run([
    '/opt/rocm/llvm/bin/clang++', '-x', 'assembler',
    '-target', 'amdgcn-amd-amdhsa', '-mcpu=gfx950', '-mwavefrontsize64',
    '-c', 'fwd_fp8_integrated.s', '-o', 'fwd_fp8_integrated.o'
], check=True)
subprocess.run([
    '/opt/rocm/llvm/bin/clang++',
    '-target', 'amdgcn-amd-amdhsa', '-mcpu=gfx950', '-mwavefrontsize64',
    'fwd_fp8_integrated.o', '-o', 'fwd_fp8_integrated.co'
], check=True)

hip = ctypes.CDLL('libamdhip64.so')
mod = ctypes.c_void_p()
hip.hipModuleLoad(ctypes.byref(mod), b'fwd_fp8_integrated.co')
func = ctypes.c_void_p()
hip.hipModuleGetFunction(ctypes.byref(func), mod, b'_ZN5aiter17fwd_fp8_integratedE')

def test_kernel(Q, K, V):
    Q_fp8 = Q.to(torch.float8_e4m3fn)
    K_fp8 = K.to(torch.float8_e4m3fn)
    V_fp8 = V.to(torch.float8_e4m3fn)
    
    Q_gpu = Q_fp8.view(torch.uint8).cuda()
    K_gpu = K_fp8.view(torch.uint8).cuda()
    V_gpu = V_fp8.view(torch.uint8).cuda()
    O = torch.zeros(32, 32, dtype=torch.float32, device='cuda')
    
    args = (ctypes.c_void_p * 4)(
        ctypes.cast(ctypes.pointer(ctypes.c_uint64(O.data_ptr())), ctypes.c_void_p),
        ctypes.cast(ctypes.pointer(ctypes.c_uint64(Q_gpu.data_ptr())), ctypes.c_void_p),
        ctypes.cast(ctypes.pointer(ctypes.c_uint64(K_gpu.data_ptr())), ctypes.c_void_p),
        ctypes.cast(ctypes.pointer(ctypes.c_uint64(V_gpu.data_ptr())), ctypes.c_void_p),
    )
    hip.hipModuleLaunchKernel(func, 1, 1, 1, 64, 1, 1, 4096, None, args, None)
    hip.hipDeviceSynchronize()
    
    # Reference
    S = torch.matmul(Q_fp8.float(), K_fp8.float().T)
    P = torch.softmax(S, dim=-1)
    O_ref = torch.matmul(P, V_fp8.float())
    
    return O.cpu(), O_ref

# Test 1: Random inputs (50 seeds)
print("\n[1] Random Input Tests (50 seeds)")
pass_count = 0
max_errors = []
for seed in range(50):
    torch.manual_seed(seed)
    Q = torch.randn(32, 32) * 0.3
    K = torch.randn(32, 32) * 0.3
    V = torch.randn(32, 32) * 0.5
    O_gpu, O_ref = test_kernel(Q, K, V)
    err = (O_gpu - O_ref).abs().max().item()
    max_errors.append(err)
    if err < 0.05 and not torch.isnan(O_gpu).any():
        pass_count += 1
print(f"    Passed: {pass_count}/50")
print(f"    Max error: {max(max_errors):.6f}")
print(f"    Avg error: {np.mean(max_errors):.6f}")

# Test 2: Determinism
print("\n[2] Determinism Test (20 runs)")
torch.manual_seed(42)
Q, K, V = torch.randn(32,32)*0.3, torch.randn(32,32)*0.3, torch.randn(32,32)*0.5
results = [test_kernel(Q, K, V)[0].numpy() for _ in range(20)]
deterministic = all(np.array_equal(results[0], r) for r in results[1:])
print(f"    Deterministic: {deterministic}")

# Test 3: No NaN/Inf
print("\n[3] NaN/Inf Check (50 seeds)")
nan_count = inf_count = 0
for seed in range(50):
    torch.manual_seed(seed)
    Q, K, V = torch.randn(32,32)*0.3, torch.randn(32,32)*0.3, torch.randn(32,32)*0.5
    O_gpu, _ = test_kernel(Q, K, V)
    if torch.isnan(O_gpu).any(): nan_count += 1
    if torch.isinf(O_gpu).any(): inf_count += 1
print(f"    NaN occurrences: {nan_count}/50")
print(f"    Inf occurrences: {inf_count}/50")

# Test 4: Output range
print("\n[4] Output Range Check")
torch.manual_seed(999)
Q, K, V = torch.randn(32,32)*0.3, torch.randn(32,32)*0.3, torch.randn(32,32)*0.5
O_gpu, O_ref = test_kernel(Q, K, V)
print(f"    Kernel: [{O_gpu.min():.4f}, {O_gpu.max():.4f}]")
print(f"    Ref:    [{O_ref.min():.4f}, {O_ref.max():.4f}]")

print("\n" + "=" * 70)
print("VALIDATION COMPLETE")
print("=" * 70)

hip.hipModuleUnload(mod)
EOF
```

---

## Error Budget Analysis

| Stage | Expected Error | Actual Error | Source |
|-------|---------------|--------------|--------|
| Q→FP8 | ~0.5% | Measured | Quantization |
| K→FP8 | ~0.5% | Measured | Quantization |
| QK MFMA | ~1e-5 | < 1e-5 | FP32 accumulator |
| Softmax | ~1e-5 | < 1e-5 | FP32 computation |
| P→FP8 | ~1% | Measured | Quantization |
| V→FP8 | ~0.5% | Measured | Quantization |
| PV MFMA | ~1e-5 | < 1e-5 | FP32 accumulator |
| **Total** | ~2-3% | < 2% | Cumulative |

---

## Known Limitations (Phase 1)

1. **Shape**: Only 32×32 matrices (head_dim=32)
2. **Batch/Heads**: Single batch, single head
3. **Sequence Length**: Fixed at 32
4. **Scaling**: No attention scaling applied
5. **Causal Masking**: Not implemented

---

---

## Benchmark Results (Jan 2026)

### FP8 ASM (Current - Phase 1)

| Shape (S×D) | Time (μs) | GFLOP/s | TF/s |
|-------------|-----------|---------|------|
| 32×32 | 7.58 | 17.3 | 0.017 |

*Note: Launch overhead dominated at tiny shape*

### BF16 ASM Baseline (for comparison)

| Shape (B×S×H×D) | Time (ms) | TF/s |
|-----------------|-----------|------|
| 1×1024×40×128 | 0.054 | 396 |
| 1×4096×40×128 | 0.440 | 781 |
| 1×8192×40×128 | 1.402 | 980 |

**FP8 Target**: >1300 TF/s at production shapes (+30% over BF16)

---

## Milestone 2.0: buffer_load→LDS Validation

### Test: `test_qk_fixed.py`

**Purpose**: Verify QK MFMA with scalable buffer_load→LDS memory pattern

**Test Command**:
```bash
cd /sgl-workspace/aiter/hsa/gfx950/fmha_v3_fwd_fp8
python test_qk_fixed.py
```

**Validation Criteria**:
- Max absolute error vs FP32 reference: < 0.001
- No NaN or Inf values
- Identical results to flat_load version

**Key Verification Points**:
1. Buffer descriptor setup: `s[8:11]` with `s[11]=0x20000`
2. Async load to LDS: `buffer_load_dwordx4 v_offset, s[8:11], 0 offen lds`
3. **Critical**: Store pattern must use correct MFMA output layout formula
4. LDS allocation: 2KB for Q + K (HD=32)

**Results**:
```
S_ref[0,:8]: [0.0478, 0.357, 2.282, -0.488, 3.277, -0.630, -2.339, 1.457]
S_out[0,:8]: [0.0478, 0.357, 2.282, -0.488, 3.277, -0.630, -2.339, 1.457]
Max error: 0.000137
```

---

### Supporting Tests

| Test File | Purpose | Status |
|-----------|---------|--------|
| `test_buffer_lds.py` | Basic buffer_load→LDS roundtrip | ✅ PASS |
| `test_qk_fixed.py` | QK MFMA with buffer_load | ✅ PASS |
| `decode_mfma_layout.py` | MFMA output layout analysis | ✅ Complete |

---

---

## Milestone 2.2: HD=128 QK MFMA Validation

### Test: `test_qk_hd128.py`

**Purpose**: Verify S[32×32] = Q[32×128] @ K^T[128×32] with 8 MFMA passes

**Test Command**:
```bash
cd /sgl-workspace/aiter/hsa/gfx950/fmha_v3_fwd_fp8
python test_qk_hd128.py
```

**Results (Jan 2026)**:
```
[TEST 2] Multiple seeds
  Seed 0: max_err=0.000046 [PASS]
  Seed 1: max_err=0.000053 [PASS]
  Seed 2: max_err=0.000061 [PASS]
  Seed 3: max_err=0.000046 [PASS]
  Seed 4: max_err=0.000053 [PASS]
```

**Validation Criteria**:
- Max absolute error vs FP32 reference: < 0.001 ✅
- No NaN or Inf values ✅
- All seeds pass ✅

**Key Implementation Details**:
1. buffer_load with soffset (SGPR) for 4KB loading
2. 8 MFMA passes accumulating into S[32×32]
3. Correct LDS addressing: row * 128 + half * 8

---

## Next Phase Requirements

Phase 2 (HD=128) remaining:
- Softmax (same as HD=32, standard softmax OK for S=32)
- 8 PV MFMA passes for O[32×128]
- Full integration test
- **Online softmax** deferred until K-tile loop (Phase 3)
