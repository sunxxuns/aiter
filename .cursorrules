###task###
 to write an assembly kernel to do fp8 flash attention, to provide tf/s higher than bf16
 the bf16 flash attention as the golden reference:
 /sgl-workspace/aiter/hsa/gfx950/fmha_v3_fwd/fwd_hd128_bf16.s

###Make sure ###
do not fall back to triton or CK kernels, we only want asm
do not rewrite different strategy than the bf16 reference, as it would be very challenging
use per tensor scaled fp8 1-4-3 input make sure it's same as mfma fp8 kernel expect. 
first downcast then calling mfma fp8
we are using the same strategy as the bf16 version, such as online softmax
read past lessnons recorded in .lessons

###CRITICAL: Python Debug Buffer Bug - WASTED HOURS OF DEBUGGING!###
When reading uint32 values from a float32 debug buffer in Python:

WRONG (returns 0 for values like 0x2e9b391c stored as raw bits):
  b64_lo = int(d[0].item())   # BUG: int(0.000000) = 0

CORRECT (interprets raw bits properly):
  import struct
  def as_u32(f): return struct.unpack('I', struct.pack('f', f))[0]
  b64_lo = as_u32(d[0].item())  # Returns 0x2e9b391c correctly

This bug caused HOURS debugging "ds_read_b64 returns zeros" when the instruction
was working correctly! Kernel stores uint32 via v_mov_b32, Python int() truncates.
ALWAYS use struct.unpack for uint32 values in float32 debug buffers!

###Follow BF16 Patterns for Performance###
The BF16 reference kernel uses SWIZZLED LDS layout for performance (bank-conflict-free).
- The swizzling with 0x408 stride, base offsets like 0x8200 is for PERFORMANCE
- Simple row-major LDS layout DOES work correctly (ds_read_b64 is fine)
- But BF16's swizzled layout is recommended for optimal throughput

### benchmark ###
to benchmark tf/s use python /sgl-workspace/sglang/benchmark/kernels/bench_mi350_fmha_asm.py --seq-len 32130
this is the shape we are aiming for
this is how we bench bf16 flash attn kernel around 1000TF/s, fp8 should at least have 30% more
divide the diffcult task into small tasks and make the numeric right
### hill climbing strategy ###
-refer to .milestone to understand roadmap and the current phase
-adding/revising .s file to implement
-compare with the bf16 reference and update the same and difference in .comp
-return if there's significant difference non necessary
-doing numeric tests 
-doing benchmar on tf/s
-return if numeric tests does not pass or there's non scalable performance
-write down lessons and update .milestone .lessons .bench .numerics for reviewer to decide whether to move to next phase
-check if current .bench results are scalable toward the goal and update .scale

#additional:
do not do long reasoning on the entire flash attn kernel, instead, calve out problemtic part and do minmal test/debug,
for each time, do vigorious numerica test, use structured or random input to reveal potential accumulation or order issues,
do not use uniform input got false positive
# log files
.bench: record tf/s and reason if it meets expected value for fp8 (using bf16 as a reference)
.scale: reasoning if current implementation is scalable to > 1300 TF/s for the shapes in --seq-len 32130
.numerics: make sure the layout correct, yield correct results
.comp: make sure following the patterns in bf16 asm, with only the necessary differences
.milestone: record road maps and summarize current status
