# FP8 Flash Attention - Scalability Analysis

## Current Implementation

### Resource Usage (test_full_hd128.s)
- VGPRs: 68 (plenty of headroom)
- SGPRs: 24
- LDS: 8192 bytes
- AGPRs: 4 (for MFMA accumulator)

### Tile Configuration
- Q tile: 32 queries × 128 head_dim
- K tile: 32 keys × 128 head_dim
- V tile: 32 values × 128 head_dim
- S tile: 32 × 32 attention scores

---

## Scalability to Long Sequences

### K-tile Loop (Phase 3)
- Each tile: 32 keys
- seq_len=32130 → 1005 tiles
- Must maintain online softmax state across tiles

### Memory Pattern
```
Per tile:
- Load K: 32 × 128 × 1 = 4KB (FP8)
- Load V: 32 × 128 × 1 = 4KB (FP8)
- QK MFMA: 32×32 F32 = 4KB
- PV MFMA: accumulate to O

Total per tile: ~12KB new data + LDS reuse
```

### Compute Pattern
```
Per tile:
- QK: 32 × 32 × 128 × 2 = 262K ops (FP8 MFMA)
- Softmax: 32 × 32 × ~10 = 10K ops (F32)
- PV: 32 × 128 × 32 × 2 = 262K ops (FP8 MFMA)

Total: ~524K FP8 ops + 10K F32 ops per tile
```

---

## Projected Performance

### Hardware Limits (gfx950)
- FP8 MFMA: ~2000 TF/s theoretical peak
- Memory BW: ~3.2 TB/s HBM

### Kernel Efficiency
- BF16 achieves ~1000 TF/s (50% of BF16 peak)
- FP8 target: 1300 TF/s (~65% of FP8 peak)

### Bottleneck Analysis
- Short sequences (seq<1024): Memory bound
- Long sequences (seq>4096): Compute bound
- Sweet spot: seq=32130 should be compute bound

---

## Occupancy

### Current (single tile)
- 1 workgroup × 64 threads
- Low occupancy acceptable for debugging

### Production Target
- Multiple Q-blocks per workgroup
- 4 waves for latency hiding
- LDS double-buffering for K/V loads
