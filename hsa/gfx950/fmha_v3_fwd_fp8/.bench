# FP8 Flash Attention Benchmark

## Quick Benchmark

```bash
cd /sgl-workspace/aiter/hsa/gfx950/fmha_v3_fwd_fp8

# Single sequence length
python bench_fp8_attention.py --seq-len 1024

# Sweep all sequence lengths  
python bench_fp8_attention.py --sweep

# BF16 reference (for comparison)
python /sgl-workspace/sglang/benchmark/kernels/bench_mi350_fmha_asm.py --seq-len 32130
```

## Current Results (Single-Head, 32 Output Rows)

```
SeqLen  Tiles   Time(us)     TFLOPS
--------------------------------------
    32      1        6.6     0.0805
    64      2        9.8     0.1076
   128      4       16.2     0.1305
   256      8       28.8     0.1471
   512     16       54.1     0.1567
  1024     32      104.6     0.1620
  2048     64      205.7     0.1647
  4096    128      408.1     0.1661
```

## FP8 vs BF16 Comparison (`--compare`)

```
SeqLen    FP8 us/GF   BF16 us/GF    Speedup
--------------------------------------------
    32    12422.990    56398.823     4.54x  ← FP8 wins (same work)
    64     9285.919    13382.402     1.44x  ← FP8 wins
   128     7646.149     3283.553     0.43x  ← BF16 wins (parallelism)
   256     6782.253      885.982     0.13x
  1024     6171.260       80.362     0.01x
  4096     6023.830       11.216     0.00x
```

**Key Insight**:
- FP8 compute efficiency is **~2x better** than BF16 (us/GF at seq=32)
- BF16 wins at large seq_len due to **multi-workgroup parallelism**
- FP8 needs Phase 5+ (multi-head) to match BF16 at production sizes

## Target Performance (Full Kernel)

- **BF16 baseline**: ~1000 TF/s (at 40 heads, seq=32130)
- **FP8 target**: >1300 TF/s (30% improvement)

## FLOP Calculation

For flash attention with batch=1, heads=H, seq_len=N, head_dim=128:

```
Per head:
  QK MFMA:  32 * N * 128 * 2 FLOPs
  Softmax:  32 * N * 5 FLOPs  
  PV MFMA:  32 * N * 128 * 2 FLOPs
  Total:    32 * N * 517 FLOPs

Full kernel (H heads):
  Total:    H * 32 * N * 517 FLOPs
```

## Benchmark Methodology

- Warmup: 10 iterations
- Measure: 100 iterations  
- Timing: `hipEventElapsedTime` (GPU-accurate)

## Why Current TF/s is Low

1. **Single head**: Production uses 40 heads
2. **32 output rows**: Only one workgroup
3. **Memory bound**: Small compute/memory ratio at low seq_len
4. **Launch overhead**: Dominates at small sizes

## Next Steps (Phase 5+)

1. **Multi-head support**: Launch multiple workgroups
2. **Batch support**: Process multiple sequences
3. **Integration**: Wire into aiter.ops.mha API
4. **Optimization**: Prefetching, scheduling, LDS banking
