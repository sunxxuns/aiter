# FP8 Flash Attention Development Milestone

## Goal
Achieve >1300 TF/s (30% improvement over BF16's ~1000 TF/s)

## Current State

### Working Kernels (64 threads, 32 Q rows)
| Kernel | Status | Notes |
|--------|--------|-------|
| `fwd_fp8_kloop.s` | âœ… | K-loop with online softmax, HD=128 |
| `fwd_fp8_full.s` | âœ… | Full seq_len support |
| `fwd_fp8_256t_min.s` | âœ… | 256T launch, wave 0 only computes |

### Performance Gap
- Current: ~150 TF/s (32 Q rows, 64 threads effective)
- BF16 reference: ~1000 TF/s (128 Q rows, 256 threads)
- Gap: ~6.7Ã— lower utilization

---

## Key Insight: BF16 Architecture (CORRECTED)

### Initial Wrong Assumption
> "4 waves share 32 Q rows, each wave handles 8 rows"

### Actual BF16 Architecture (from fwd_hd128_bf16.s analysis)
> "4 waves handle 128 Q rows total, each wave has its own 32 rows"

| Wave | Q Rows | Complete MFMA Tile | Independent |
|------|--------|-------------------|-------------|
| 0 | 0-31 | âœ… 32Ã—32 | âœ… |
| 1 | 32-63 | âœ… 32Ã—32 | âœ… |
| 2 | 64-95 | âœ… 32Ã—32 | âœ… |
| 3 | 96-127 | âœ… 32Ã—32 | âœ… |

**Evidence from BF16 kernel**:
```asm
// Line 2469: Output row offset
s_mul_i32 s40, s5, 32              // wave_id Ã— 32 rows
```

---

## Hardware Constraint: MFMA 32Ã—32 Minimum

**On gfx950, only one FP8/BF16 MFMA size exists**:
- FP8: `v_mfma_f32_32x32x16_fp8_fp8`
- BF16: `v_mfma_f32_32x32x8_bf16`

No 8Ã—8 or 16Ã—16 variants available!

This means:
- Each wave MUST compute 32 output rows (complete MFMA tile)
- Cannot split 32 rows across 4 waves efficiently
- The "compute 32, use 8" approach would waste 75% compute

---

## Path to >1300 TF/s

### Phase 1: Increase Q Tile Size âœ…â†’ðŸ”„
Current: 32 Q rows per workgroup
Target: 128 Q rows per workgroup (4Ã— more work)

### Phase 2: Per-Wave Independence
- Wave 0: Loads Q[0:31], computes S[0:31, :], outputs O[0:31, :]
- Wave 1: Loads Q[32:63], computes S[32:63, :], outputs O[32:63, :]
- Wave 2: Loads Q[64:95], computes S[64:95, :], outputs O[64:95, :]
- Wave 3: Loads Q[96:127], computes S[96:127, :], outputs O[96:127, :]

### Phase 3: Shared K/V Loading
- All 256 threads cooperate to load K and V tiles
- K/V shared in LDS across all waves
- Only Q and intermediate P are per-wave

### Expected Performance
| Metric | Current | Target |
|--------|---------|--------|
| Q rows/workgroup | 32 | 128 |
| Active threads | 64 | 256 |
| TF/s | 150 | 1300+ |
| vs BF16 | 15% | 130% |

---

## LDS Layout - Lessons Learned

### Simple Row-Major Works!
After extensive debugging, we found that simple row-major LDS layout DOES work correctly.
The "ds_read_b64 returns zeros" issue was a **Python test bug** (see .lessons).

Simple row-major is fine:
```
Wave 0: Q at LDS[0:4095]        (32Ã—128 bytes)
Wave 1: Q at LDS[4096:8191]
...
```

### BF16 Swizzling for Performance
BF16 uses swizzled layout for **performance** (bank-conflict-free), not correctness:
```asm
s_mul_i32 s63, 0x408, s5         // wave_id Ã— 1032 (swizzled stride)
s_add_u32 s63, 0x8200, s63       // + 33280 base
```

**Recommendation**: Use BF16 swizzling for optimal performance, but simple layout works.

---

## Implementation Steps (UPDATED)

### Phase 1: Working 64T Kernel âœ… COMPLETE
- fwd_fp8_kloop.s: K-loop with online softmax
- ds_read_b64 works correctly (Python test bug was fixed)

### Phase 1.5: Build 256T FP8 Kernel Following BF16 Exactly [CURRENT]
**Decision**: Use ds_read_b64_tr_b16 with "2 FP8 = 1 BF16" packing (see lesson 8.8)

**Why this approach**:
- EXACT same LDS byte layout as BF16
- EXACT same swizzle pattern (proven bank-conflict-free)
- EXACT same instruction sequence (ds_read_b64_tr_b16)
- Matches .cursorrules: "do not rewrite different strategy than bf16 reference"

**Implementation plan**:
1. [x] Understand BF16's swizzle formula (read offsets derived)
2. [x] Confirm ds_read_b64_tr_b16 works with FP8 pairs
3. [x] Document approach (lesson 8.8)
4. [x] **Swizzled write + transpose read verified** (step2_flat.s)
   - buffer_load ... offen lds has buffer descriptor issues
   - Used flat_load + ds_write_b128 with swizzled addresses instead
   - Transpose read gives correct pattern: stride-4 gather
   - Thread 0 reads: [row0, row4, row8, row12] = perfect!
5. [x] **QK MFMA verified** (step3_qk.s)
   - Simple row-major LDS layout works fine
   - MFMA output values match reference (1024/1024, max_diff=0.0002)
   - Output layout is MFMA-native (not row-major), but that's OK
   - Softmax and PV MFMA use the same layout, no conversion needed
6. [x] **Working 64T kernel verified** (fwd_fp8_kloop.s)
   - Uniform input: max_diff=0.000000 (exact match!)
   - P/V read patterns are complex but correct
   - P stored as F32 in LDS, converted to FP8 on read
   - V read with row-stride pattern (stride 128)

### Phase 2: Scale to 256T (128 Q rows) [NEXT]
1. [ ] Extend fwd_fp8_kloop.s to 4 waves
2. [ ] Each wave handles its own 32 Q rows (independent)
3. [ ] All waves share K/V loading (cooperative)
4. [ ] Per-wave output to O[wave_id*32 : wave_id*32+32]
5. [ ] Follow BF16's wave coordination pattern

### Phase 3: Optimize for >1300 TF/s
5. [ ] Double buffering for K/V tiles
6. [ ] Instruction scheduling (hide memory latency)
7. [ ] Benchmark and tune

### Key Insights
- Simple row-major LDS works (no swizzling required for correctness)
- BF16 swizzling is for performance optimization only
- Waves are independent (no cross-wave reduction needed)
- **CRITICAL**: Kernel metadata must match working kernel:
  ```asm
  .amdhsa_next_free_vgpr 148
  .amdhsa_accum_offset 148
  ```
  Using lower values causes MFMA output corruption/NaN!

---

## Files Reference

```
Working:
â”œâ”€â”€ fwd_fp8_kloop.s      # 64T, 32 Q rows, K-loop, online softmax
â”œâ”€â”€ fwd_fp8_full.s       # 64T, 32 Q rows, full seq_len
â””â”€â”€ fwd_fp8_256t_min.s   # 256T launch, wave 0 only (proof of concept)

Tests:
â”œâ”€â”€ test_kloop_attn.py   # Tests fwd_fp8_kloop.s
â”œâ”€â”€ test_full_attention.py
â”œâ”€â”€ test_numerics.py     # Comprehensive numeric validation
â”œâ”€â”€ test_bf16_baseline.py
â”œâ”€â”€ debug_harness.py     # Debug test suite
â””â”€â”€ asm_validator.py     # Static analysis
```
