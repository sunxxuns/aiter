# FP8 Flash Attention Development Milestone

## Goal
Achieve >1300 TF/s (30% improvement over BF16's ~1000 TF/s)

## Current State

### Working Kernels (64 threads, 32 Q rows)
| Kernel | Status | Notes |
|--------|--------|-------|
| `fwd_fp8_kloop.s` | âœ… | K-loop with online softmax, HD=128 |
| `fwd_fp8_full.s` | âœ… | Full seq_len support |
| `fwd_fp8_256t_min.s` | âœ… | 256T launch, wave 0 only computes |

### Performance Gap
- Current: ~150 TF/s (32 Q rows, 64 threads effective)
- BF16 reference: ~1000 TF/s (128 Q rows, 256 threads)
- Gap: ~6.7Ã— lower utilization

---

## Key Insight: BF16 Architecture (CORRECTED)

### Initial Wrong Assumption
> "4 waves share 32 Q rows, each wave handles 8 rows"

### Actual BF16 Architecture (from fwd_hd128_bf16.s analysis)
> "4 waves handle 128 Q rows total, each wave has its own 32 rows"

| Wave | Q Rows | Complete MFMA Tile | Independent |
|------|--------|-------------------|-------------|
| 0 | 0-31 | âœ… 32Ã—32 | âœ… |
| 1 | 32-63 | âœ… 32Ã—32 | âœ… |
| 2 | 64-95 | âœ… 32Ã—32 | âœ… |
| 3 | 96-127 | âœ… 32Ã—32 | âœ… |

**Evidence from BF16 kernel**:
```asm
// Line 2469: Output row offset
s_mul_i32 s40, s5, 32              // wave_id Ã— 32 rows
```

---

## Hardware Constraint: MFMA 32Ã—32 Minimum

**On gfx950, only one FP8/BF16 MFMA size exists**:
- FP8: `v_mfma_f32_32x32x16_fp8_fp8`
- BF16: `v_mfma_f32_32x32x8_bf16`

No 8Ã—8 or 16Ã—16 variants available!

This means:
- Each wave MUST compute 32 output rows (complete MFMA tile)
- Cannot split 32 rows across 4 waves efficiently
- The "compute 32, use 8" approach would waste 75% compute

---

## Path to >1300 TF/s

### Phase 1: Increase Q Tile Size âœ…â†’ðŸ”„
Current: 32 Q rows per workgroup
Target: 128 Q rows per workgroup (4Ã— more work)

### Phase 2: Per-Wave Independence
- Wave 0: Loads Q[0:31], computes S[0:31, :], outputs O[0:31, :]
- Wave 1: Loads Q[32:63], computes S[32:63, :], outputs O[32:63, :]
- Wave 2: Loads Q[64:95], computes S[64:95, :], outputs O[64:95, :]
- Wave 3: Loads Q[96:127], computes S[96:127, :], outputs O[96:127, :]

### Phase 3: Shared K/V Loading
- All 256 threads cooperate to load K and V tiles
- K/V shared in LDS across all waves
- Only Q and intermediate P are per-wave

### Expected Performance
| Metric | Current | Target |
|--------|---------|--------|
| Q rows/workgroup | 32 | 128 |
| Active threads | 64 | 256 |
| TF/s | 150 | 1300+ |
| vs BF16 | 15% | 130% |

---

## LDS Layout (64KB available)

```
Offset      Size    Content
0x0000      16KB    Q[128Ã—128Ã—1B] - all 128 rows
0x4000      4KB     K tile[32Ã—128Ã—1B]
0x5000      4KB     V tile[32Ã—128Ã—1B]  
0x6000      16KB    P intermediate per wave (4Ã—4KB)
0x8000      24KB    Scratch/prefetch
```

---

## Implementation Steps

1. [ ] Modify kernel to accept 128 Q rows (increase LDS allocation)
2. [ ] Per-wave Q region: wave loads Q[wave_id*32:(wave_id+1)*32]
3. [ ] Shared K/V: all 256 threads cooperate to load
4. [ ] Per-wave softmax (rows are independent, no cross-wave sync)
5. [ ] Per-wave PV compute and output
6. [ ] Benchmark and tune

---

## Files Reference

```
Working:
â”œâ”€â”€ fwd_fp8_kloop.s      # 64T, 32 Q rows, K-loop, online softmax
â”œâ”€â”€ fwd_fp8_full.s       # 64T, 32 Q rows, full seq_len
â””â”€â”€ fwd_fp8_256t_min.s   # 256T launch, wave 0 only (proof of concept)

Tests:
â”œâ”€â”€ test_kloop_attn.py   # Tests fwd_fp8_kloop.s
â”œâ”€â”€ test_full_attention.py
â”œâ”€â”€ test_numerics.py     # Comprehensive numeric validation
â”œâ”€â”€ test_bf16_baseline.py
â”œâ”€â”€ debug_harness.py     # Debug test suite
â””â”€â”€ asm_validator.py     # Static analysis
```
