# FP8 Flash Attention Kernel Progress

## Current Status: FULL HEAD_DIM=128 WORKING (v4)

### Completed Steps

1. ✅ **Basic FP8 kernel structure** (v1)
   - Kernel launch, args parsing, thread mapping

2. ✅ **QK MFMA computation** (v1)
   - Q/K loading from global memory to LDS
   - ds_read_b64_tr_b8 for transpose read
   - v_mfma_f32_32x32x16_fp8_fp8 for 8 K-tiles (K=0..127)
   - Accumulated FP32 results in v[32:47]

3. ✅ **Online softmax** (v1)
   - Row-wise max finding with v_max_f32
   - Exp computation with v_exp_f32
   - Running sum accumulation
   - Final 1/sum normalization

4. ✅ **PV MFMA computation** (v2 - NUMERICALLY CORRECT)
   - V loading: 16 strided flat_load_ubyte per thread
   - LDS layout: K-inner V[D,K] at offset D*32 + K
   - ds_read_b64 for MFMA B operand
   - P→FP8 packing with v_cvt_pk_fp8_f32
   - v_mfma_f32_32x32x16_fp8_fp8 for 2 K-groups (K=0..31)

5. ✅ **Output normalization and store** (v2)
   - Multiply by 1/sum
   - Store FP32 output to global memory

6. ✅ **Debug store removal** (v3_clean)
   - Removed all debug stores
   - Performance: 4.58us → 3.11us (32% faster)

7. ✅ **D-tile loop for full head_dim=128** (v4)
   - Loops 4x to process D=0..31, 32..63, 64..95, 96..127
   - P→FP8 conversion done once before loop (reused)
   - Performance: 5.15us for full head_dim (better than expected 4×3.11=12.4us)

### Current Kernel Performance

| Kernel | Time | Notes |
|--------|------|-------|
| fwd_hd128_fp8_v3.s | 4.58 us | With debug stores, D=0..31 only |
| fwd_hd128_fp8_v3_clean.s | 3.11 us | No debug stores, D=0..31 only |
| fwd_hd128_fp8_v4.s | 5.15 us | Full head_dim=128, D-tile loop |

### Numerical Accuracy (Verified)

**v3_clean (D=0..31):**
- Controlled V pattern: ✓ Expected 15.5, Got 15.5
- All zeros V: ✓ Expected 0.0, Got 0.0  
- Constant V=1: ✓ Expected 1.0, Got 1.0
- PyTorch reference: ✓ Max error 0.36

**v4 (full head_dim=128):**
- All 4 D-tiles: ✓ Controlled pattern 15.5
- PyTorch reference per D-tile: max_err 0.33-0.48, mean_err ~0.10

### Next Steps

1. **Multi-wave support** (ANALYZING)
   - Currently single wave (64 threads)
   - Need multiple waves for larger sequences

2. **Performance optimization**
   - Coalesced V loading (replace strided loads)
   - Better LDS utilization
   - Instruction scheduling

## BF16 Multi-Wave Architecture Analysis

### Thread Organization (from fwd_hd128_bf16.co disassembly)

```assembly
v_lshrrev_b32_e32 v3, 6, v0      // wave_id = tid >> 6
v_and_b32_e32 v0, 63, v0         // lane_id = tid & 63
v_readfirstlane_b32 s5, v3       // s5 = wave index for conditionals
```

### Key Characteristics

| Property | BF16 Kernel | FP8 v4 (current) |
|----------|-------------|------------------|
| Max threads | 512 (8 waves) | 64 (1 wave) |
| VGPRs | 256 | 120 |
| SGPRs | 96 | 50 |
| LDS | 160KB | 32KB |
| Barriers | 20 | few |

### Work Distribution Pattern

1. **Data Loading**:
   - Waves 0-3 (s5 < 4): Load V to LDS
   - Waves 4-7: Skip loading via `s_cbranch_scc0`
   - Uses `buffer_load_dwordx4 ... offen lds` for direct global→LDS

2. **Computation**:
   - All waves participate in MFMA after s_barrier
   - Each wave processes different Q rows

3. **Conditional Execution**:
   ```assembly
   s_cmp_lt_i32 s5, 4           // if wave_id < 4
   s_cbranch_scc0 skip_load    // skip V loading for waves 4-7
   ```

### FP8 Multi-Wave Options

**Option A: Scale Up Current Design (Simple)**
- Keep 64 threads, add workgroup_id_x for Q-tile parallelism
- Each workgroup processes one Q-tile independently
- Pro: Minimal code change, good for small batch sizes
- Con: Limited parallelism within single attention head

**Option B: 4-Wave Design (256 threads)**
- Wave 0: Q row 0-7 (threads 0-63)
- Wave 1: Q row 8-15 (threads 64-127)
- Wave 2: Q row 16-23 (threads 128-191)
- Wave 3: Q row 24-31 (threads 192-255)
- Pro: Better LDS utilization, more compute density
- Con: More complex coordination

**Option C: 2-Wave Design (128 threads)**
- Wave 0: Q row 0-15 (threads 0-63)
- Wave 1: Q row 16-31 (threads 64-127)
- Middle ground between A and B

### Recommended Approach for FP8

Start with **Option A** (multiple workgroups, 64 threads each):
1. Already working single-wave kernel
2. Launch grid with (num_q_tiles, num_heads, batch_size)
3. Each workgroup independent - no inter-wave coordination needed
4. Good parallelism at batch level

Later optimize with **Option B** for single-batch high-performance

### Files

```
fwd_hd128_fp8_v2.s       - Working kernel with debug stores (reference)
fwd_hd128_fp8_v3.s       - Working kernel with debug stores (copy of v2)
fwd_hd128_fp8_v3_clean.s - D=0..31 only, no debug stores
fwd_hd128_fp8_v4.s       - Full head_dim=128 with D-tile loop
```

### Key Learnings

1. **ds_read_b64_tr_b8** has addressing limitations (128-byte window)
2. **Strided V loads** work but are slow (16 flat_load_ubyte per thread)
3. **K-inner LDS layout** V[D,K] enables simple ds_read_b64 for MFMA
4. **Debug stores** can be safely removed by replacing with s_nop

### Architecture Notes

- Target: AMD gfx950
- MFMA: v_mfma_f32_32x32x16_fp8_fp8 (64 FLOPs per instruction)
- Threads: 64 (single wave)
- LDS: 32KB allocated
- Tile: 32×32 attention with 128-dim heads
