# FP8 Flash Attention Kernel Progress

## Current Status: ❌ NUMERICAL BUGS - NOT WORKING (Jan 9, 2026)

**CRITICAL**: All kernels (v2, v3, v4) have fundamental numerical bugs that were NOT caught by previous testing. Previous claims of "numerically correct" were incorrect.

### Verified Numerical Issues

| Test | Expected | Actual (all versions) | Issue |
|------|----------|----------------------|-------|
| V = 1 (uniform) | 1.0 | 2.0 | 2× factor |
| V[k,:] = k | 15.5 | 31.0 | 2× factor |
| V[:,d] = d | D=15→15 | D=15→0 | Wrong D mapping |

### Issue 1: 2× Output Factor

Both K halves contribute full output instead of half:
- V[k<16,:]=1, V[k>=16,:]=0 → output = 1.0 (should be 0.5)
- V[k<16,:]=0, V[k>=16,:]=1 → output = 1.0 (should be 0.5)

**Root cause**: P values (softmax output) or MFMA reduction is doubled

### Issue 2: Wrong D-Position Mapping

MFMA 32×32 output distribution differs from assumed layout:
- Output store assumes row-segment per thread
- Actual MFMA: each thread owns ONE column (16 rows)

### Issue 3: Incomplete Output Coverage

| Kernel | Nonzero | Expected |
|--------|---------|----------|
| v2/v3 (64 threads) | 1024/8192 (12.5%) | 32×32 = 1024 ✓ |
| v4 (64 threads) | 4096/8192 (50%) | 32×128 = 4096 ✓ |

Coverage is correct, but positions/values are wrong.

### Steps Completed (with bugs)

1. ✅ Basic FP8 kernel structure
2. ✅ QK MFMA computation
3. ✅ Online softmax (but may have bugs)
4. ❌ PV MFMA computation (2× factor bug)
5. ❌ Output store (wrong D mapping)
6. ✅ Debug store removal
7. ❌ D-tile loop (inherits v3 bugs)

### ROOT CAUSE FOUND

**The P operand packing is fundamentally wrong!**

After QK MFMA, thread's v[32:47] contains P[M=0..15, K=tid%32]:
- 16 values at 16 different M (Q-row) positions
- All at ONE K column

Current kernel packs: `a[0:1] = P[M=0..7, K=tid%32]` (M dimension)
But PV MFMA needs: `A[M, K=0..15]` (K dimension)

**This packs M as if it were K, causing wrong computation!**

### Fix Found (from BF16 kernel analysis)

**BF16 computes V × P (not P × V):**
- A operand = V^T (transposed V, read with ds_read_b64_tr_b16)
- B operand = P^T (QK MFMA output is ALREADY in correct layout!)
- Output = O^T[D, Q] which is transposed in final store

**Why this works:**
After QK MFMA, thread t has P[Q_base:Q_base+16, K=t%32]:
- For P×V (current FP8): A needs P[single_M, K_range] - WRONG layout
- For V×P (BF16 style): B needs P^T[K, Q_range] = P[Q_range, single_K] - CORRECT!

**FP8 kernel changes needed:**
1. Swap PV MFMA operand order: V as A operand, P as B operand
2. Keep V reading with transpose (ds_read_b64_tr_b8)
3. Fix output store: O[Q=t%32, D_base:D_base+16] (transpose from O^T)

No P redistribution needed!

### Next Steps

1. ~~Create minimal test verifying V×P approach with FP8 MFMA~~ ✓
2. Modify fwd_hd128_fp8_v3_clean.s to use V×P order
3. Fix output store pattern for transposed output
4. Verify numerics with random data

### Key Discovery: MFMA Output Interleaving (Jan 9, 2026)

**Minimal V×P Test Results:**
- With V=1, P=1, K=16: Output = 16.0 (correct!)
- V×P MFMA itself works correctly

**MFMA 32×32×16 Output Distribution:**
Thread t (t < 64) owns 16 output M-rows at N column t%32.
The M-rows are NOT contiguous! They are interleaved:

- Threads 0-31: M rows 0,1,2,3, 8,9,10,11, ... (even 4-groups)
- Threads 32-63: M rows 4,5,6,7, 12,13,14,15, ... (odd 4-groups)

**V-by-D Test Results:**
- V=D/32, P=1, K=16 → Expected O[Q,D] = D/2
- Position 0-3: Got D/2 for D=0,1,2,3 ✓
- Position 4-7: Got D/2 for D=8,9,10,11 ✗ (should be D=4,5,6,7)

**Solution: permlane32_swap**
BF16 kernel uses v_permlane32_swap_b32_e32 to exchange values between
threads 0-31 and 32-63, reordering the interleaved output before storing.

For FP8 kernel, need:
```asm
v_permlane32_swap_b32_e32 v48, v52  // Swap rows 0-3↔8-11 between lane groups
v_permlane32_swap_b32_e32 v49, v53
v_permlane32_swap_b32_e32 v50, v54
v_permlane32_swap_b32_e32 v51, v55
// Similar for remaining 8 values
```

After swap:
- Threads 0-31 have D=0-7 (consecutive) in v48-v55
- Threads 32-63 have D=8-15 (consecutive) in v48-v55

Store pattern:
- Threads 0-31: O[Q=t, D=0..7]
- Threads 32-63: O[Q=t-32, D=8..15]

## BF16 Multi-Wave Architecture Analysis (CORRECTED)

### Actual Configuration (from fmha_fwd.csv)

```
ts_qo=256, ts_kv=32  → 256 threads = 4 waves (NOT 8)
```

### Thread Organization (from fwd_hd128_bf16.co disassembly)

```assembly
v_lshrrev_b32_e32 v3, 6, v0      // wave_id = tid >> 6 (0-3 for 256 threads)
v_and_b32_e32 v0, 63, v0         // lane_id = tid & 63
v_readfirstlane_b32 s5, v3       // s5 = wave index (broadcast to SGPR)
```

### Key Characteristics

| Property | BF16 Kernel | FP8 v4 (current) |
|----------|-------------|------------------|
| Threads | 256 (4 waves) | 64 (1 wave) |
| VGPRs | 256 | 120 |
| SGPRs | 96 | 50 |
| LDS | 160KB | 32KB |
| Barriers | 20 | few |
| MFMA outputs | v[32:47]..v[144:159] (8 sets) | v[48:63] (1 set) |

### Work Distribution: Each Wave Processes Different Q Rows

**Wave index (s5) determines LDS offset for each wave**:
```assembly
s_mul_i32 s40, s5, s50       // wave_id * row_stride
s_mul_i32 s63, 0x408, s5     // wave_id * 1032 bytes (Q LDS offset)
s_mul_i32 s64, 0x410, s5     // wave_id * 1040 bytes (K LDS offset)
```

**4-wave work split (256 threads processing 64 Q rows)**:
- Wave 0 (s5=0): Q rows 0-15, LDS offset 0
- Wave 1 (s5=1): Q rows 16-31, LDS offset +1032
- Wave 2 (s5=2): Q rows 32-47, LDS offset +2064
- Wave 3 (s5=3): Q rows 48-63, LDS offset +3096

### Data Loading Strategy

**ALL 4 waves load cooperatively** (not half-and-half):
1. Each wave loads its portion of Q, K, V data to LDS
2. s_barrier synchronizes all waves
3. All waves then do MFMA on their respective Q rows

**The `s_cmp_lt_i32 s5, 4` check**:
```assembly
s_cmp_lt_i32 s5, 4           // if wave_id < 4
s_cbranch_scc0 label_08A3    // branch if wave_id >= 4
```
This exists for potential 8-wave support, but with 256 threads (4 waves),
s5 is always 0-3, so the branch is NEVER taken. All 4 waves execute same path.

### FP8 Multi-Wave Options

**Option A: Multiple Workgroups (Current Design)**
- Keep 64 threads per workgroup
- Launch grid=(num_q_tiles, num_heads, batch_size)
- Pro: Already working, simple
- Con: Less efficient for single-head/small-batch

**Option B: 4-Wave Design (Match BF16)**
- 256 threads = 4 waves
- Each wave: 8 Q rows (current single wave does 32)
- Cooperative loading: all waves contribute to LDS
- Pro: Better occupancy, match BF16 parallelism
- Con: Requires code restructuring

**Option C: 2-Wave Design (Middle Ground)**
- 128 threads = 2 waves  
- Each wave: 16 Q rows
- Simpler than 4-wave, more parallel than 1-wave

### Recommended Implementation Plan

1. **Phase 1**: Keep current v4 (64 threads), verify multi-workgroup launch works
2. **Phase 2**: Implement 2-wave (128 threads) for better parallelism
3. **Phase 3**: If needed, scale to 4-wave (256 threads) for max performance

### Files

```
fwd_hd128_fp8_v2.s       - Working kernel with debug stores (reference)
fwd_hd128_fp8_v3.s       - Working kernel with debug stores (copy of v2)
fwd_hd128_fp8_v3_clean.s - D=0..31 only, no debug stores
fwd_hd128_fp8_v4.s       - Full head_dim=128 with D-tile loop (64 Q rows)
fwd_hd128_fp8_v5.s       - 4-wave implementation (128 Q rows, 256 threads)
```

### Key Learnings

1. **ds_read_b64_tr_b8** has addressing limitations (128-byte window)
2. **Strided V loads** work but are slow (16 flat_load_ubyte per thread)
3. **K-inner LDS layout** V[D,K] enables simple ds_read_b64 for MFMA
4. **Debug stores** can be safely removed by replacing with s_nop

### Architecture Notes

- Target: AMD gfx950
- MFMA: v_mfma_f32_32x32x16_fp8_fp8 (64 FLOPs per instruction)
- Threads: 64 (single wave)
- LDS: 32KB allocated
- Tile: 32×32 attention with 128-dim heads
