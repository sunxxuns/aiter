# FP8 Flash Attention Kernel Progress

## Current Status: FULL HEAD_DIM=128 WORKING (v4)

### Completed Steps

1. ✅ **Basic FP8 kernel structure** (v1)
   - Kernel launch, args parsing, thread mapping

2. ✅ **QK MFMA computation** (v1)
   - Q/K loading from global memory to LDS
   - ds_read_b64_tr_b8 for transpose read
   - v_mfma_f32_32x32x16_fp8_fp8 for 8 K-tiles (K=0..127)
   - Accumulated FP32 results in v[32:47]

3. ✅ **Online softmax** (v1)
   - Row-wise max finding with v_max_f32
   - Exp computation with v_exp_f32
   - Running sum accumulation
   - Final 1/sum normalization

4. ✅ **PV MFMA computation** (v2 - NUMERICALLY CORRECT)
   - V loading: 16 strided flat_load_ubyte per thread
   - LDS layout: K-inner V[D,K] at offset D*32 + K
   - ds_read_b64 for MFMA B operand
   - P→FP8 packing with v_cvt_pk_fp8_f32
   - v_mfma_f32_32x32x16_fp8_fp8 for 2 K-groups (K=0..31)

5. ✅ **Output normalization and store** (v2)
   - Multiply by 1/sum
   - Store FP32 output to global memory

6. ✅ **Debug store removal** (v3_clean)
   - Removed all debug stores
   - Performance: 4.58us → 3.11us (32% faster)

7. ✅ **D-tile loop for full head_dim=128** (v4)
   - Loops 4x to process D=0..31, 32..63, 64..95, 96..127
   - P→FP8 conversion done once before loop (reused)
   - Performance: 5.15us for full head_dim (better than expected 4×3.11=12.4us)

### Current Kernel Performance

| Kernel | Time | Notes |
|--------|------|-------|
| fwd_hd128_fp8_v3.s | 4.58 us | With debug stores, D=0..31 only |
| fwd_hd128_fp8_v3_clean.s | 3.11 us | No debug stores, D=0..31 only |
| fwd_hd128_fp8_v4.s | 5.15 us | Full head_dim=128, D-tile loop |

### Numerical Accuracy (Verified)

**v3_clean (D=0..31):**
- Controlled V pattern: ✓ Expected 15.5, Got 15.5
- All zeros V: ✓ Expected 0.0, Got 0.0  
- Constant V=1: ✓ Expected 1.0, Got 1.0
- PyTorch reference: ✓ Max error 0.36

**v4 (full head_dim=128):**
- All 4 D-tiles: ✓ Controlled pattern 15.5
- PyTorch reference per D-tile: max_err 0.33-0.48, mean_err ~0.10

### Next Steps

1. **Multi-wave support** (ANALYZING)
   - Currently single wave (64 threads)
   - Need multiple waves for larger sequences

2. **Performance optimization**
   - Coalesced V loading (replace strided loads)
   - Better LDS utilization
   - Instruction scheduling

## BF16 Multi-Wave Architecture Analysis (CORRECTED)

### Actual Configuration (from fmha_fwd.csv)

```
ts_qo=256, ts_kv=32  → 256 threads = 4 waves (NOT 8)
```

### Thread Organization (from fwd_hd128_bf16.co disassembly)

```assembly
v_lshrrev_b32_e32 v3, 6, v0      // wave_id = tid >> 6 (0-3 for 256 threads)
v_and_b32_e32 v0, 63, v0         // lane_id = tid & 63
v_readfirstlane_b32 s5, v3       // s5 = wave index (broadcast to SGPR)
```

### Key Characteristics

| Property | BF16 Kernel | FP8 v4 (current) |
|----------|-------------|------------------|
| Threads | 256 (4 waves) | 64 (1 wave) |
| VGPRs | 256 | 120 |
| SGPRs | 96 | 50 |
| LDS | 160KB | 32KB |
| Barriers | 20 | few |
| MFMA outputs | v[32:47]..v[144:159] (8 sets) | v[48:63] (1 set) |

### Work Distribution: Each Wave Processes Different Q Rows

**Wave index (s5) determines LDS offset for each wave**:
```assembly
s_mul_i32 s40, s5, s50       // wave_id * row_stride
s_mul_i32 s63, 0x408, s5     // wave_id * 1032 bytes (Q LDS offset)
s_mul_i32 s64, 0x410, s5     // wave_id * 1040 bytes (K LDS offset)
```

**4-wave work split (256 threads processing 64 Q rows)**:
- Wave 0 (s5=0): Q rows 0-15, LDS offset 0
- Wave 1 (s5=1): Q rows 16-31, LDS offset +1032
- Wave 2 (s5=2): Q rows 32-47, LDS offset +2064
- Wave 3 (s5=3): Q rows 48-63, LDS offset +3096

### Data Loading Strategy

**ALL 4 waves load cooperatively** (not half-and-half):
1. Each wave loads its portion of Q, K, V data to LDS
2. s_barrier synchronizes all waves
3. All waves then do MFMA on their respective Q rows

**The `s_cmp_lt_i32 s5, 4` check**:
```assembly
s_cmp_lt_i32 s5, 4           // if wave_id < 4
s_cbranch_scc0 label_08A3    // branch if wave_id >= 4
```
This exists for potential 8-wave support, but with 256 threads (4 waves),
s5 is always 0-3, so the branch is NEVER taken. All 4 waves execute same path.

### FP8 Multi-Wave Options

**Option A: Multiple Workgroups (Current Design)**
- Keep 64 threads per workgroup
- Launch grid=(num_q_tiles, num_heads, batch_size)
- Pro: Already working, simple
- Con: Less efficient for single-head/small-batch

**Option B: 4-Wave Design (Match BF16)**
- 256 threads = 4 waves
- Each wave: 8 Q rows (current single wave does 32)
- Cooperative loading: all waves contribute to LDS
- Pro: Better occupancy, match BF16 parallelism
- Con: Requires code restructuring

**Option C: 2-Wave Design (Middle Ground)**
- 128 threads = 2 waves  
- Each wave: 16 Q rows
- Simpler than 4-wave, more parallel than 1-wave

### Recommended Implementation Plan

1. **Phase 1**: Keep current v4 (64 threads), verify multi-workgroup launch works
2. **Phase 2**: Implement 2-wave (128 threads) for better parallelism
3. **Phase 3**: If needed, scale to 4-wave (256 threads) for max performance

### Files

```
fwd_hd128_fp8_v2.s       - Working kernel with debug stores (reference)
fwd_hd128_fp8_v3.s       - Working kernel with debug stores (copy of v2)
fwd_hd128_fp8_v3_clean.s - D=0..31 only, no debug stores
fwd_hd128_fp8_v4.s       - Full head_dim=128 with D-tile loop
```

### Key Learnings

1. **ds_read_b64_tr_b8** has addressing limitations (128-byte window)
2. **Strided V loads** work but are slow (16 flat_load_ubyte per thread)
3. **K-inner LDS layout** V[D,K] enables simple ds_read_b64 for MFMA
4. **Debug stores** can be safely removed by replacing with s_nop

### Architecture Notes

- Target: AMD gfx950
- MFMA: v_mfma_f32_32x32x16_fp8_fp8 (64 FLOPs per instruction)
- Threads: 64 (single wave)
- LDS: 32KB allocated
- Tile: 32×32 attention with 128-dim heads
