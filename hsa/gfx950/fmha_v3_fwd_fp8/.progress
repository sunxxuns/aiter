# FP8 Flash Attention Kernel Progress

## Current Status: ❌ NUMERICAL BUGS - NOT WORKING (Jan 9, 2026)

**CRITICAL**: All kernels (v2, v3, v4) have fundamental numerical bugs that were NOT caught by previous testing. Previous claims of "numerically correct" were incorrect.

### Verified Numerical Issues

| Test | Expected | Actual (all versions) | Issue |
|------|----------|----------------------|-------|
| V = 1 (uniform) | 1.0 | 2.0 | 2× factor |
| V[k,:] = k | 15.5 | 31.0 | 2× factor |
| V[:,d] = d | D=15→15 | D=15→0 | Wrong D mapping |

### Issue 1: 2× Output Factor

Both K halves contribute full output instead of half:
- V[k<16,:]=1, V[k>=16,:]=0 → output = 1.0 (should be 0.5)
- V[k<16,:]=0, V[k>=16,:]=1 → output = 1.0 (should be 0.5)

**Root cause**: P values (softmax output) or MFMA reduction is doubled

### Issue 2: Wrong D-Position Mapping

MFMA 32×32 output distribution differs from assumed layout:
- Output store assumes row-segment per thread
- Actual MFMA: each thread owns ONE column (16 rows)

### Issue 3: Incomplete Output Coverage

| Kernel | Nonzero | Expected |
|--------|---------|----------|
| v2/v3 (64 threads) | 1024/8192 (12.5%) | 32×32 = 1024 ✓ |
| v4 (64 threads) | 4096/8192 (50%) | 32×128 = 4096 ✓ |

Coverage is correct, but positions/values are wrong.

### Steps Completed (with bugs)

1. ✅ Basic FP8 kernel structure
2. ✅ QK MFMA computation
3. ✅ Online softmax (but may have bugs)
4. ❌ PV MFMA computation (2× factor bug)
5. ❌ Output store (wrong D mapping)
6. ✅ Debug store removal
7. ❌ D-tile loop (inherits v3 bugs)

### ROOT CAUSE FOUND

**The P operand packing is fundamentally wrong!**

After QK MFMA, thread's v[32:47] contains P[M=0..15, K=tid%32]:
- 16 values at 16 different M (Q-row) positions
- All at ONE K column

Current kernel packs: `a[0:1] = P[M=0..7, K=tid%32]` (M dimension)
But PV MFMA needs: `A[M, K=0..15]` (K dimension)

**This packs M as if it were K, causing wrong computation!**

### Fix Required

**Option A: Cross-thread P gather**
- Use LDS to redistribute P values
- Each thread writes P[M_range, K=own] to LDS[M_range * 32 + K=own]
- Then reads P[M=own, K_range] from LDS[M=own * 32 + K_range]
- Requires barrier and extra LDS space

**Option B: Restructure QK MFMA output**
- Store QK to LDS in a layout where rows are contiguous
- Read back with each thread getting one row's K values
- Then softmax and PV work on correct data

**Option C: Study BF16 kernel**
- The BF16 kernel must handle this correctly
- Analyze how it transforms QK output before PV

### Next Steps

1. Study BF16 kernel's P handling between QK and PV MFMAs
2. Implement P redistribution via LDS
3. Verify with controlled tests

## BF16 Multi-Wave Architecture Analysis (CORRECTED)

### Actual Configuration (from fmha_fwd.csv)

```
ts_qo=256, ts_kv=32  → 256 threads = 4 waves (NOT 8)
```

### Thread Organization (from fwd_hd128_bf16.co disassembly)

```assembly
v_lshrrev_b32_e32 v3, 6, v0      // wave_id = tid >> 6 (0-3 for 256 threads)
v_and_b32_e32 v0, 63, v0         // lane_id = tid & 63
v_readfirstlane_b32 s5, v3       // s5 = wave index (broadcast to SGPR)
```

### Key Characteristics

| Property | BF16 Kernel | FP8 v4 (current) |
|----------|-------------|------------------|
| Threads | 256 (4 waves) | 64 (1 wave) |
| VGPRs | 256 | 120 |
| SGPRs | 96 | 50 |
| LDS | 160KB | 32KB |
| Barriers | 20 | few |
| MFMA outputs | v[32:47]..v[144:159] (8 sets) | v[48:63] (1 set) |

### Work Distribution: Each Wave Processes Different Q Rows

**Wave index (s5) determines LDS offset for each wave**:
```assembly
s_mul_i32 s40, s5, s50       // wave_id * row_stride
s_mul_i32 s63, 0x408, s5     // wave_id * 1032 bytes (Q LDS offset)
s_mul_i32 s64, 0x410, s5     // wave_id * 1040 bytes (K LDS offset)
```

**4-wave work split (256 threads processing 64 Q rows)**:
- Wave 0 (s5=0): Q rows 0-15, LDS offset 0
- Wave 1 (s5=1): Q rows 16-31, LDS offset +1032
- Wave 2 (s5=2): Q rows 32-47, LDS offset +2064
- Wave 3 (s5=3): Q rows 48-63, LDS offset +3096

### Data Loading Strategy

**ALL 4 waves load cooperatively** (not half-and-half):
1. Each wave loads its portion of Q, K, V data to LDS
2. s_barrier synchronizes all waves
3. All waves then do MFMA on their respective Q rows

**The `s_cmp_lt_i32 s5, 4` check**:
```assembly
s_cmp_lt_i32 s5, 4           // if wave_id < 4
s_cbranch_scc0 label_08A3    // branch if wave_id >= 4
```
This exists for potential 8-wave support, but with 256 threads (4 waves),
s5 is always 0-3, so the branch is NEVER taken. All 4 waves execute same path.

### FP8 Multi-Wave Options

**Option A: Multiple Workgroups (Current Design)**
- Keep 64 threads per workgroup
- Launch grid=(num_q_tiles, num_heads, batch_size)
- Pro: Already working, simple
- Con: Less efficient for single-head/small-batch

**Option B: 4-Wave Design (Match BF16)**
- 256 threads = 4 waves
- Each wave: 8 Q rows (current single wave does 32)
- Cooperative loading: all waves contribute to LDS
- Pro: Better occupancy, match BF16 parallelism
- Con: Requires code restructuring

**Option C: 2-Wave Design (Middle Ground)**
- 128 threads = 2 waves  
- Each wave: 16 Q rows
- Simpler than 4-wave, more parallel than 1-wave

### Recommended Implementation Plan

1. **Phase 1**: Keep current v4 (64 threads), verify multi-workgroup launch works
2. **Phase 2**: Implement 2-wave (128 threads) for better parallelism
3. **Phase 3**: If needed, scale to 4-wave (256 threads) for max performance

### Files

```
fwd_hd128_fp8_v2.s       - Working kernel with debug stores (reference)
fwd_hd128_fp8_v3.s       - Working kernel with debug stores (copy of v2)
fwd_hd128_fp8_v3_clean.s - D=0..31 only, no debug stores
fwd_hd128_fp8_v4.s       - Full head_dim=128 with D-tile loop (64 Q rows)
fwd_hd128_fp8_v5.s       - 4-wave implementation (128 Q rows, 256 threads)
```

### Key Learnings

1. **ds_read_b64_tr_b8** has addressing limitations (128-byte window)
2. **Strided V loads** work but are slow (16 flat_load_ubyte per thread)
3. **K-inner LDS layout** V[D,K] enables simple ds_read_b64 for MFMA
4. **Debug stores** can be safely removed by replacing with s_nop

### Architecture Notes

- Target: AMD gfx950
- MFMA: v_mfma_f32_32x32x16_fp8_fp8 (64 FLOPs per instruction)
- Threads: 64 (single wave)
- LDS: 32KB allocated
- Tile: 32×32 attention with 128-dim heads
