# FP8 Flash Attention - BF16 Comparison

## Reference Kernel
`hsa/gfx950/fmha_v3_fwd/fwd_hd128_bf16.s`

---

## Key Patterns from BF16 (Must Follow)

### 1. Buffer Descriptor Setup
```asm
// BF16 pattern
s_load_dwordx2 s[8:9], s[0:1], 0x08   // Load base address
s_mov_b32 s10, 4096                    // Size
s_mov_b32 s11, 0x20000                 // Flags (offen mode)
```
**FP8 Status**: ✅ Same pattern used

### 2. K-tile Offset Management
```asm
// BF16 K-loop pattern
s_mov_b32 s34, 0                       // K offset starts at 0
s_mul_i32 s43, 64, s47                 // K stride = 64 * stride_param
...
buffer_load_dwordx4 v4, s[12:15], s34 offen lds
s_add_i32 s34, s43, s34                // Advance to next tile
```
**FP8 Status**: ✅ Same pattern
- `s27` = K offset (starts at 0)
- `s26` = K stride (4096 bytes = 32 rows × 128 cols × 1 byte)
- `s_add_i32 s27, s27, s26` advances offset

### 3. Online Softmax State
```asm
// BF16 maintains across K-tiles:
v_mov_b32_e32 v24, 0xff7fffff          // running_max = -large
v_mov_b32_e32 v16, 0                   // running_sum = 0
// v[96:159] = O accumulator
```
**FP8 Status**: ✅ Same pattern
- `v70` = running_max (init to 0xff800000 = -inf)
- `v71` = running_sum (init to 0)
- `v[80:143]` = O accumulator (all 128 HD cols, 4 tiles of 16 regs each)

### 4. Correction Factor
```asm
// When new tile has larger max:
v_sub_f32_e32 v16, v_old_max, v_new_max
v_mul_f32_e32 v16, s37, v16            // * log2(e)/sqrt(d)
v_exp_f32_e32 v16, v16                 // correction = exp(...)
// Apply to O accumulator:
v_mul_f32_e32 v110, v16, v110
```
**FP8 Status**: ✅ Same pattern
```asm
v_sub_f32_e32 v23, v70, v22            // old_max - new_max
v_mul_f32_e32 v23, s2, v23             // * scale
v_exp_f32_e32 v23, v23                 // correction
v_mul_f32_e32 v80, v80, v23            // rescale O
```

### 5. V1 Recalculation Before K Load
**Critical Fix**: BF16 kernel sets VGPR offset before each buffer_load block.
FP8 must recalculate `v1 = tid * 16` before K load in loop:
```asm
K_TILE_LOOP:
    v_lshlrev_b32_e32 v1, 4, v0        // MUST recalculate!
    buffer_load_dwordx4 v1, s[12:15], s20 offen lds
```

---

## FP8 Implementation Status

### Same as BF16 ✅
| Feature | BF16 | FP8 |
|---------|------|-----|
| S^T = K @ Q^T | ✅ | ✅ |
| Row-wise softmax | VGPR sum + permlane32_swap | Same |
| Buffer descriptor | s10=size, s11=0x20000 | Same |
| Scalar offset | buffer_load with soff | Same |
| Online softmax | running_max, running_sum | Same |
| O rescaling | correction * O | Same |
| Final norm | O / running_sum | Same |

### Different from BF16
| Aspect | BF16 | FP8 |
|--------|------|-----|
| MFMA | `v_mfma_f32_32x32x16_bf16` | `v_mfma_f32_32x32x16_fp8_fp8` |
| Operand size | 4 VGPRs (8 BF16) | 2 VGPRs (8 FP8) |
| Element size | 2 bytes | 1 byte |
| FP8 conversion | N/A | `v_cvt_pk_fp8_f32` before PV MFMA |
| P storage | Direct to MFMA | F32 → LDS → FP8 |

---

## Discovered Bugs (Fixed)

### 1. `.args` Metadata Required
FP8 kernels MUST include explicit `.args:` section in `.amdgpu_metadata`:
```yaml
.amdgpu_metadata
---
amdhsa.kernels:
  - .name: kernel_name
    .args:
      - {.name: ptr_O, .size: 8, .offset: 0, .value_kind: global_buffer, .address_space: global}
      - {.name: ptr_Q, .size: 8, .offset: 8, .value_kind: global_buffer, .address_space: global}
      ...
```
**Symptom**: Memory fault at address (nil)
**Cause**: Without `.args`, HIP runtime can't pass kernel arguments correctly
**Note**: BF16 kernel has this from original compilation

### 2. VGPR Offset Must Be Recalculated (Critical Bug)
The `v1` register (per-thread offset for buffer_load) must be recalculated before each K-tile load inside the loop.
```asm
K_TILE_LOOP:
    v_lshlrev_b32_e32 v1, 4, v0        // MUST recalculate here!
    buffer_load_dwordx4 v1, s[12:15], s20 offen lds
```
**Symptom**: Output is all zeros (computation completely wrong, not precision issue)
**Cause**: v1 was set once for Q load, became stale by K load time
**Fix**: Add `v_lshlrev_b32_e32 v1, 4, v0` at start of K_TILE_LOOP

### 3. Buffer Descriptor Size Must Be Max (Critical Bug)
The buffer descriptor size field must be -1 (max) for multi-tile access with scalar offsets.
```asm
// WRONG - limits access to first 4096 bytes
s_mov_b32 s14, 4096       // K buffer size
s_mov_b32 s18, 4096       // V buffer size

// CORRECT - allows full buffer access
s_mov_b32 s14, -1         // K buffer size = max
s_mov_b32 s18, -1         // V buffer size = max
```
**Symptom**: Only tile 0 data used; output is 1/N of expected for N tiles
**Cause**: With size=4096, `s27=4096` offset for tile 1 exceeds bounds → returns 0
**Fix**: Set buffer descriptor size to -1 (max) for K and V

### 4. F32/FP8 Running Sum Consideration
- `running_sum` computed from F32 P values
- `O` computed from FP8 P @ FP8 V
- Small precision difference in final `O / running_sum`
**Result**: ~0.09-0.18 max error depending on tile count

### 5. AGPR Write Timing for Multi-HD-Tile (Critical Bug)
When doing multiple HD tiles per K-pass, AGPR writes must happen AFTER V packing, just before MFMA.
```asm
// WRONG - AGPR write before V read causes stale data
v_accvgpr_write_b32 a0, v28     // Write P to AGPRs
v_accvgpr_write_b32 a1, v29
s_nop 1
// ... V read and packing (modifies v30) ...
v_mfma_f32_32x32x16_fp8_fp8 v[80:95], a[0:1], v[30:31], v[80:95]

// CORRECT - AGPR write after V packing, before MFMA
// ... V read and packing (v30 = V data) ...
v_accvgpr_write_b32 a0, v28     // Write P to AGPRs
v_accvgpr_write_b32 a1, v29
s_nop 1
v_mfma_f32_32x32x16_fp8_fp8 v[80:95], a[0:1], v[30:31], v[80:95]
```
**Symptom**: First HD tile (cols 0-31) gives NaN/wrong values, other tiles work
**Cause**: v30 used as temp in P conversion, then overwritten by V packing - order matters
**Fix**: Keep original BF16 pattern - AGPR write immediately before MFMA

---

## LDS Layout Comparison

| Region | BF16 (~64KB) | FP8 64T (12KB) | FP8 256T (32KB) |
|--------|--------------|----------------|-----------------|
| Q | offset 0 | offset 0 (4KB) | offset 0 (4KB) |
| K | offset ~8KB | offset 4096 | offset 4096 (4KB) |
| V | per-tile | offset 8192 | offset 8192 (4KB) |
| P | in registers | offset 4096 | offset 12288 (4KB) |
| Scratch | - | - | offset 16384+ |

---

## Phase 6: 256-Thread Architecture

### BF16 Reference (256 threads = 4 waves)
```asm
// Thread decomposition
v_lshrrev_b32_e32 v1, 6, v0    // wave_id = thread / 64
v_and_b32_e32 v0, 63, v0       // lane_id = thread % 64

// BF16 uses wave_id to select output rows
// Each wave: 8 Q-rows (32 total / 4 waves)
```

### FP8 256T Implementation Plan

#### Step 1: Thread Decomposition
```asm
// Save in v1, v0
v_lshrrev_b32_e32 v1, 6, v0    // v1 = wave_id (0-3)
v_and_b32_e32 v0, 63, v0       // v0 = lane_id (0-63)
v_lshlrev_b32_e32 v2, 3, v1    // v2 = wave_id * 8 = q_row_base
```

#### Step 2: Q Load (all waves cooperate)
```asm
// 256 threads × 16 bytes = 4KB = one Q tile
// thread_id = wave*64 + lane → offset = thread_id * 16
v_lshlrev_b32_e32 v3, 6, v1    // wave * 64
v_add_u32_e32 v3, v0, v3       // + lane = thread_id
v_lshlrev_b32_e32 v3, 4, v3    // × 16 bytes
buffer_load_dwordx4 v3, s[8:11], 0 offen lds
```

#### Step 3: K Load (all waves cooperate)
Same as Q load, different destination:
```asm
v_add_u32_e32 v4, 4096, v3     // LDS offset for K
buffer_load_dwordx4 v4, s[12:15], s27 offen lds
```

#### Step 4: QK MFMA (each wave handles 8 Q-rows)
```asm
// Wave's Q starts at: wave_id * 8 * 128 = wave_id * 1024
// For 8 Q-rows, need smaller tile or multiple passes

// Option A: 8x32 output per wave (requires adjustment)
// Option B: Each wave does full 32x32, selects its 8 rows
// Following BF16: use MFMA 32x32, each wave gets 8 rows of result
```

#### Step 5: Softmax (per-wave with cross-wave reduction)
```asm
// Each wave computes max/sum for its 8 rows
// Need cross-wave reduction for global max
v_permlane32_swap_b32_e32 v_max, v_max  // Within wave
// Cross-wave via LDS or shared accumulator
```

#### Step 6: PV MFMA (each wave outputs 8 rows)
Similar structure to QK.

### Resources Required

| Resource | FP8 64T | FP8 256T | BF16 256T |
|----------|---------|----------|-----------|
| Threads | 64 | 256 | 256 |
| LDS | 12KB | 32KB | 64KB |
| VGPRs | 148 | ~200 | 256 |
| AGPRs | 4 | ~16 | 96 |
| Waves | 1 | 4 | 4 |

### Key Differences from 64T

1. **Thread ID**: `v0` is lane (0-63), `v1` is wave (0-3)
2. **Output rows**: Each wave handles 8 rows, not 32
3. **LDS access**: Coordinate to avoid conflicts
4. **Reduction**: Need cross-wave max/sum

---

## Test Results (Full HD=128 Output)

```
seq_len=32  (1 tile):  max_error=0.088 ✅ (all 128 columns)
seq_len=64  (2 tiles): max_error=0.073 ✅ (all 128 columns)
seq_len=96  (3 tiles): max_error=0.060 ✅ (all 128 columns)
seq_len=128 (4 tiles): max_error=0.066 ✅ (all 128 columns)
seq_len=256 (8 tiles): max_error=0.043 ✅ (debug harness)
```

Note: Multi-tile errors generally DECREASE with more tiles (proper accumulation).

---

## Debug Tools

### 1. Assembly Validator (`asm_validator.py`)
Static analysis to detect common issues:
```bash
python asm_validator.py fwd_fp8_kloop.s
```
Detects:
- Buffer descriptor size limits
- VGPR not recalculated in loops
- Missing barriers

### 2. Runtime Parameter Validator
Pre-launch check for buffer overflow:
```python
from asm_validator import validate_kernel_params
validate_kernel_params(seq_len=128, k_buffer_size=4096)  # Will warn!
```

### 3. Debug Harness (`debug_harness.py`)
Comprehensive kernel testing:
```bash
python debug_harness.py fwd_fp8_kloop.s
```
Tests:
- V=1 identity (output should be 1.0)
- Tile isolation (each tile contributes)
- Accumulation check (sum of isolated = full)
- Element ratio consistency (detects addressing bugs)

### 4. BF16 Baseline Test (`test_bf16_baseline.py`)
Validates testing methodology against known-working BF16 kernel:
```bash
python test_bf16_baseline.py
```

**Comparison (BF16 vs FP8):**
| Metric | BF16 | FP8 | Notes |
|--------|------|-----|-------|
| V=1 identity | 1.0000 | ~1.01 | FP8 quantization |
| Reference max_err | 0.001 | 0.03-0.09 | Expected precision diff |
| Ratio consistency std | 0.003 | 0.18-0.27 | FP8 noise |

BF16 passes all tests → validates our methodology is correct
