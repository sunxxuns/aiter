# FP8 Flash Attention Scalability Analysis

## Target
- BF16 baseline: ~1000 TF/s @ seq=32130, H=40
- FP8 target: >1300 TF/s (30% improvement)

## Current Status: ❌ NOT SCALABLE

### Performance Gap (Single Head, seq=32128)
| Metric | FP8 | BF16 | Gap |
|--------|-----|------|-----|
| Time | 4.91ms | 0.74ms | 6.6x slower |
| TF/s | 108.7 | 717.2 | 0.15x |

### Resource Utilization Analysis

#### 1. Thread Count: ❌ SEVERELY UNDERUTILIZED
| Aspect | FP8 (current) | BF16 (reference) | Issue |
|--------|---------------|------------------|-------|
| Threads/WG | 64 | 256 | 4x fewer |
| Waves/WG | 1 | 4 | 4x fewer |
| Occupancy | ~25% | ~100% | 4x lower |

**Impact**: GPU compute units are 75% idle.

#### 2. Memory Bandwidth: ❌ NOT HIDDEN
| Aspect | FP8 (current) | BF16 (reference) |
|--------|---------------|------------------|
| Prefetching | None | K/V overlap |
| Double buffering | No | Yes |
| Memory latency | Exposed | Hidden |

**Impact**: Waiting for memory instead of computing.

#### 3. MFMA Utilization: ❌ POOR SCHEDULING
| Aspect | FP8 (current) | BF16 (reference) |
|--------|---------------|------------------|
| MFMA latency | 64 cycles | 64 cycles |
| Latency hiding | None | Pipelined |
| Ops between MFMA | Few | Many |

**Impact**: MFMA units idle waiting for data.

#### 4. LDS Efficiency: ⚠️ SUBOPTIMAL
| Aspect | FP8 (current) | BF16 (reference) |
|--------|---------------|------------------|
| Bank conflicts | Likely | Minimized |
| Swizzling | Basic | Optimized |
| Double buffer | No | Yes |

### Scaling Analysis

```
SeqLen   FP8 TF/s   BF16 TF/s   FP8/BF16   us/Q-tile
------------------------------------------------------
  1024       5.1       12.7      0.40x        3.3
  4096      20.9       89.5      0.23x        3.2
 16384      71.3      426.1      0.17x        3.8
 32128     108.7      717.2      0.15x        4.9
```

**Observation**: FP8/BF16 ratio DECREASES with size (0.40x → 0.15x)
**Cause**: BF16 scales better due to better resource utilization

### Theoretical Analysis

#### FP8 Should Be Faster Because:
1. FP8 MFMA: 2x throughput vs BF16 MFMA
2. FP8 data: 0.5x memory bandwidth (1 byte vs 2 bytes)

#### FP8 Is Slower Because:
1. 4x fewer threads → 4x lower occupancy
2. No prefetching → memory latency exposed
3. No pipelining → MFMA units idle
4. Sequential K-loop → no overlap

### Projected Performance After Optimization

If we fix resource utilization:
```
Current FP8:     108.7 TF/s
Fix occupancy:   108.7 × 4 = ~435 TF/s (4x improvement)
Add prefetch:    435 × 1.5 = ~650 TF/s (hide latency)
FP8 MFMA boost:  650 × 1.5 = ~975 TF/s (FP8 compute advantage)
With scheduling: 975 × 1.3 = ~1270 TF/s
```

Target 1300 TF/s is **achievable** with proper optimization.

## Required Changes for Scalability

### Phase 6: Occupancy (Critical)
- [ ] Increase to 256 threads (4 waves)
- [ ] Adjust LDS layout for 4 waves
- [ ] Update register allocation

### Phase 7: Prefetching
- [ ] Double-buffer K/V in LDS
- [ ] Overlap load of tile N+1 with compute of tile N
- [ ] Async memory operations

### Phase 8: Pipelining
- [ ] Interleave MFMA with memory ops
- [ ] Hide 64-cycle MFMA latency
- [ ] Instruction scheduling optimization

### Phase 9: LDS Optimization
- [ ] Bank conflict analysis
- [ ] Swizzle pattern optimization
- [ ] Minimize LDS traffic

## Conclusion

**Current kernel is NOT scalable to target.**

Root cause: Fundamental resource underutilization (25% occupancy).
Solution: Must increase threads/workgroup to 256 before other optimizations matter.

The correctness is proven, but architecture needs redesign for performance.
