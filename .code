# CODE MODEL STATUS - Updated 2025-01-16

## CRITICAL: PREVIOUS BENCHMARKS ARE INVALID!

**BUG**: All previous benchmarks launched only 1 block instead of full grid!
- The "1617 TF/s" number is WRONG - based on single block timing
- Single block doesn't represent real workload

**MUST FIX**: Re-benchmark with correct grid size:
```python
grid_x = math.ceil(seq_len / Q_rows_per_block)  # Q-tile blocks  
grid_y = batch * num_heads                       # batch × heads
# Example: B=1, H=40, S=32130, Q_rows=128 → grid=(252, 40, 1) = 10,080 blocks
```

---

## CURRENT BEST: fwd_fp8_qk_4qtile_v2.s
- **Performance**: **NEEDS RE-BENCHMARK** (1617 TF/s was single-block, invalid)
- **Numerics**: NEEDS VERIFICATION with random input
- **Layout**: pitch-136 (bank-conflict-free)
- **Approach**: 4 waves, each handles 1 Q-tile (32 rows)

## MAJOR BREAKTHROUGH: Fixed Wave Redundancy Bug

### The Bug (in fwd_fp8_qk_4qtile.s)
All 4 waves were computing IDENTICAL MFMAs:
- v200 = tid % 64 (wave_tid) used for MFMA addressing
- All waves had same v200 values (0-63)
- 3 out of 4 waves wasted doing redundant work

### The Fix (in fwd_fp8_qk_4qtile_v2.s)
Each wave computes a different Q-tile:
- Wave 0 → Q-tile 0 (rows 0-31)
- Wave 1 → Q-tile 1 (rows 32-63)
- Wave 2 → Q-tile 2 (rows 64-95)
- Wave 3 → Q-tile 3 (rows 96-127)

Result: **4x speedup** (405 → 1617 TF/s)

## PERFORMANCE COMPARISON (CORRECTED)
| Kernel | TF/s | Notes |
|--------|------|-------|
| fwd_fp8_qk_4qtile_v11 | **386** | Multi-block verified |
| BF16 full attention | ~1000 | Reference |
| Triton FP8 full attention | ~1300 | Target |

**Previous claims of 1617 TF/s were INVALID (single-block benchmark)**

## KEY RESULT: FP8 QK at 386 TF/s (multi-block verified)

**Real performance**: 386 TF/s with proper multi-block grid
**Target**: 1300+ TF/s (Triton FP8 full attention)
**Gap**: 3.4x improvement needed

**Root causes of slow performance:**
1. Using K=16 MFMA instead of K=64 MFMA (2x loss)
2. No k-loop (only 1 K-tile per block)
3. Potential bank conflicts
4. No pipelining

## ARCHITECTURE DETAILS

### Wave-to-Row Mapping
- 256 threads per block (4 waves × 64 threads)
- Each wave handles 32 Q rows (1 Q-tile)
- 128 Q rows total per block

### VGPR Allocation (per wave)
- v[0:15]: Output accumulator (16 VGPRs)
- v[16:31]: Pre-loaded Q data (16 VGPRs)
- v[32:47]: K data from LDS (16 VGPRs)
- v[180+]: Addresses and temps

### LDS Layout (pitch-136)
- Q_LDS_0: 0 (rows 0-31)
- Q_LDS_1: 4352 (rows 32-63)
- Q_LDS_2: 8704 (rows 64-95)
- Q_LDS_3: 13056 (rows 96-127)
- K_LDS_A: 17408 (double-buffered)
- K_LDS_B: 21760
- Total: 28672 bytes

## BENCHMARK RESULTS (CORRECT MULTI-BLOCK - 2025-01-16)

**Using bench_v11.py with grid=(252, 1, 1) for S=32130:**

| Config | Time (ms) | TF/s | Notes |
|--------|-----------|------|-------|
| Single head, S=32130 | 0.702 | **376.3** | 252 blocks |
| H=40 heads, S=32130 | 27.39 | **385.9** | Sequential launches |

**Comparison:**
| Kernel | TF/s | Status |
|--------|------|--------|
| Our FP8 QK-only | 386 | Multi-block verified |
| BF16 full attention | ~1000 | Reference |
| Triton FP8 full attention | ~1300 | Target |

**Gap analysis**: We're at 386 TF/s, need 1300+ TF/s (3.4x improvement needed)

## NEXT STEPS

### Priority 1: Add softmax + PV for full attention
- QK stage is now fast enough
- Need online softmax (exp, max, sum)
- Need P×V MFMA stage
- Then can fairly compare with BF16 full attention

### Priority 2: Consider 8-Q-tile variant
- 8 Q-tiles = 256 rows per block
- Would need on-demand Q loading (not pre-loaded)
- Tested: slower than 4-Q-tile due to LDS latency

### Priority 3: buffer_load...lds optimization
- BF16 uses direct global→LDS loads
- Would reduce instruction count
- Complex m0 setup required

## FILES REFERENCE
- **Best kernel**: `fwd_fp8_qk_4qtile_v2.s` (1617 TF/s, correct)
- BF16 reference: `fwd_hd128_bf16.s` (987 TF/s)
- Buggy version: `fwd_fp8_qk_4qtile.s` (redundant waves)
- 8-Q-tile: `fwd_fp8_qk_8qtile.s` (slower, numeric issues)

## COMMIT
- `0156e9b5e`: FP8 FMHA: Fix wave redundancy bug, achieve 1617 TF/s
