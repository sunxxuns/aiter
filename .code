# CODE MODEL STATUS - Updated 2025-01-16

## CURRENT APPROACH: Adapt Triton's FP8 Flash Attention Assembly

### Decision
Start from Triton's working 1298 TF/s kernel and clean it up for our use.

### Source File
- **Input**: `triton_fp8_fmha.s` (2073 lines, dumped from Triton cache)
- **Output**: `fwd_fp8_triton_based.s` (cleaned up, standalone kernel)

---

## TRITON FP8 ANALYSIS (1298 TF/s at B=1, H=40, S=32130)

### Key Differences from Our Kernel

| Aspect | Triton (1298 TF/s) | Our v2 (386 TF/s) |
|--------|-------------------|-------------------|
| **MFMA** | `v_mfma_f32_32x32x64_f8f6f4` (K=64) | `v_mfma_f32_32x32x16_fp8_fp8` (K=16) |
| **Efficiency** | **2K FLOPs/cycle** | 1K FLOPs/cycle |
| **LDS read** | `ds_read_b128` (128-bit) | `ds_read_b64` (64-bit) |
| **V transpose** | `ds_read_b64_tr_b8` | N/A |
| **Full attention** | Yes (QK + softmax + PV) | QK only |

### Critical Insight
Triton's 2x speedup comes from **K=64 MFMA instruction**, not scheduling.
- `v_mfma_f32_32x32x64_f8f6f4`: 128K FLOPs in 64 cycles = 2K FLOPs/cycle
- `v_mfma_f32_32x32x16_fp8_fp8`: 32K FLOPs in 32 cycles = 1K FLOPs/cycle

### Triton Assembly Patterns

**MFMA (8 occurrences in main loop):**
```asm
v_mfma_f32_32x32x64_f8f6f4 v[82:97], v[66:73], v[122:129], v[98:113]
v_mfma_f32_32x32x64_f8f6f4 v[66:81], v[160:167], v[122:129], v[98:113]
```

**LDS Reads:**
```asm
ds_read_b128 v[118:121], v5          ; 128-bit read for QK
ds_read_b64_tr_b8 v[74:75], v145     ; transposed read for PV
```

**LDS Writes:**
```asm
ds_write_b128 v142, v[4:7]           ; 128-bit write
```

---

## PROGRESS (2025-01-16)

### K=64 MFMA Verified Working
- Created `test_mfma_k64.s` - minimal test for `v_mfma_f32_32x32x64_f8f6f4`
- **RESULT**: All-ones test passes (64 FP8 ones × 64 FP8 ones = 64.0)
- K=64 MFMA operand layout confirmed: 8 VGPRs per operand (64 FP8 elements)

### Triton Assembly Dumped
- File: `triton_fp8_fmha.s` (2073 lines)
- Key patterns identified:
  - Uses `ds_read_b128` (128-bit LDS reads)
  - Uses `ds_read_b64_tr_b8` for transposed reads (V operand)
  - QK uses 4 MFMAs, PV uses 4 MFMAs per iteration

### MFMA K=64 Operand Layout
```
v_mfma_f32_32x32x64_f8f6f4 D[0:15], A[0:7], B[0:7], C[0:15]
- A[32,64]: 32 rows × 64 cols, 8 VGPRs
- B[32,64]: 32 rows × 64 cols (transposed), 8 VGPRs  
- C/D[32,32]: 32 rows × 32 cols, 16 VGPRs
- Each lane L holds row (L % 32)'s data
- 64 FP8 packed: 8 FP8 per VGPR
```

## TRITON HSACO DIRECT CALL - BLOCKED (verified 2025-01-16)

Multiple attempts to call Triton's compiled HSACO directly all failed with segfaults:

**Why it fails:**
1. Triton generates specialized launcher code (`__triton_launcher.*.so`)
2. Argument marshaling includes extra bookkeeping beyond visible metadata
3. N_CTX is a constexpr - baked into kernel at compile time
   - kernarg_size=120 bytes but Python passes 124 bytes worth of args
   - The kernel expects N_CTX=1024 specifically (from assembly dump)
4. Dynamic LDS allocation not matching runtime expectations

**Attempted fixes:**
- Different argument layouts (4 variations)
- Different LDS sizes (0, 16KB, 24KB, 32KB, 64KB)
- Matching exact tensor shapes from assembly dump (1024 seq_len)
- All resulted in segmentation faults

**Decision**: Don't try to call Triton HSACO directly. Instead:
1. Use Triton through Python for reference (1289 TF/s verified)
2. Build our own kernel applying Triton's key optimizations

## CURRENT STATUS

### K=64 MFMA QK Kernel - WORKING (single K-tile)
- `fwd_fp8_k64_v3.co` correctly computes Q[32,128] × K[32,128]^T
- MFMA values are correct (verified with sorted value comparison)  
- Output layout is permuted (not row-major), needs permlane fix
- Single tile: 128.0 for all-ones ✓

### K-tile Loop - BLOCKED (deep debugging needed)
Multiple K-tiles crash even with:
- Scalar offset (soffset) approach
- VGPR offset approach  
- Pointer update approach
- Fully unrolled code with separate descriptors

Key finding: Shifting K pointer from Python WORKS, but any in-kernel
offset modification causes crashes. This suggests:
1. Buffer descriptor format might need special handling for 64-thread parallel access
2. OR there's an alignment/page boundary issue with the K tensor
3. OR buffer_load with non-zero offset behaves differently than expected on gfx950

BF16 reference uses `buffer_load ... lds` (direct to LDS) pattern which we haven't tried.

## KEY FINDINGS

1. **Single K-tile MFMA works** - fwd_fp8_k64_v3.co produces correct values
2. **Python pointer shifting works** - can process multiple K-tiles by shifting K_ptr from Python
3. **In-kernel offset modification crashes** - any offset >= 4096 causes memory access fault
4. **Even buffer_load ... lds crashes** - the issue is NOT specific to the load pattern

This suggests either:
- gfx950-specific behavior with buffer descriptors  
- Some alignment/page boundary issue
- A fundamental misunderstanding of the instruction behavior

## RECOMMENDED PATH FORWARD

### Option 1: Focus on Full Pipeline First (RECOMMENDED)
1. Use single-tile kernels to implement:
   - Softmax (max, exp, sum reduction)
   - P×V multiplication with transposed V
2. Validate end-to-end numerics with small sequences
3. Return to K-loop optimization later

### Option 2: Deep Debug K-loop
1. Study BF16 kernel's exact buffer descriptor setup
2. Check if `buffer_load ... lds` requires specific m0/descriptor patterns
3. Test with hipcc-generated code to compare

### Option 3: Use Triton as Reference Implementation
- Triton achieves 1298 TF/s with its K-loop
- Focus on understanding WHY Triton works but our asm doesn't
- May require studying Triton's LLVM IR → asm translation

---

## BENCHMARK TARGETS

| Metric | Current | Target |
|--------|---------|--------|
| TF/s (QK only) | 386 | N/A |
| TF/s (full attention) | N/A | 1300+ |
| vs BF16 | 0.39x | 1.3x |
| vs Triton FP8 | 0.30x | 1.0x |

---

## FILES

- `triton_fp8_fmha.s` - Dumped Triton assembly (source)
- `fwd_fp8_triton_based.s` - Our cleaned up version (target)
- `bench_triton_target.py` - Benchmark script for Triton
- `dump_triton_asm.py` - Script to dump Triton assembly

---

## VERIFIED PERFORMANCE (2025-01-16)

**Triton FP8 Flash Attention:**
```
B=1, H=40, S=32130, D=128: 1298.6 TF/s (FP8), 651.9 TF/s (FP16)
```

**Our kernel (v11 multi-block):**
```
H=40 heads, S=32130: 385.9 TF/s
```
