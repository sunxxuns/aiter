# Lessons Learned

## ds_read_b64_tr_b8 / ds_read_b64_tr_b16 Transpose Read Instructions (GFX950)

### Key Discovery: Base Address Scaling

The transpose read instructions use **scaled base addresses**:

| Instruction | Element Size | Base Scaling | Stride |
|------------|--------------|--------------|--------|
| `ds_read_b64_tr_b8` (FP8) | 1 byte | base/8 = byte position | 16 bytes |
| `ds_read_b64_tr_b16` (BF16) | 2 bytes | base/8 = element position | 32 bytes |

**Critical**: `base = D * 8` to read from byte/element position D

### FP8 ds_read_b64_tr_b8 Pattern

```
base = n * 8  →  reads from byte position n with stride 16
```

**Example with LDS[i] = i:**
```
base=0   → reads positions 0, 16, 32, 48, 64, 80, 96, 112  → values [0, 16, 32, ...]
base=8   → reads positions 1, 17, 33, 49, 65, 81, 97, 113  → values [1, 17, 33, ...]
base=16  → reads positions 2, 18, 34, 50, 66, 82, 98, 114  → values [2, 18, 34, ...]
```

### Required LDS Layout for MFMA V Operand

**For FP8 (16-byte rows):**
```
LDS[K*16 + D] = V[K, D]   where K=row, D=column (0..15)
```

**Reading pattern:**
```assembly
v_and_b32_e32 v6, 15, v0           // D = lane % 16
v_lshlrev_b32_e32 v6, 3, v6        // base = D * 8 (REQUIRED SCALING!)
ds_read_b64_tr_b8 v[result], v6    // Lane n gets V[K=0..7, D=n]
```

### Common Mistakes to Avoid

1. **Forgetting base scaling**: `base = D` does NOT work, must use `base = D * 8`
2. **Wrong LDS row size**: Must match stride (16 bytes for FP8, 32 bytes for BF16)
3. **Assuming per-lane addressing**: With same base for all lanes, only reads from ONE row repeatedly
4. **Confusing with regular ds_read**: tr_b8/tr_b16 have fixed stride, not configurable

### BF16 ds_read_b64_tr_b16 Pattern

Similar but with 32-byte stride and 2-byte elements:
```
base = n * 8  →  reads from BF16 element n with stride 16 elements (32 bytes)
```

**LDS Layout:**
```
LDS[K*32 + D*2] = V[K, D]   where K=row, D=column (0..15), 2 bytes per BF16
```

### Test Files Reference

- `hsa/gfx950/fmha_v3_fwd_fp8/test_tr_fp8.s` - FP8 transpose read test
- `hsa/gfx950/fmha_v3_fwd_fp8/test_tr_base.s` - BF16 transpose read test  
- `hsa/gfx950/fmha_v3_fwd_fp8/test_v_load_fp8.s` - Working FP8 V loading with tr_b8
- `hsa/gfx950/fmha_v3_fwd_fp8/TR_INVESTIGATION.md` - Detailed investigation notes

### Verified Working Code

```assembly
// FP8 V Loading with ds_read_b64_tr_b8
// LDS layout: 16-byte rows, V[K, D] at LDS[K*16 + D]

// Load V to LDS (16-byte rows)
v_lshrrev_b32_e32 v1, 1, v0           // K = tid / 2
v_and_b32_e32 v2, 1, v0               // d_half = tid % 2
v_lshlrev_b32_e32 v3, 3, v2           // d_offset = d_half * 8
// ... load from global to v[20:21] ...
v_lshlrev_b32_e32 v5, 4, v1           // K * 16
v_add_u32_e32 v5, v3, v5              // K*16 + d_offset
ds_write_b64 v5, v[20:21]

s_barrier

// Read with transpose - CRITICAL: base = D * 8
v_and_b32_e32 v6, 15, v0              // D = lane % 16
v_lshlrev_b32_e32 v6, 3, v6           // base = D * 8 (SCALING!)
ds_read_b64_tr_b8 v[30:31], v6        // Gets V[K=0..7, D=lane%16]
```

**Result**: Each lane correctly gets 8 K values at its D position!

### LDS Offset Limitation for tr_b8

**CRITICAL**: `ds_read_b64_tr_b8` has complex addressing that limits its use:

1. **Base scaling**: base = position * 8, address = base/8 + i*16 for i=0..7
2. **Range limitation**: For larger byte positions (e.g., 128+), base = 1024+ may cause incorrect addressing
3. **Observed behavior**: base=1024 (for position=128) reads from LDS[1024] directly instead of LDS[128]

**Working range**: Only positions 0..127 (base 0..1016) are reliable for accessing a 128-byte window.

**For K=0..7**: Works correctly with base = D * 8 (D = column 0..15)
**For K=8..15**: Unreliable; falls back to reading wrong addresses

**Solutions**:
1. Use regular `ds_read_b64` with K-inner LDS layout (V[D, K])
2. For FP8 with 32 K values needed, use 16 strided loads as fallback
3. Structure LDS to keep each transpose-read window within 128 bytes

### Debug Store Removal

Debug stores are NOT needed for correctness - they're purely for debugging. Earlier issues with removal were due to script bugs, not actual dependencies.

**Correct removal method**:
Replace each debug block with a minimal placeholder (e.g., `s_nop 7`) to preserve code structure:

```python
# Replace debug block with s_nop while preserving blank lines
if '// DEBUG:' in line and 'Store output' not in line:
    result.append('    s_nop 7\n')
    result.append('\n')
    # Skip to after s_waitcnt vmcnt(0)
```

**Performance impact**:
- With debug stores: 4.58 us
- Without debug stores: 3.63 us (21% faster)

**Common mistakes**:
1. Regex removing extra blank lines, breaking section structure
2. Accidentally removing the final output store (labeled as DEBUG)

## FP8 Flash Attention Kernel Implementation

### Working Kernel: `fwd_hd128_fp8_v2.s`

**Key Components**:
- 64 threads (single wave) processing 32×32 attention tiles
- Online softmax with streaming max/sum computation
- FP8 MFMA `v_mfma_f32_32x32x16_fp8_fp8` for QK and PV computations

**V Loading Strategy (Current - Strided Loads)**:
```assembly
// 16 strided byte loads per thread
flat_load_ubyte v72, v[10:11]          // V[K=0, D=d]
flat_load_ubyte v73, v[10:11] offset:128  // V[K=1, D=d]
// ... 14 more loads with stride 128
```

This is slow but numerically correct. Each thread loads one D column, 16 K values.

**LDS Layout for V (K-inner)**:
```
V[D, K] at offset D*32 + K
ds_read_b64 at D*32 gives consecutive K values for MFMA B operand
```

### MFMA Operand Mapping (32x32x16 FP8)

**A operand (P values)**:
- Pack FP32 softmax P into FP8 with `v_cvt_pk_fp8_f32`
- Write to AGPRs `a[0:3]` (4 registers × 4 FP8 = 16 values)

**B operand (V values)**:
- Lanes 0-31: V[K=0..7, D=lane]
- Lanes 32-63: V[K=8..15, D=lane-32]
- Read with `ds_read_b64` from K-inner LDS layout

### Debug Store Removal Warning

When removing debug stores, be VERY careful not to remove:
1. The actual output store (may be labeled "DEBUG: Store output accumulators")
2. Essential computation code between debug blocks

**Safe removal strategy**:
1. Comment out debug blocks first (don't delete)
2. Test after each removal
3. Only delete after verification

### ds_read_b64_tr_b8 Addressing Details (Further Investigation)

**Without base*8 scaling**:
- The instruction reads the same byte 8 times (no stride)
- Example: base=128 reads LDS[128] eight times → [80, 80, 80, 80, 80, 80, 80, 80]

**With base*8 scaling**:
- Stride of 16 is enabled
- But for base >= 1024, the address appears to be used directly
- Example: base=1024 (position=128) reads from LDS[1024] (K data region) instead of LDS[128] (V data region)

**Practical 128-byte window**:
- ds_read_b64_tr_b8 reliably accesses 8 values × 16-byte stride = 128 bytes total
- For K=0..7: positions 0..112 (base 0..896) work correctly
- For K=8..15: positions 128..240 (base 1024..1920) show incorrect behavior

**Workaround for K>7**:
The current working kernel uses strided `flat_load_ubyte` for V loading (16 loads per thread) with K-inner LDS layout and regular `ds_read_b64` for reading, avoiding the tr_b8 limitation entirely.

## FP8 Flash Attention: Known Numerical Issues (Jan 9, 2026)

**CRITICAL**: All FP8 kernels (v2, v3, v4) have fundamental numerical bugs. Previous commit claims of "numerically correct" were NOT verified properly.

### Issue 1: 2× Output Factor

**Symptom**: Output is consistently 2× expected value
- V = 1 (uniform): output = 2.0 (expected 1.0)
- V[k,:] = k: output = 31.0 (expected 15.5)

**Root Cause Analysis** (in progress):
- Both PV MFMAs (K=0..15 and K=16..31) produce full output instead of half each
- Test: V[k<16,:]=1, V[k>=16,:]=0 gives 1.0 (should be 0.5)
- Test: V[k<16,:]=0, V[k>=16,:]=1 gives 1.0 (should be 0.5)
- This confirms both K ranges contribute full (not half) output

**Possible Causes**:
1. P values (softmax) are not split correctly for K=0..15 vs K=16..31
2. Each MFMA uses the same P values instead of different K slices
3. Softmax sum is computed incorrectly (sum=16 instead of 32)

### Issue 2: Wrong D-Position Mapping

**Symptom**: V[:,d]=d does not produce output[:,d]=d
- D=0 → 0.0 (correct)
- D=15 → 0.0 (expected 15.0)
- D=31 → 2.0 (expected 31.0)

**Root Cause**:
The MFMA 32×32 output distribution differs from assumed layout:
- Each thread's 16 outputs are NOT at consecutive N columns
- Output store pattern assumes wrong thread-to-position mapping

**MFMA 32×32×16 Output Distribution** (needs verification):
```
tid 0: M=0..15, N=0 (single column, 16 rows)
tid 1: M=0..15, N=1
...
tid 31: M=0..15, N=31
tid 32: M=16..31, N=0
...
tid 63: M=16..31, N=31
```

This means each thread writes ONE column (16 M rows), not a row segment.

### Issue 3: Incomplete Output Coverage

**Observed**:
- v2/v3: 1024/8192 nonzero (12.5%) - single 32×32 D-tile
- v4: 4096/8192 nonzero (50%) - full head_dim but wrong values

### Test Methodology

Use these tests to verify numerical correctness:
```python
# Test 1: V=1 should give output=1.0
# Test 2: V[k,:]=k should give output=15.5 (mean of 0..31)
# Test 3: V[:,d]=d should give output[:,d]=d
# Test 4: Random V comparison with PyTorch reference (correlation)
```

### ROOT CAUSE IDENTIFIED (Jan 9, 2026)

**The 2× factor is caused by incorrect P operand packing for PV MFMA!**

After QK MFMA, each thread's v[32:47] contains:
- 16 P values for 16 different M (Q-row) positions
- At ONE K (attention) column: K = tid % 32

The current kernel packs these directly:
```assembly
a[0] = pack(v32, v33, v34, v35)  // P[M=0..3, K=tid%32]
a[1] = pack(v36, v37, v38, v39)  // P[M=4..7, K=tid%32]
```

But PV MFMA A operand needs:
- P[one_M_row, K=0..15] - the same M row, varying K columns

**The kernel is packing the M dimension as if it were K dimension!**

This explains:
- Why V=1 gives 2×: the "wrong K" packing still sums correctly with uniform values
- Why V[k,:]=k gives 31 instead of 15.5: only the largest K value (31) dominates
- Why both K ranges (K=0..15 and K=16..31) give full output

**THE CORRECT FIX** (discovered from BF16 kernel analysis):

BF16 kernel solves this by computing **V × P** instead of P × V:
- A operand = V^T (read with ds_read_b64_tr_b16)
- B operand = P^T (QK MFMA output is ALREADY in correct layout!)
- Output = O^T[D, Q] which is transposed in final store

After QK MFMA, thread t has P[Q_base:Q_base+16, K=t%32]:
- For P×V: A needs P[single_M, K_range] - WRONG, we have P[M_range, single_K]
- For V×P: B needs P^T[K, Q_range] = P[Q_range, single_K] - CORRECT!

**FP8 MFMA Instruction Format:**
```
v_mfma_f32_32x32x16_fp8_fp8 vdst, src_a, src_b, vsrc_c
- src_a: AGPRs a[N:N+1] (2 dwords = 8 FP8 values)
- src_b: VGPRs v[M:M+1] (2 dwords = 8 FP8 values)
```

**FP8 kernel FIX steps:**
1. **Read V → AGPRs** (instead of VGPRs):
   - Load V from LDS with transpose pattern
   - Pack V FP8 values into 2 dwords
   - Use `v_accvgpr_write_b32` to move to AGPRs
   
2. **Keep P in VGPRs** (don't move to AGPRs):
   - After softmax, P is in v[32:47]
   - Pack P FP8 values and keep in VGPRs
   - Use as B operand directly

3. **Fix MFMA operand order:**
   - Current: `v_mfma a[P], v[V]` = P × V (WRONG)
   - Fixed: `v_mfma a[V], v[P]` = V × P (CORRECT)

4. **Fix output store (transpose):**
   - Thread t owns O^T[D_base:D_base+16, Q=t%32]
   - After transpose: O[Q=t%32, D_base:D_base+16]
   - Offset = (t%32)*512 + (t//32)*64 (for head_dim=128)

No P redistribution needed - P values are already in correct B operand layout!

**BF16 Kernel Verification (Jan 9, 2026)**:
Tested BF16 kernel with non-uniform inputs to confirm V×P approach:
- uniform_v (V=1): Output exactly 1.0 ✓
- v_by_k (V[k,:]=k): K-weighted average works (correlation 0.9999) ✓
- v_by_d (V[:,d]=d): D position perfectly preserved (correlation 1.0) ✓
- random: Very high correlation (0.999998) ✓

The v_by_d test is critical - proves D dimension is correctly mapped through V×P.

**FP8 V×P Implementation Findings (Jan 9, 2026)**:

1. **Minimal V×P Test PASSED**: With V=1, P=1, K=16 → Output = 16.0 ✓
   - The basic V×P MFMA mechanics work correctly
   - FP8 packing/MFMA/output fundamentals are sound

2. **MFMA Output Interleaving Discovered**:
   - MFMA 32×32×16 output is NOT contiguous M-rows
   - Threads 0-31: M rows 0,1,2,3, 8,9,10,11, 16,17,18,19, 24,25,26,27
   - Threads 32-63: M rows 4,5,6,7, 12,13,14,15, 20,21,22,23, 28,29,30,31
   - Solution: Use v_permlane32_swap_b32_e32 to reorder before storing

3. **P REDISTRIBUTION REQUIRED for non-uniform attention**:
   After QK MFMA: Thread t has P[Q_base:Q_base+16, K=t%32]
   For V×P B operand: Thread t needs P[Q=t%32, K_range=(t/32)*8:+8]
   
   The layouts are TRANSPOSED:
   - Have: 16 Q values at single K column
   - Need: 8 K values at single Q row
   
   Must write P to LDS and re-read with transposed access pattern:
   - Write: P[Q_row, K_col] at LDS_P_OFFSET + Q_row*32 + K_col
   - Read: P[Q=t%32, K_range] at LDS_P_OFFSET + (t%32)*32 + (t/32)*8

4. **Why uniform P works without redistribution**:
   When all P values are identical, the transposed read gives same values.
   This is why minimal tests with P=1 passed.

5. **Rigorous MFMA Testing (Jan 9, 2026)**:
   Created tests with non-uniform inputs to reveal bugs:
   
   a) K-sum test (A=1, B=K) → Output = 120 ✓
      - Confirms K reduction works correctly
      - Confirms FP8 values 0-15 are exact
   
   b) M-index test (A=M, B=1) → Reveals FP8 precision
      - M values 17,19,21,23,25,27,29,31 round to even in FP8 e4m3
      - MFMA output interleaving confirmed: 0,1,2,3,8,9,10,11,...
   
   c) P×V attention test (P=1/16, V=d) → Output = d ✓
      - D values correctly propagated through MFMA
      - Basic P×V mechanics work

6. **MFMA 32x32x16 Complete Layout**:
   
   A operand (32M × 16K):
   - Thread t provides A[M=t%32, K=(t/32)*8:(t/32)*8+8]
   - Threads 0-31: K=0..7 for M rows 0-31
   - Threads 32-63: K=8..15 for M rows 0-31
   
   B operand (16K × 32N):
   - Thread t provides B[K=(t/32)*8:(t/32)*8+8, N=t%32]
   - Threads 0-31: K=0..7 for N cols 0-31
   - Threads 32-63: K=8..15 for N cols 0-31
   
   Output (32M × 32N):
   - Thread t owns C[(t/32)*16:(t/32)*16+16 INTERLEAVED, N=t%32]
   - Interleaved rows: 0,1,2,3,8,9,10,11,16,17,18,19,24,25,26,27 for threads 0-31
   
   **The Critical Mismatch**:
   After QK MFMA, thread t has P[Q_rows (interleaved), K=t%32]
   For PV MFMA A operand, thread 0 needs P[Q=0, K=0..7]
   But P[Q=0, K=0..7] is spread across threads 0,1,2,3,4,5,6,7's position 0!
   
   **Redistribution IS required** - no way around it.

### Lessons

1. **Never trust commit messages** about numerical correctness without running actual tests
2. **Test with controlled patterns** (uniform, varying by K, varying by D) before random
3. **Understand MFMA output layout** before implementing output store
4. **Cross-verify K reduction**: test with K-dependent V values
5. **MFMA operand layout mismatch**: QK output gives P[M_range, single_K], but PV needs P[single_M, K_range]
