# FP8 Flash Attention Assembly Kernel - Lessons Learned

## Critical Discoveries (Priority Order)

### 1. FP8 FORMAT: MFMA Uses e4m3fn (OCP), NOT e4m3fnuz!

**THE MOST CRITICAL FINDING**: Using the wrong FP8 format causes 2× output values!

| Value    | e4m3fnuz byte | e4m3fn (OCP) byte |
|----------|---------------|-------------------|
| 0.015625 | 16            | 8                 |
| 0.03125  | 24            | 16                |
| 0.0625   | 32            | 24                |
| 0.125    | 40            | 32                |

**Rules**:
- `v_cvt_pk_fp8_f32` produces OCP (e4m3fn) format bytes
- In PyTorch: use `torch.float8_e4m3fn`, NOT `torch.float8_e4m3fnuz`
- If loading FP8 from global memory, ensure it's stored in OCP format

**Test**: `test_pack_debug.s` compares manual packing vs `v_cvt_pk_fp8_f32`

---

### 2. P Redistribution is REQUIRED for PV MFMA

After QK MFMA, thread t has:
- `P[Q_rows_interleaved, K=t%32]` - 16 Q values at ONE K column

For PV MFMA A operand, thread t needs:
- `P[Q=t%32, K_range]` - ONE Q value at 8 K columns

**The layouts are TRANSPOSED** - must redistribute via LDS:

```assembly
// Write P to LDS: P[Q, K] at offset Q*32 + K
v_and_b32_e32 v4, 31, v0              // K = tid % 32
v_add_u32_e32 v5, LDS_P_OFFSET, v4
ds_write_b8 v5, v3                    // Write for Q=0
ds_write_b8 v5, v3 offset:32          // Write for Q=1
// ... repeat for all Q rows

s_barrier

// Read P for PV MFMA: P[Q=tid%32, K_start..K_start+8]
v_and_b32_e32 v6, 31, v0              // Q = tid % 32
v_lshrrev_b32_e32 v7, 5, v0           // K_group = tid / 32
v_lshlrev_b32_e32 v7, 3, v7           // K_start = K_group * 8
v_lshlrev_b32_e32 v6, 5, v6           // Q * 32
v_add_u32_e32 v6, v6, v7              // + K_start
ds_read_b64 v[32:33], v6              // Read 8 FP8 P values
```

**Why uniform P works without redistribution**: When all P values are identical, the transposed read gives same values.

**Test**: `test_p_redistrib.s` and `test_nonuniform_p.s` verify this pattern.

---

### 3. MFMA 32x32x16 Operand and Output Layout

**A operand (32M × 16K)**:
- Thread t provides `A[M=t%32, K=(t/32)*8:(t/32)*8+8]`
- Threads 0-31: K=0..7 for M rows 0-31
- Threads 32-63: K=8..15 for M rows 0-31

**B operand (16K × 32N)**:
- Thread t provides `B[K=(t/32)*8:(t/32)*8+8, N=t%32]`
- Threads 0-31: K=0..7 for N cols 0-31
- Threads 32-63: K=8..15 for N cols 0-31

**Output (32M × 32N) - INTERLEAVED**:
- Thread t owns `C[M_rows_interleaved, N=t%32]`
- Threads 0-31: M rows 0,1,2,3, 8,9,10,11, 16,17,18,19, 24,25,26,27
- Threads 32-63: M rows 4,5,6,7, 12,13,14,15, 20,21,22,23, 28,29,30,31

**Scatter store pattern** for interleaved output:
```assembly
v_and_b32_e32 v3, 31, v0              // N = tid % 32
v_lshrrev_b32_e32 v4, 5, v0           // M_base = (tid/32)*4
// Store v48-v51 to M_base+0,1,2,3
// Store v52-v55 to M_base+8,9,10,11
// Store v56-v59 to M_base+16,17,18,19
// Store v60-v63 to M_base+24,25,26,27
```

---

### 4. FP8 Precision Limitations

FP8 e4m3 has limited precision for larger values:
- Values 0-16: Exact representation
- Values 17,19,21,23,25,27,29,31: Round to nearest even (17→16, 19→20, etc.)
- Max representable: ~240

**Tests should use values in exact FP8 range** to distinguish algorithm bugs from precision loss.

---

## Verified Test Patterns

| Test | Pattern | Expected | Purpose |
|------|---------|----------|---------|
| `test_mfma_k_sum.s` | A=1, B=K | 120 | K reduction correctness |
| `test_mfma_m_index.s` | A=M, B=1 | M*16 | M-row distribution |
| `test_attention_pv.s` | P=1/16, V=d | d | D value propagation |
| `test_p_redistrib.s` | P=(k+1)/16 via LDS | 8.5 | P redistribution |
| `test_full_attn_v2.s` | P=1/16, V=d/32 | d/32 | Full PV computation |
| `test_nonuniform_p.s` | P=(k+1)/136, V=1 | 0.98 | Non-uniform P |

**All tests use non-uniform inputs** to avoid false positives from uniform data.

---

## ds_read_b64_tr_b8 Transpose Read (Reference)

**Base address scaling**: `base = position * 8`

```assembly
v_and_b32_e32 v6, 15, v0           // D = lane % 16
v_lshlrev_b32_e32 v6, 3, v6        // base = D * 8 (REQUIRED!)
ds_read_b64_tr_b8 v[30:31], v6     // Reads with stride 16
```

**LDS Layout for tr_b8**: 16-byte rows, `V[K, D]` at `LDS[K*16 + D]`

**Limitation**: Reliable only for positions 0-127 (first 128 bytes).

---

## Key Lessons

1. **Always test with non-uniform inputs** - uniform values hide bugs
2. **Verify FP8 format matches MFMA** - e4m3fn (OCP), not e4m3fnuz
3. **MFMA output is interleaved** - not contiguous M rows
4. **P redistribution cannot be avoided** - QK output layout differs from PV input
5. **Use scatter stores** for interleaved output pattern
6. **Check FP8 precision** - some values round, affecting test expectations

---

## File Reference

```
hsa/gfx950/fmha_v3_fwd_fp8/
├── test_mfma_k_sum.s       # K reduction test
├── test_mfma_m_index.s     # M-row distribution test
├── test_attention_pv.s     # Basic P×V test
├── test_p_redistrib.s      # P redistribution via LDS
├── test_full_attn_v2.s     # Full attention with e4m3fn
├── test_nonuniform_p.s     # Non-uniform P values
├── test_pack_debug.s       # FP8 packing comparison
├── test_mfma_2x_debug.s    # 2× factor debugging
├── test_mfma_layouts.py    # Python reference patterns
└── .progress               # Development progress tracking
```

---

### 7. v_cvt_pk_fp8_f32 Leaves Garbage in High 16 Bits

**THE BUG**: `v_cvt_pk_fp8_f32 v_dst, v_src0, v_src1` only writes to the low 16 bits. The high 16 bits contain undefined garbage!

**Symptom**: NaN outputs from MFMA when using converted FP8 values

**Fix**: Always mask after conversion before combining:
```assembly
v_cvt_pk_fp8_f32 v72, v64, v65        // Low 16 bits = valid FP8
v_and_b32_e32 v72, 0xFFFF, v72        // REQUIRED: Clear garbage
v_cvt_pk_fp8_f32 v73, v66, v67
v_lshlrev_b32_e32 v73, 16, v73        // Shift to high 16 bits
v_or_b32_e32 v20, v72, v73            // Safe to combine now
```

**Without mask**: The garbage in v72's high bits gets OR'd with the shifted data.

---

### 8. MFMA Output Layout - DECODED

**For v_mfma_f32_32x32x16_fp8_fp8, the output S[row, col] maps to:**

```
Thread tid, VGPR v:
  col = tid % 32
  row = ((v - 32) % 4) + ((tid // 32) * 4) + ((v - 32) // 4) * 8
```

**Visual Layout (first 16 rows):**
```
row  | col 0-7 threads            | vreg
-----|----------------------------|------
0-3  | threads 0-7 (tid//32=0)    | v32-v35
4-7  | threads 32-39 (tid//32=1)  | v32-v35  
8-11 | threads 0-7 (tid//32=0)    | v36-v39
12-15| threads 32-39 (tid//32=1)  | v36-v39
16-19| threads 0-7                | v40-v43
20-23| threads 32-39              | v40-v43
24-27| threads 0-7                | v44-v47
28-31| threads 32-39              | v44-v47
```

**CORRECT Store Pattern:**
```assembly
// col = tid % 32, row_half = tid // 32
.macro STORE_VREG vreg, row_mod4, row_8_group
    v_mov_b32_e32 v7, \row_mod4
    v_add_u32_e32 v7, v7, v_row_half_x4   // + (tid//32)*4
    v_add_u32_e32 v7, \row_8_group * 8, v7
    v_lshlrev_b32_e32 v7, 5, v7           // row * 32
    v_add_u32_e32 v7, v7, v_col           // + col
    v_lshlrev_b32_e32 v7, 2, v7           // × 4 (byte offset)
    flat_store_dword [ptr_S + v7], vreg
.endm
```

---

### 9. buffer_load→LDS Pattern (Scalable Memory)

**buffer_load_dwordx4 ... lds** writes directly to LDS, bypassing VGPRs:

```assembly
// Setup buffer descriptor s[8:11]
// s[8:9] = base addr, s[10] = size, s[11] = 0x20000 (flags)
s_mov_b32 s11, 0x20000

// Each thread loads 16 bytes from buffer[voffset] to LDS[m0 + tid*16]
v_lshlrev_b32_e32 v1, 4, v0     // voffset = tid * 16
s_mov_b32 m0, LDS_BASE
buffer_load_dwordx4 v1, s[8:11], 0 offen lds

// 64 threads × 16 bytes = 1024 bytes loaded in ONE instruction
s_waitcnt vmcnt(0)
s_barrier

// Read from LDS to VGPRs for MFMA
ds_read_b64 v[20:21], v_lds_addr
```

**Advantages over flat_load:**
- Asynchronous (overlaps with compute)
- Shared loading (one wavefront loads for all threads)
- Hardware addressing (no VGPR address calculation)
- Double-buffer ready

---

### 10. HD=128 Loading: Multiple buffer_load with soffset

For Q[32×128] = 4KB, need 4 wavefront loads (each 1KB):

```assembly
// Setup: voffset fixed at tid*16, use soffset to advance global address
v_lshlrev_b32_e32 v1, 4, v0    // voffset = tid * 16 (fixed)

// Load 0: global[0:1024] → LDS[0:1024]
s_mov_b32 m0, 0
s_mov_b32 s20, 0
buffer_load_dwordx4 v1, s[8:11], s20 offen lds

// Load 1: global[1024:2048] → LDS[1024:2048]
s_mov_b32 m0, 1024
s_mov_b32 s20, 1024
buffer_load_dwordx4 v1, s[8:11], s20 offen lds

// Load 2: global[2048:3072] → LDS[2048:3072]
s_mov_b32 m0, 2048
s_mov_b32 s20, 2048
buffer_load_dwordx4 v1, s[8:11], s20 offen lds

// Load 3: global[3072:4096] → LDS[3072:4096]
s_mov_b32 m0, 3072
s_mov_b32 s20, 3072
buffer_load_dwordx4 v1, s[8:11], s20 offen lds
```

**Key insight**: soffset must be an SGPR, not an immediate constant!

**Result**: Row-major Q[32×128] contiguously in LDS[0:4096]

---

### 11. QK MFMA for HD=128: 8 Passes with Correct Addressing

For S = Q @ K^T with Q[32×128], K[32×128], S[32×32]:

```assembly
// Thread mapping: row = tid % 32, half = tid / 32
v_and_b32_e32 v2, 31, v0              // row = tid % 32
v_lshrrev_b32_e32 v3, 5, v0           // half = tid / 32

// Q base offset: row * 128 (row stride) + half * 8 (k sub-offset)
v_lshlrev_b32_e32 v5, 7, v2           // row * 128
v_lshlrev_b32_e32 v4, 3, v3           // half * 8
v_add_u32_e32 v5, v5, v4              // Q base

// K base offset: same pattern, at LDS offset 4096
v_add_u32_e32 v6, 4096, v5

// 8 MFMA passes, advancing by 16 bytes each (16 FP8 = one K-chunk)
ds_read_b64 v[20:21], v5              // Q[row, 0:8] or Q[row, 8:16]
ds_read_b64 v[22:23], v6              // K[row, 0:8] or K[row, 8:16]
v_mfma_f32_32x32x16_fp8_fp8 v[32:47], a[0:1], v[22:23], v[32:47]

v_add_u32 v7, 16, v5                  // next k-chunk
v_add_u32 v8, 16, v6
// ... repeat for 8 total passes
```

**Tested**: max_err = 0.00007 (test_qk_hd128.py)

