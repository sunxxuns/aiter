# FP8 Flash Attention - Hardware Knowledge

Focus: Verified HW facts that prevent repeated mistakes.
NOT for: Strategy decisions, session notes, unverified hypotheses.

---

## 1. LDS LAYOUT

### 1.1 Bank Conflict Rules
SYMPTOM: Kernel is slow, rocprof shows high SQ_LDS_BANK_CONFLICT
CAUSE: Multiple threads accessing same bank (32 banks, 4 bytes each)
FIX: Use stride that spreads accesses across banks

| Stride | Conflict Factor | Notes |
|--------|-----------------|-------|
| 128    | 16x             | All rows hit same bank (128 = 32×4) |
| 132    | 0x              | Optimal - each row different bank |
| 136    | 0x              | Also optimal |

### 1.2 Stride-132 Formula
```asm
// Write: LDS[row * 132 + col]
// Read:  same formula
v_mov_b32_e32 v2, 132
v_mul_lo_u32 v5, row, v2
v_add_u32_e32 v5, col, v5
```

### 1.3 XOR Swizzle Alternative
```asm
// swizzle = ((offset & 0x1ff) >> 7) << 3
// addr = offset ^ swizzle
v_and_b32_e32 tmp, 0x1ff, offset
v_lshrrev_b32_e32 tmp, 7, tmp
v_lshlrev_b32_e32 tmp, 3, tmp
v_xor_b32_e32 addr, tmp, offset
```

---

## 2. MFMA MAPPING

### 2.1 FP8 MFMA 32x32x16 Operand Layout
**A operand (32M × 16K)**: 2 AGPRs = 8 FP8 values
- Thread t: `A[M=t%32, K=(t/32)*8:(t/32)*8+8]`
- Threads 0-31: K=0..7, threads 32-63: K=8..15

**B operand (16K × 32N)**: 2 VGPRs = 8 FP8 values
- Thread t: `B[K=(t/32)*8:(t/32)*8+8, N=t%32]`

### 2.2 MFMA Output Layout (INTERLEAVED!)
Output 32M × 32N stored in 16 VGPRs (v32-v47):
```
row  | threads 0-31  | threads 32-63 | vreg
-----|---------------|---------------|------
0-3  | v32-v35       | -             |
4-7  | -             | v32-v35       |
8-11 | v36-v39       | -             |
...
```
Formula: `row = ((v-32)%4) + ((tid/32)*4) + ((v-32)/4)*8`

### 2.3 K=64 MFMA (2x Efficiency)
```asm
// v_mfma_f32_32x32x64_f8f6f4: 2K FLOPs/cycle (vs 1K for K=16)
v_mfma_f32_32x32x64_f8f6f4 v[acc], v[A], v[B], v[acc]
// A: 8 VGPRs, B: 8 VGPRs, acc: 16 VGPRs
```

---

## 3. MEMORY OPS

### 3.1 buffer_load→LDS Pattern
```asm
// Direct global→LDS, bypasses VGPRs
s_mov_b32 s11, 0x20000              // buffer flags
v_lshlrev_b32_e32 v1, 4, v0         // voffset = tid * 16
s_mov_b32 m0, LDS_BASE
buffer_load_dwordx4 v1, s[8:11], 0 offen lds
// 64 threads × 16 bytes = 1024 bytes
```

### 3.2 soffset Must Be SGPR
```asm
// BAD: immediate doesn't work for soffset
buffer_load_dwordx4 v1, s[8:11], 1024 offen lds  // ERROR

// GOOD: use SGPR
s_mov_b32 s20, 1024
buffer_load_dwordx4 v1, s[8:11], s20 offen lds   // OK
```

### 3.3 NEVER Use flat_load/flat_store
SYMPTOM: Kernel works but slow
CAUSE: flat_* has higher latency, worse scheduling
FIX: Always use buffer_load/buffer_store with descriptors

---

## 4. REGISTER HAZARDS

### 4.1 VGPR Corruption in K-Loop
SYMPTOM: v[0:15] accumulators corrupt after 2+ K-tiles
CAUSE: Pipeline interaction between buffer_load and MFMA
FIX: Use high VGPRs for accumulators
```asm
// BAD
v_mfma_f32_32x32x64_f8f6f4 v[0:15], ...

// GOOD
v_mfma_f32_32x32x64_f8f6f4 v[80:95], ...
```

### 4.2 v_cvt_pk_fp8_f32 Garbage in High Bits
SYMPTOM: NaN from MFMA after FP8 conversion
CAUSE: v_cvt_pk_fp8_f32 only writes low 16 bits
FIX: Mask after conversion
```asm
v_cvt_pk_fp8_f32 v72, v64, v65
v_and_b32_e32 v72, 0xFFFF, v72     // REQUIRED: clear garbage
```

### 4.3 amdhsa_accum_offset Must Match VGPRs
SYMPTOM: MFMA output is garbage/NaN
CAUSE: Metadata doesn't reserve enough AGPRs
FIX: Match known-working kernel (usually 148+)
```asm
.amdhsa_next_free_vgpr 148
.amdhsa_accum_offset 148
```

### 4.4 v_readfirstlane_b32 Same for All Waves
SYMPTOM: All waves compute same result
CAUSE: v_readfirstlane returns lane 0 value to SGPR (same for all waves)
FIX: Keep wave-varying values in VGPRs

---

## 5. DEBUG TECHNIQUES

### 5.1 Reading uint32 from float32 Buffer
SYMPTOM: Debug values show 0 when kernel wrote valid data
CAUSE: Python int() truncates small floats to 0
FIX: Use struct.unpack
```python
# BAD
val = int(tensor.item())  # Returns 0 for 0x2e9b391c!

# GOOD
import struct
def as_u32(f): return struct.unpack('I', struct.pack('f', f))[0]
val = as_u32(tensor.item())  # Returns 0x2e9b391c
```

### 5.2 Test Patterns (NEVER Use Uniform Input)
| Pattern | Purpose |
|---------|---------|
| Random σ≤0.5 | Reveals layout/ordering bugs |
| Q=K=1.0 | Verify basic MFMA (expect output = K×16) |
| Single row varies | Check row distribution |
| Identity pattern | Exact output verification |

### 5.3 HIP_VISIBLE_DEVICES
```bash
# Code model uses GPU 0
HIP_VISIBLE_DEVICES=0 python test.py

# Domain model uses GPU 7
HIP_VISIBLE_DEVICES=7 python test.py
```

---

## 6. FP8 FORMAT

### 6.1 Use e4m3fn (OCP), NOT e4m3fnuz
SYMPTOM: 2× output values
CAUSE: Wrong FP8 format interpretation
FIX: Use torch.float8_e4m3fn

| Value    | e4m3fn | e4m3fnuz |
|----------|--------|----------|
| 1.0      | 0x38   | 0x40     |
| 0.125    | 0x20   | 0x28     |

### 6.2 FP8 Precision Limits
- Values 0-16: Exact
- Values >16: Round to nearest even
- Max: ~448

---

## 7. MFMA INSTRUCTION CONSTRAINTS

### 7.1 Minimum Size for FP8/BF16: 32×32 Only
```bash
# NO 16x16 or 8x8 for FP8!
v_mfma_f32_16x16x32_fp8_fp8  # ERROR - doesn't exist
v_mfma_f32_32x32x16_fp8_fp8  # OK
```

### 7.2 s_nop Requirements
```asm
v_accvgpr_write_b32 a0, v30
s_nop 1                          // Required before MFMA
v_mfma_f32_32x32x16_fp8_fp8 ...
s_nop 15                         // Required after MFMA
```

---

## 8. BF16 REFERENCE PATTERNS

### 8.1 Multi-Wave Layout (256T = 4 waves)
| Wave | Q Rows | LDS Offset |
|------|--------|------------|
| 0    | 0-31   | 0x8200 + 0×0x408 |
| 1    | 32-63  | 0x8200 + 1×0x408 |
| 2    | 64-95  | 0x8200 + 2×0x408 |
| 3    | 96-127 | 0x8200 + 3×0x408 |

### 8.2 Each Wave Handles 32 Q Rows (Full MFMA Tile)
WRONG: "Each wave handles 8 Q rows"
CORRECT: "Each wave handles 32 Q rows"
