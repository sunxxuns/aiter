CDNA4 Instruction Set Architecture Reference Guide 
5-August-2025
CDNA4 Instruction Set Architecture   Specification Agreement 
This Specification Agreement ("Agreement") is a legal agreement between Advanced Micro Devices, Inc. ("AMD") and "You" as the recipient of the attached AMD Specification ("Specification"). If you are accessing the Specification as part of your performance of work for another party, you acknowledge that you have authority to bind such party to the terms and conditions of this Agreement. If you accessed the Specification by any means or otherwise use or provide Feedback (defined below) on the Specification, You agree to the terms and conditions set forth in this Agreement. If You do not agree to the terms and conditions set forth in this Agreement, you are not licensed to use the Specification; do not use, access, or provide Feedback about the Specification. In consideration of Your use or access of the Specification (in whole or in part), the receipt and sufficiency of which are acknowledged, You agree as follows: 
1. You may review the Specification only (a) as a reference to assist You in planning and designing Your product, service or technology ("Product") to interface with an AMD product in compliance with the requirements as set forth in the Specification and (b) to provide Feedback about the information disclosed in the Specification to AMD. 
2. Except as expressly set forth in Paragraph 1, all rights in and to the Specification are retained by AMD. This Agreement does not give You any rights under any AMD patents, copyrights, trademarks, or other intellectual property rights. You may not (i) duplicate any part of the Specification; (ii) remove this Agreement or any notices from the Specification, or (iii) give any part of the Specification, or assign or otherwise provide Your rights under this Agreement, to anyone else. 
3. You agree that You shall not use nor procure others to use the contents of this Agreement for (i) modifying any existing patent or patent application or creating any continuation, continuation in part or other extension of any patent or patent application, nor (ii) analyzing, assessing any patent or patent application (which shall include the creation or modification of any patent claim charts or infringement analyses). 
4. The Specification may contain preliminary information, errors, or inaccuracies, or may not include certain necessary information. Additionally, AMD reserves the right to discontinue or make changes to the Specification and its products at any time without notice. The Specification is provided entirely "AS IS." AMD MAKES NO WARRANTY OF ANY KIND AND DISCLAIMS ALL EXPRESS, IMPLIED AND STATUTORY WARRANTIES, INCLUDING BUT NOT LIMITED TO IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NONINFRINGEMENT, TITLE OR THOSE WARRANTIES ARISING AS A COURSE OF DEALING OR CUSTOM OF TRADE. AMD SHALL NOT BE LIABLE FOR DIRECT, INDIRECT, CONSEQUENTIAL, SPECIAL, INCIDENTAL, PUNITIVE OR EXEMPLARY DAMAGES OF ANY KIND (INCLUDING LOSS OF BUSINESS, LOSS OF INFORMATION OR DATA, LOST PROFITS, LOSS OF CAPITAL, LOSS OF GOODWILL) REGARDLESS OF THE FORM OF ACTION WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE) AND STRICT PRODUCT LIABILITY OR OTHERWISE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 
5. Furthermore, AMD’s products are not designed, intended, authorized or warranted for use as components in systems intended for surgical implant into the body, or in other applications intended to support or sustain life, or in any other application in which the failure of AMD’s product could create a situation where personal injury, death, or severe property or environmental damage may occur. 
6. You have no obligation to give AMD any suggestions, comments, or feedback ("Feedback") relating to the Specification. However, any Feedback You voluntarily provide may be used by AMD without restriction, fee, or obligation of confidentiality. Accordingly, if You do give AMD Feedback on any version of the Specification, You agree AMD may freely use, reproduce, license, distribute, and otherwise commercialize Your Feedback in any product, as well as has the right to sublicense third parties to do the same. Further, You will not give AMD any Feedback that You may have reason to believe is (i) subject to any patent, copyright or other intellectual property claim or right of any third party; or (ii) subject to license terms which seek to require any product or intellectual property incorporating or derived from Feedback or any Product or other AMD intellectual property to be licensed to or otherwise provided to any third party. 
7. You shall adhere to all applicable U.S., European, and other export laws, including but not limited to the U.S. Export Administration Regulations ("EAR"), (15 C.F.R. Sections 730 through 774), and E.U. Council Regulation (EC) No 428/2009 of 5 May 2009. Further, pursuant to Section 740.6 of the EAR, You hereby certifies that, except pursuant to a license granted by the United States Department of Commerce Bureau of Industry and Security or as otherwise permitted pursuant to a License Exception under the U.S. Export Administration Regulations ("EAR"), You will not (1) export, re-export or release to a national of a country in Country Groups D:1, E:1 or E:2 any restricted technology, software, or source code You receive hereunder, or (2) export to Country Groups D:1, E:1 or E:2 the direct product of such technology or software, if such foreign produced direct product is subject to national security controls as identified on the Commerce Control List (currently found in Supplement 1 to Part 774 of EAR). For the most current Country Group listings, or for additional information about the EAR or Your obligations under those regulations, please refer to the U.S. Bureau of Industry and Security’s website at http://www.bis.doc.gov/. This Section 7 is applicable solely to Specifications shall not apply to any Specifications that are released publicly. 
8. If You are a part of the U.S. Government, then the Specification is provided with "RESTRICTED RIGHTS" as set forth in subparagraphs (c) (1) and (2) of the Commercial Computer Software-Restricted Rights clause at FAR 52.227-14 or subparagraph (c) (1)(ii) of the Rights in Technical Data and Computer Software clause at DFARS 252.277-7013, as applicable. 
9. This Agreement is governed by the laws of the State of California without regard to its choice of law principles. Any dispute involving it must be brought in a court having jurisdiction of such dispute in Santa Clara County, California, and You waive any defenses and rights allowing the dispute to be litigated elsewhere. If any part of this agreement is unenforceable, it will be considered modified to the extent necessary to make it 
ii of 600
CDNA4 Instruction Set Architecture   
enforceable, and the remainder shall continue in effect. The failure of AMD to enforce any rights granted hereunder or to take action against You in the event of any breach hereunder shall not be deemed a waiver by AMD as to subsequent enforcement of rights or subsequent actions in the event of future breaches. This Agreement is the entire agreement between You and AMD concerning the Specification; it may be changed only by a written document signed by both You and an authorized representative of AMD. 
DISCLAIMER 
The information presented in this document is for informational purposes only and may contain technical inaccuracies, omissions, and typographical errors. The information contained herein is subject to change and may be rendered inaccurate for many reasons, including but not limited to product and roadmap changes, component and motherboard version changes, new model and/or product releases, product differences between differing manufacturers, software changes, BIOS flashes, firmware upgrades, or the like. Any computer system has risks of security vulnerabilities that cannot be completely prevented or mitigated. AMD assumes no obligation to update or otherwise correct or revise this information. However, AMD reserves the right to revise this information and to make changes from time to time to the content hereof without obligation of AMD to notify any person of such revisions or changes. THIS INFORMATION IS PROVIDED "AS IS." AMD MAKES NO REPRESENTATIONS OR WARRANTIES WITH RESPECT TO THE CONTENTS HEREOF AND ASSUMES NO RESPONSIBILITY FOR ANY INACCURACIES, ERRORS, OR OMISSIONS THAT MAY APPEAR IN THIS INFORMATION. AMD SPECIFICALLY DISCLAIMS ANY IMPLIED WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR ANY PARTICULAR PURPOSE. IN NO EVENT WILL AMD BE LIABLE TO ANY PERSON FOR ANY RELIANCE, DIRECT, INDIRECT, SPECIAL, OR OTHER CONSEQUENTIAL DAMAGES ARISING FROM THE USE OF ANY INFORMATION CONTAINED HEREIN, EVEN IF AMD IS EXPRESSLY ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 
AMD, the AMD Arrow logo and combinations thereof are trademarks of Advanced Micro Devices, Inc. Other product names used in this publication are for identification purposes only and may be trademarks of their respective companies. 
Copyright © 2024 Advanced Micro Devices, Inc. All rights reserved. 
  

Advanced Micro Devices, Inc. 
2485 Augustine Drive 
Santa Clara, CA, 95054 
www.amd.com 
iii of 600
CDNA4 Instruction Set Architecture   Contents 
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 About This Document . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Audience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Organization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Conventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Contact Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 
1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.1. Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2. Program Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.1. Compute Shaders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2. Data Sharing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2.1. Local Data Share (LDS) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.3. Device Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3. Kernel State. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 3.1. State Overview. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 3.2. Program Counter (PC) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.3. EXECute Mask. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.4. Status registers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.5. Mode register . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 3.6. GPRs and LDS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.6.1. Out-of-Range behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.6.2. SGPR Allocation and storage. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.6.3. SGPR Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.6.4. VGPR Allocation and Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.6.5. LDS Allocation and Clamping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.7. M0 Memory Descriptor. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.8. SCC: Scalar Condition code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.9. Vector Compares: VCC and VCCZ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.10. Trap and Exception registers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.10.1. Trap Status register . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3.11. Memory Violations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3.12. Hardware ID Registers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 3.13. GPR Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 4. Program Flow Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 4.1. Program Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 4.2. Branching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 4.3. Workgroups. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 4.4. Data Dependency Resolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 4.5. Manually Inserted Wait States (NOPs). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 
iv of 600
CDNA4 Instruction Set Architecture   
4.6. Arbitrary Divergent Control Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 5. Scalar ALU Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 5.1. SALU Instruction Formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 5.2. Scalar ALU Operands . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 5.3. Scalar Condition Code (SCC) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 5.4. Integer Arithmetic Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 5.5. Conditional Instructions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 5.6. Comparison Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 5.7. Bit-Wise Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 5.8. Access Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 6. Vector ALU Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 6.1. Microcode Encodings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 6.2. Operands . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 6.2.1. Instruction Inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 6.2.2. Instruction Outputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 6.2.3. Out-of-Range GPRs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 6.3. Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 6.4. Denormalized and Rounding Modes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 6.5. ALU Clamp Bit Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 6.6. VGPR Indexing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 6.6.1. Indexing Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 6.6.2. VGPR Indexing Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 6.7. Packed Math . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 6.7.1. Packed Convert . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 7. Matrix Arithmetic Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 7.1. Matrix fused-multiply-add (MFMA). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 7.1.1. Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 7.1.2. List of Dense MFMA instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 7.1.3. Usage examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 7.1.4. General input and output layout. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 7.1.5. 8-bit and Smaller Matrix Operations and Layouts. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 7.1.6. Broadcasting values. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 7.2. Block Scaled Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 7.2.1. MFMA with Block Exponent Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 7.3. BF8 / FP8 and Smaller Formats and Conversions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 7.4. Floating-point handling details and formats. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 7.5. Sparse Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 7.5.1. Details of Sparsity Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 7.6. Dependency Resolution: Required Independent Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 8. Scalar Memory Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 8.1. Microcode Encoding. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 
v of 600
CDNA4 Instruction Set Architecture   
8.2. Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 8.2.1. S_LOAD_DWORD, S_STORE_DWORD. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 8.2.2. Scalar Atomic Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 8.2.3. S_DCACHE_INV, S_DCACHE_WB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 8.2.4. S_MEMTIME . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 8.2.5. S_MEMREALTIME . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 
8.3. Dependency Checking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 8.4. Alignment and Bounds Checking. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 9. Vector Memory Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 9.1. Vector Memory Buffer Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 9.1.1. Simplified Buffer Addressing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 9.1.2. Buffer Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 9.1.3. VGPR Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 9.1.4. Buffer Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 9.1.5. Buffer Addressing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 9.1.6. 16-bit Memory Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 9.1.7. Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 9.1.8. Buffer Resource. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 9.1.9. Memory Buffer Load to LDS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 9.1.10. Memory Scope and Temporal Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 9.1.11. Data Formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 9.2. Float Memory Atomics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 9.2.1. Rounding of Float Atomics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 9.2.2. Denormal (Subnormal) Handling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 9.2.3. NaN Handling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 10. Flat Memory Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 10.1. Flat Memory Instruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 10.2. Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 10.2.1. Ordering. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 10.2.2. Important Timing Consideration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 10.3. Addressing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 10.3.1. Atomics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 10.4. Global. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 10.5. Scratch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 10.6. Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 10.7. Scratch Space (Private) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 11. Data Share Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 11.1. Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 11.2. Dataflow in Memory Hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 11.3. LDS Access. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 11.3.1. Data Share Indexed and Atomic Access . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 
vi of 600
CDNA4 Instruction Set Architecture   
11.4. MFMA Transpose Load from LDS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 12. Instructions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 12.1. SOP2 Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 12.2. SOPK Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 12.3. SOP1 Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119 12.4. SOPC Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137 12.5. SOPP Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142 12.5.1. Send Message. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150 12.6. SMEM Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150 12.7. VOP2 Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173 12.7.1. VOP2 using VOP3 encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191 12.8. VOP1 Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191 12.8.1. VOP1 using VOP3 encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220 12.9. VOPC Instructions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220 12.9.1. VOPC using VOP3A encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268 12.10. VOP3P Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268 12.11. VOP3A & VOP3B Instructions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310 12.12. LDS Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457 12.13. MUBUF Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495 12.14. MTBUF Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515 12.15. FLAT, Scratch and Global Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 520 12.15.1. Flat Instructions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 520 12.15.2. Scratch Instructions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535 12.15.3. Global Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 541 12.16. Instruction Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 556 12.16.1. DPP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 556 12.16.2. SDWA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 556 13. Microcode Formats. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 558 13.1. Scalar ALU and Control Formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 559 13.1.1. SOP2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 559 13.1.2. SOPK. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561 13.1.3. SOP1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 562 13.1.4. SOPC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564 13.1.5. SOPP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 566 13.2. Scalar Memory Format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 566 13.2.1. SMEM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 566 13.3. Vector ALU Formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568 13.3.1. VOP2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568 13.3.2. VOP1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 570 13.3.3. VOPC. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 572 13.3.4. VOP3A. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 577 
vii of 600
CDNA4 Instruction Set Architecture   
13.3.5. VOP3B. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 584 13.3.6. VOP3P. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 586 13.3.7. SDWA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 590 13.3.8. SDWAB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 590 13.3.9. DPP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 591 
13.4. LDS format. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 592 13.4.1. DS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 592 13.5. Vector Memory Buffer Formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 594 13.5.1. MTBUF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 595 13.5.2. MUBUF. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 596 13.6. Flat Formats. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 597 13.6.1. FLAT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 598 13.6.2. GLOBAL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 599 13.6.3. SCRATCH . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 600 
viii of 600
CDNA4 Instruction Set Architecture   Preface 
About This Document 
This document describes the current environment, organization and program state of AMD CDNA4 devices. It details the instruction set and the microcode formats native to this family of processors that are accessible to programmers and compilers. 
The document specifies the instructions (including the format of each type of instruction) and the relevant program state (including how the program state interacts with the instructions). Some instruction fields are mutually dependent; not all possible settings for all fields are legal. This document specifies the valid combinations. 
The main purposes of this document are to: 
1. Specify the language constructs and behavior, including the organization of each type of instruction in both text syntax and binary format. 
2. Provide a reference of instruction operation that compiler writers can use to maximize performance of the processor. 
Audience 
This document is intended for programmers writing application and system software, including operating systems, compilers, loaders, linkers, device drivers, and system utilities. It assumes that programmers are writing compute-intensive parallel applications (streaming applications) and assumes an understanding of requisite programming practices. 
Organization 
This document begins with an overview of the AMD CDNA processors' hardware and programming environment (Chapter 1). 
Chapter 2 describes the organization of CDNA programs. 
Chapter 3 describes the program state that is maintained. 
Chapter 4 describes the program flow. 
Chapter 5 describes the scalar ALU operations. 
Chapter 6 describes the vector ALU operations. 
Chapter 7 describes the vector Matrix ALU operations. 
Chapter 8 describes the scalar memory operations. 
Chapter 9 describes the vector memory operations. 
Chapter 10 provides information about the flat memory instructions. 
Chapter 11 describes the data share operations. 
Chapter 12 describes instruction details, first by the microcode format to which they belong, then in alphabetic order. 
Finally, Chapter 13 provides a detailed specification of each microcode format. 
About This Document 1 of 600
CDNA4 Instruction Set Architecture   Conventions 
The following conventions are used in this document: 
mono-spaced font 
	A filename, file path or code.
	* 
	Any number of alphanumeric characters in the name of a code format, parameter, or instruction.
	< > 
	Angle brackets denote streams.
	[1,2) 
	A range that includes the left-most value (in this case, 1), but excludes the right-most value (in this case, 2).
	[1,2] 
	A range that includes both the left-most and right-most values.
	{x | y} 
	One of the multiple options listed. In this case, X or Y.
	0.0 
	A single-precision (32-bit) floating-point value.
	1011b 
	A binary value, in this example a 4-bit value.
	7:4 
	A bit range, from bit 7 to bit 4, inclusive. The high-order bit is shown first.
	italicized word or phrase 
	The first use of a term or concept basic to the understanding of stream computing.
	



Contact Information 
For information concerning AMD Accelerated Parallel Processing development, please see: http://developer.amd.com/ 
Conventions 2 of 600
CDNA4 Instruction Set Architecture   Chapter 1. Introduction 
AMD CDNA processors implement a parallel micro-architecture that is designed to provide an excellent platform for general-purpose data parallel applications. Data-intensive applications that require high bandwidth or are computationally intensive are a candidate for running on an AMD CDNA processor. 
The figure below shows a block diagram of the AMD CDNA Generation series processors   Figure 1. AMD CDNA Generation Series Block Diagram 
The CDNA device includes a data-parallel processor (DPP) array, a command processor, a memory controller, and other logic (not shown). The CDNA command processor reads commands that the host has written to memory-mapped CDNA registers in the system-memory address space. The command processor sends hardware-generated interrupts to the host when the command is completed. The CDNA memory controller has direct access to all CDNA device memory and the host-specified areas of system memory. To satisfy read and write requests, the memory controller performs the functions of a direct-memory access (DMA) controller, including computing memory-address offsets based on the format of the requested data in memory. In the CDNA environment, a complete application includes two parts: 
• a program running on the host processor, and 
• programs, called kernels, running on the CDNA processor. 
The CDNA programs are controlled by host commands that 
• set CDNA internal base-address and other configuration registers, 
• specify the data domain on which the CDNA Accelerator is to operate, 
• invalidate and flush caches on the CDNA Accelerator, and 
• cause the CDNA Accelerator to begin execution of a program. 
The CDNA driver program runs on the host. 
3 of 600
CDNA4 Instruction Set Architecture   
The DPP array is the heart of the CDNA processor. The array is organized as a set of compute unit pipelines, each independent from the others, that are designed to operate in parallel on streams of floating-point or integer data. The compute unit pipelines can process data or, through the memory controller, transfer data to, or from, memory. Computation in a compute unit pipeline can be made conditional. Outputs written to memory can also be made conditional. 
When it receives a request, the compute unit pipeline loads instructions and data from memory, begins execution, and continues until the end of the kernel. As kernels are running, the CDNA hardware automatically fetches instructions from memory into on-chip caches; CDNA software plays no role in this. CDNA kernels can load data from off-chip memory into on-chip general-purpose registers (GPRs) and caches. 
The AMD CDNA devices can detect floating point exceptions and can generate interrupts. In particular, they can detect IEEE floating-point exceptions in hardware; these can be recorded for post-execution analysis. The software interrupts shown in the previous figure from the command processor to the host represent hardware generated interrupts for signaling command-completion and related management functions. 
The CDNA processor is designed to hide memory latency by keeping track of potentially hundreds of work items in different stages of execution, and by overlapping compute operations with memory-access operations. 
1.1. Terminology 
Table 1. Basic Terms 
Term 
CDNA Processor 
	Description 
The shader processor is a scalar and vector ALU capable of running complex programs on behalf of a wavefront.
	Dispatch 
	A dispatch launches a 1D, 2D, or 3D grid of work to the CDNA processor array.
	Workgroup 
	A workgroup is a collection of wavefronts that have the ability to synchronize with each other quickly; they also can share data through the Local Data Share.
	Wavefront 
	A collection of 64 work-items that execute in parallel on a single CDNA processor.
	Work-item 
	A single element of work: one element from the dispatch grid, or in graphics a pixel or vertex.
	Literal Constant 
	A 32-bit integer or float constant that is placed in the instruction stream.
	

	Scalar ALU (SALU) The scalar ALU operates on one value per wavefront and manages all control flow.
	

	Vector ALU (VALU) The vector ALU maintains Vector GPRs that are unique for each work item and execute arithmetic operations uniquely on each work-item.
	Microcode format 
	The microcode format describes the bit patterns used to encode instructions. Each instruction is either 32 or 64 bits.
	Instruction 
	An instruction is the basic unit of the kernel. Instructions include: vector ALU, scalar ALU, memory transfer, and control flow operations.
	Buffer Resource (V#)
	A buffer resource descriptor describes a buffer in memory: address, data format, stride, etc.
	



1.1. Terminology 4 of 600
CDNA4 Instruction Set Architecture   Chapter 2. Program Organization 
CDNA kernels are programs executed by the CDNA processor. Conceptually, the kernel is executed independently on every work-item, but in reality the CDNA processor groups 64 work-items into a wavefront, which executes the kernel on all 64 work-items in one pass. 
The CDNA processor consists of: 
• A scalar ALU, which operates on one value per wavefront (common to all work items). • A vector ALU, which operates on unique values per work-item. 
• Local data storage, which allows work-items within a workgroup to communicate and share data. • Scalar memory, which can transfer data between SGPRs and memory through a cache. • Vector memory, which can transfer data between VGPRs and memory 
All kernel control flow is handled using scalar ALU instructions. This includes if/else, branches and looping. Scalar ALU (SALU) and memory instructions work on an entire wavefront and operate on up to two SGPRs, as well as literal constants. 
Vector memory and ALU instructions operate on all work-items in the wavefront at one time. In order to support branching and conditional execute, every wavefront has an EXECute mask that determines which work-items are active at that moment, and which are dormant. Active work-items execute the vector instruction, and dormant ones treat the instruction as a NOP. The EXEC mask can be changed at any time by Scalar ALU instructions. 
Vector ALU instructions can take up to three arguments, which can come from VGPRs, SGPRs, or literal constants that are part of the instruction stream. They operate on all work-items enabled by the EXEC mask. Vector compare and add with- carryout return a bit-per-work-item mask back to the SGPRs to indicate, per work-item, which had a "true" result from the compare or generated a carry-out. 
Vector memory instructions transfer data between VGPRs and memory. Each work-item supplies its own memory address and supplies or receives unique data. These instructions are also subject to the EXEC mask. 
2.1. Compute Shaders 
Compute kernels (shaders) are generic programs that can run on the CDNA processor, taking data from memory, processing it, and writing results back to memory. Compute kernels are created by a dispatch, which causes the CDNA processors to run the kernel over all of the work-items in a 1D, 2D, or 3D grid of data. The CDNA processor walks through this grid and generates wavefronts, which then run the compute kernel. Each work-item is initialized with its unique address (index) within the grid. Based on this index, the work-item computes the address of the data it is required to work on and what to do with the results. 
2.2. Data Sharing 
The AMD CDNA stream processors can share data between different work-items. Data sharing can significantly boost performance. The figure below shows the memory hierarchy that is available to each work-item. 
2.1. Compute Shaders 5 of 600
CDNA4 Instruction Set Architecture Figure 2. Shared Memory Hierarchy 
2.2.1. Local Data Share (LDS) 
Each compute unit has a 160 kB memory space that enables low-latency communication between work-items within a work-group, or the work-items within a wavefront; this is the local data share (LDS). This memory is configured with 64 banks, each with 640 entries of 4 bytes. The shared memory contains 32 integer atomic units designed to enable fast, unordered atomic operations. This memory can be used as a software cache for predictable re-use of data, a data exchange machine for the work-items of a work-group, or as a cooperative way to enable efficient access to off-chip memory. 
2.3. Device Memory 
The AMD CDNA devices offer several methods for access to off-chip memory from the processing elements (PE) within each compute unit. On the primary read path, the device consists of multiple channels of L2 read write cache that provides data to an L1 cache for each compute unit. Specific cache-less load instructions can force data to be retrieved from device memory during an execution of a load clause. Load requests that overlap within the clause are cached with respect to each other. The output cache is formed by two levels of cache: the first for write-combining cache (collect scatter and store operations and combine them to provide good access patterns to memory); the second is a read/write cache with atomic units that lets each processing element complete unordered atomic accesses that return the initial value. Each processing element provides the destination address on which the atomic operation acts, the data to be used in the atomic operation, and a return address for the read/write atomic unit to store the pre-op value in memory. Each store or atomic operation can be set up to return an acknowledgment to the requesting PE upon write confirmation of the return value (pre-atomic op value at destination) being stored to device memory. 
This acknowledgment has two purposes: 
2.3. Device Memory 6 of 600
CDNA4 Instruction Set Architecture 
• enabling a PE to recover the pre-op value from an atomic operation by performing a cache-less load from its return address after receipt of the write confirmation acknowledgment, and 
• enabling the system to maintain a relaxed consistency model. 
Each scatter write from a given PE to a given memory channel maintains order. The acknowledgment enables one processing element to implement a fence to maintain serial consistency by ensuring all writes have been posted to memory prior to completing a subsequent write. In this manner, the system can maintain a relaxed consistency model between all parallel work-items operating on the system. 
2.3. Device Memory 7 of 600
CDNA4 Instruction Set Architecture Chapter 3. Kernel State 
This chapter describes the kernel states visible to the shader program. 
3.1. State Overview 
The table below shows all of the hardware states readable or writable by a shader program. 
Table 2. Readable and Writable Hardware States 
Abbrev. 
PC 
	Name 
Program Counter 
	Size 
(bits) 
48 
	Description 
Points to the memory address of the next shader instruction to execute.
	V0-V255 
	VGPR 
	32 
	Vector general-purpose register ("architectural VGPRs").
	AV0-AV255 
	AccVGPR 
	32 
	Matrix Accumulation Vector general-purpose register.
	S0-S103 
	SGPR 
	32 
	Vector general-purpose register.
	LDS 
	Local Data Share 
	160kB 
	Local data share is a scratch RAM with built-in arithmetic capabilities that allow data to be shared between threads in a workgroup.
	EXEC 
	Execute Mask 
	64 
	A bit mask with one bit per thread, which is applied to vector instructions and controls that threads execute and that ignore the instruction.
	EXECZ 
	EXEC is zero 
	1 
	A single bit flag indicating that the EXEC mask is all zeros.
	VCC 
	Vector Condition Code 
	64 
	A bit mask with one bit per thread; it holds the result of a vector compare operation.
	VCCZ 
	VCC is zero 
	1 
	A single bit-flag indicating that the VCC mask is all zeros.
	SCC 
	Scalar Condition Code 
	1 
	Result from a scalar ALU comparison instruction.
	FLAT_SCRATCH 
	Flat scratch address 
	64 
	The 64-bit base address of scratch memory, in NumSGPRs-5 and -6. Read Only.
	XNACK_MASK 
	Address translation failure. 
	64 
	Bit mask of threads that have failed their address translation.
	STATUS 
	Status 
	32 
	Read-only shader status bits.
	MODE 
	Mode 
	32 
	Writable shader mode bits.
	M0 
	Memory Reg 
	32 
	A temporary register that has various uses, including GPR indexing and bounds checking.
	HW_ID 
	Hardware ID 
	32 
	Read-only status register that has various wave ID state.
	XCC_ID 
	Compute ID 
	32 
	Read-only status register that contains the compute device ID.
	TRAPSTS 
	Trap Status 
	32 
	Holds information about exceptions and pending traps.
	TBA 
	Trap Base Address 
	64 
	Holds the pointer to the current trap handler program.
	TMA 
	Trap Memory Address 
	64 
	Temporary register for shader operations. For example, can hold a pointer to memory used by the trap handler.
	TTMP0-TTMP15 
	Trap Temporary SGPRs 
	32 
	16 SGPRs available only to the Trap Handler for temporary storage.
	VMCNT 
	Vector memory instruction count 6 
	

	Counts the number of VMEM instructions issued but not yet completed.
	EXPCNT 
	Export Count 
	3 
	Unused
	



3.1. State Overview 8 of 600
CDNA4 Instruction Set Architecture 
Abbrev. 
	Name 
	Size 
(bits)
	Description
	LGKMCNT 
	LDS, Constant and Message count 4 
	

	Counts the number of LDS, constant-fetch (scalar memory read), and message instructions issued but not yet completed.
	



3.2. Program Counter (PC) 
The program counter (PC) is a byte address pointing to the next instruction to execute. When a wavefront is created, the PC is initialized to the first instruction in the program. 
The PC interacts with three instructions: S_GET_PC, S_SET_PC, S_SWAP_PC. These transfer the PC to, and from, an even-aligned SGPR pair. 
Branches jump to (PC_of_the_instruction_after_the_branch + offset). The shader program cannot directly read from, or write to, the PC. Branches, GET_PC and SWAP_PC, are PC-relative to the next instruction, not the current one. S_TRAP saves the PC of the S_TRAP instruction itself. 
3.3. EXECute Mask 
The Execute mask (64-bit) determines which threads in the vector are executed: 
1 = execute, 0 = do not execute. 
EXEC can be read from, and written to, through scalar instructions; it also can be written as a result of a vector ALU compare. This mask affects vector-ALU, vector-memory, and LDS instructions. It does not affect scalar execution or branches. 
A helper bit (EXECZ) can be used as a condition for branches to skip code when EXEC is zero. This Accelerator does no optimization when EXEC = 0. The shader hardware executes every 
instruction, wasting instruction issue bandwidth. Use CBRANCH or VSKIP to rapidly skip over code when it is likely that the EXEC mask is zero. 
3.4. Status registers 
Status register fields can be read, but not written to, by the shader. These bits are initialized at wavefront creation time. The table below lists and briefly describes the status register fields. The status register fields may be written when PRIV=1. Some fields are set as a result of shader instructions. 
Table 3. Status Register Fields 
Field 
SCC 
	Bit 
Position 
1 
	Description 
Scalar condition code. Used as a carry-out bit. For a comparison instruction, this bit indicates failure or success. For logical operations, this is 1 if the result was non-zero.
	



3.2. Program Counter (PC) 9 of 600
CDNA4 Instruction Set Architecture 
Field 
	Bit 
Position
	Description
	SPI_PRIO 
	2:1 
	Wavefront priority set by the shader processor interpolator (SPI) when the wavefront is created. See the S_SETPRIO instruction (page 12-49) for details. 0 is lowest, 3 is highest priority.
	WAVE_PRIO 
	4:3 
	Wavefront priority set by the shader program. See the S_SETPRIO instruction (page 12-49) for details.
	PRIV 
	5 
	Privileged mode. Can only be active when in the trap handler. Gives write access to the TTMP, TMA, and TBA registers.
	TRAP_EN 
	6 
	Indicates that a trap handler is present. When set to zero, traps are not taken.
	EXECZ 
	9 
	Exec mask is zero.
	VCCZ 
	10 
	Vector condition code is zero.
	IN_TG 
	11 
	Wavefront is a member of a work-group of more than one wavefront.
	IN_BARRIER 
	12 
	Wavefront is waiting at a barrier.
	HALT 
	13 
	Wavefront is halted or scheduled to halt. HALT can be set by the host through wavefront-control messages, or by the shader. This bit is ignored while in the trap handler (PRIV = 1); it also is ignored if a host-initiated trap is received (request to enter the trap handler).
	TRAP 
	14 
	Wavefront is flagged to enter the trap handler as soon as possible.
	VALID 
	16 
	Wavefront is active (has been created and not yet ended).
	ECC_ERR 
	17 
	An ECC error has occurred.
	PERF_EN 
	19 
	Performance counters are enabled for this wavefront.
	COND_DBG_USER 
	20 
	Conditional debug indicator for user mode
	COND_DBG_SYS 
	21 
	Conditional debug indicator for system mode.
	ALLOW_REPLAY 
	22 
	Indicates that ATC replay is enabled.
	FATAL_HALT 
	23 
	Indicates a fatal halt has occurred.
	SCRATCH_EN 
	28 
	1 = wave has scratch space allocated; 0 = does not.
	IDLE 
	31 
	Indicates wave is idle - has no outstanding instructions.
	



3.5. Mode register 
Mode register fields can be read from, and written to, by the shader through scalar instructions. The table below lists and briefly describes the mode register fields. 
Table 4. Mode Register Fields 
Field 
FP_ROUND 
	Bit 
Position 
3:0 
	Description 
[1:0] Single precision round mode. [3:2] Double/Half precision round mode. Round Modes: 0=nearest even, 1= +infinity, 2= -infinity, 3= toward zero.
	FP_DENORM 
	7:4 
	[1:0] Single precision denormal mode. [3:2] Double/Half precision denormal mode. Denorm modes: 
0 = flush input and output denorms. 
1 = allow input denorms, flush output denorms. 
2 = flush input denorms, allow output denorms. 
3 = allow input and output denorms.
	DX10_CLAMP 
	8 
	Used by the vector ALU to force DX10-style treatment of NaNs: when set, clamp NaN to zero; otherwise, pass NaN through.
	IEEE 
	9 
	Floating point opcodes that support exception flag gathering quiet and propagate signaling NaN inputs per IEEE 754-2008. Min_dx10 and max_dx10 become IEEE 754-2008 compliant due to signaling NaN propagation and quieting.
	



3.5. Mode register 10 of 600
CDNA4 Instruction Set Architecture 
Field 
	Bit 
Position
	Description
	DEBUG 
	11 
	Forces the wavefront to jump to the exception handler after each instruction is executed (but not after ENDPGM). Only works if TRAP_EN = 1.
	EXCP_EN 
	18:12 
	Enable mask for exceptions. Enabled means if the exception occurs and TRAP_EN==1, a trap is taken. 
[12] : invalid.  
[13] : inputDenormal.  
[14] : float_div0.  
[15] : overflow.  
[16] : underflow.  
[17] : inexact. 
[18] : int_div0. 
[19] : address watch 
[20] : memory violation 
[20] : trap on wave end
	FP16_OVFL 
	23 
	If set, an overflowed FP16 result is clamped to +/- MAX_FP16, regardless of round mode, while still preserving true INF values.
	DISABLE_PERF 
	26 
	1 = disable performance counting for this wave
	GPR_IDX_EN 
	27 
	GPR index enable.
	VSKIP 
	28 
	0 = normal operation. 1 = skip (do not execute) any vector instructions: valu, vmem, lds. "Skipping" instructions occurs at high-speed (10 wavefronts per clock cycle can skip one instruction). This is much faster than issuing and discarding instructions.
	CSP 
	31:29 
	Conditional branch stack pointer.
	



3.6. GPRs and LDS 
This section describes how GPR and LDS space is allocated to a wavefront, as well as how out-of-range and misaligned accesses are handled. 
3.6.1. Out-of-Range behavior 
This section defines the behavior when a source or destination GPR or memory address is outside the legal range for a wavefront. 
Out-of-range can occur through GPR-indexing or bad programming. It is illegal to index from one register type into another (for example: SGPRs into trap registers or inline constants). It is also illegal to index within inline constants. 
The following describe the out-of-range behavior for various storage types. 
• SGPRs 
◦ Source or destination out-of-range = (sgpr < 0 || (sgpr >= sgpr_size)). 
◦ Source out-of-range: returns the value of SGPR0 (not the value 0). 
◦ Destination out-of-range: instruction writes no SGPR result. 
• VGPRs 
◦ Similar to SGPRs. It is illegal to index from SGPRs into VGPRs, or vice versa. 
◦ Out-of-range = (vgpr < 0 || (vgpr >= vgpr_size)) 
3.6. GPRs and LDS 11 of 600
CDNA4 Instruction Set Architecture 
◦ If a source VGPR is out of range, VGPR0 is used. 
◦ If a destination VGPR is out-of-range, the instruction is ignored (treated as an NOP). • LDS 
◦ If the LDS-ADDRESS is out-of-range (addr < 0 or >= (MIN(lds_size, m0)): 
▪ Writes out-of-range are discarded; it is undefined if SIZE is not a multiple of write-data-size. ▪ Reads return the value zero. 
◦ If any source-VGPR is out-of-range, use the VGPR0 value is used. 
◦ If the dest-VGPR is out of range, nullify the instruction (issue with exec=0) 
• Memory, LDS: Reads and atomics with returns. 
◦ If any source VGPR or SGPR is out-of-range, the data value is undefined. 
◦ If any destination VGPR is out-of-range, the operation is nullified by issuing the instruction as if the EXEC mask were cleared to 0. 
▪ This out-of-range check must check all VGPRs that can be returned (for example: VDST to VDST+3 for a BUFFER_LOAD_DWORDx4). 
▪ Atomic operations with out-of-range destination VGPRs are nullified: issued, but with exec mask of zero. 
Instructions with multiple destinations (for example: V_ADDC): if any destination is out-of-range, no results are written. 
3.6.2. SGPR Allocation and storage 
A wavefront can be allocated 16 to 102 SGPRs, in units of 16 GPRs (Dwords). These are logically viewed as SGPRs 0-101. The VCC is physically stored as part of the wavefront’s SGPRs in the highest numbered two SGPRs (SGPR 106 and 107; the source/destination VCC is an alias for those two SGPRs). When a trap handler is present, 16 additional SGPRs are reserved after VCC to hold the trap addresses, as well as saved-PC and trap-handler temps. These all are privileged (cannot be written to unless privilege is set). Note that if a wavefront allocates 16 SGPRs, 2 SGPRs are typically used as VCC, the remaining 14 are available to the shader. Shader hardware does not prevent use of all 16 SGPRs. 
3.6.3. SGPR Alignment 
Even-aligned SGPRs are required in the following cases. 
• When 64-bit data is used. This is required for moves to/from 64-bit registers, including the PC. • When scalar memory reads that the address-base comes from an SGPR-pair (either in SGPR). 
Quad-alignment is required for the data-GPR when a scalar memory read returns four or more Dwords. When a 64-bit quantity is stored in SGPRs, the LSBs are in SGPR[n], and the MSBs are in SGPR[n+1]. 
3.6.4. VGPR Allocation and Alignment 
VGPRs are allocated in groups of eight Dwords. 
VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total 
3.6. GPRs and LDS 12 of 600
CDNA4 Instruction Set Architecture 
VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is not required to be equal numbers of both types. 
Instructions which operate on 64-bit data must use aligned (i.e. even) VGPRs. This applies to ALU and memory instructions. 
3.6.5. LDS Allocation and Clamping 
LDS is allocated per work-group or per-wavefront when work-groups are not in use. LDS space is allocated to a work-group or wavefront in contiguous blocks of 1280 bytes on 1280-byte alignment. LDS allocations do not wrap around the LDS storage. All accesses to LDS are restricted to the space allocated to that wavefront/work group. 
Clamping of LDS reads and writes is controlled by two size registers, which contain values for the size of the LDS space allocated by SPI to this wavefront or work-group, and a possibly smaller value specified in the LDS instruction (size is held in M0). The LDS operations use the smaller of these two sizes to determine how to clamp the read/write addresses. 
3.7. M0 Memory Descriptor 
There is one 32-bit M0 register per wavefront, which can be used for: 
• Local Data Share (LDS) 
◦ LDS addressing for Memory/Vfetch → LDS: {14’h0, lds_offset[17:0]} // in bytes 
◦ { base[5:0], 16’h0} 
• Indirect GPR addressing for both vector and scalar instructions. M0 is an unsigned index. 
3.8. SCC: Scalar Condition code 
Most scalar ALU instructions set the Scalar Condition Code (SCC) bit, indicating the result of the operation. 
Compare operations: 1 = true 
Arithmetic operations: 1 = carry out 
Bit/logical operations: 1 = result was not zero 
Move: does not alter SCC 
The SCC can be used as the carry-in for extended-precision integer arithmetic, as well as the selector for conditional moves and branches. 
3.9. Vector Compares: VCC and VCCZ 
Vector ALU comparisons set the Vector Condition Code (VCC) register (1=pass, 0=fail). Also, vector compares have the option of setting EXEC to the VCC value. 
3.7. M0 Memory Descriptor 13 of 600
CDNA4 Instruction Set Architecture 
There is also a VCC summary bit (vccz) that is set to 1 when the VCC result is zero. This is useful for early-exit branch tests. VCC is also set for selected integer ALU operations (carry-out). 
Vector compares have the option of writing the result to VCC (32-bit instruction encoding) or to any SGPR (64- bit instruction encoding). VCCZ is updated every time VCC is updated: vector compares and scalar writes to VCC. 
The EXEC mask determines which threads execute an instruction. The VCC indicates which executing threads passed the conditional test, or which threads generated a carry-out from an integer add or subtract. 
V_CMP_* ⇒ VCC[n] = EXEC[n] & (test passed for thread[n]) 
VCC is fully written; there are no partial mask updates. 
VCC physically resides in the SGPR register file, so when an instruction sources VCC, that 
counts against the limit on the total number of SGPRs that can be sourced for a given instruction. VCC physically resides in the highest two user SGPRs. 
Shader Hazard with VCC The user/compiler must prevent a scalar-ALU write to the SGPR holding VCC, immediately followed by a conditional branch using VCCZ. The hardware cannot detect this, and inserts the one required wait state (hardware does detect it when the SALU writes to VCC, it only fails to do this when the SALU instruction references the SGPRs that happen to hold VCC). 
3.10. Trap and Exception registers 
Each type of exception can be enabled or disabled independently by setting, or clearing, bits in the TRAPSTS register’s EXCP_EN field. This section describes the registers which control and report kernel exceptions. 
All Trap temporary SGPRs (TTMP*) are privileged for writes - they can be written only when in the trap handler (status.priv = 1). When not privileged, writes to these are ignored and reads return zero. TMA and TBA are read-only; they can be accessed through S_GETREG_B32. 
When a trap is taken (either user initiated, exception or host initiated), the shader hardware generates an S_TRAP instruction. This loads trap information into a pair of SGPRS: 
{TTMP1, TTMP0} = {3'h0, pc_rewind[3:0], HT[0],trapID[7:0], PC[47:0]}. 
HT is set to one for host initiated traps, and zero for user traps (s_trap) or exceptions. TRAP_ID is zero for exceptions, or the user/host trapID for those traps. When the trap handler is entered, the PC of the faulting instruction is: (PC - PC_rewind*4). 
STATUS . TRAP_EN - This bit indicates to the shader whether or not a trap handler is present. When one is not present, traps are not taken, no matter whether they’re floating point, user-, or host-initiated traps. When the trap handler is present, the wavefront uses an extra 16 SGPRs for trap processing. If trap_en == 0, all traps and exceptions are ignored, and s_trap is converted by hardware to NOP. 
MODE . EXCP_EN[8:0] - Floating point exception enables. Defines which exceptions and events cause a trap. 3.10. Trap and Exception registers 14 of 600
CDNA4 Instruction Set Architecture 
Bit 
0 
	Exception 
Invalid
	1 
	Input Denormal
	2 
	Divide by zero
	3 
	Overflow
	4 
	Underflow
	5 
	Inexact
	6 
	Integer divide by zero
	7 
	Address Watch - TC (L1) has witnessed a thread access to an 'address of interest'
	



3.10.1. Trap Status register 
The trap status register records previously seen traps or exceptions. It can be read and written by the kernel. 
Table 5. Exception Field Bits 
Field 
EXCP 
	Bits 
8:0 
	Description 
Status bits of which exceptions have occurred. These bits are sticky and accumulate results until the shader program clears them. These bits are accumulated regardless of the setting of EXCP_EN. These can be read or written without shader privilege. Bit Exception 0 invalid 
1 Input Denormal 
2 Divide by zero 
3 overflow 
4 underflow 
5 inexact 
6 integer divide by zero 
7 address watch 
8 memory violation
	SAVECTX 
	10 
	A bit set by the host command indicating that this wave must jump to its trap handler and save its context. This bit must be cleared by the trap handler using S_SETREG. Note - a shader can set this bit to 1 to cause a save-context trap, and due to hardware latency the shader may execute up to 2 additional instructions before taking the trap.
	ILLEGAL_INST 
	11 
	An illegal instruction has been detected.
	ADDR_WATCH1-3 
	14:12 
	Indicates that address watch 1, 2, or 3 has been hit. Bit 12 is address watch 1; bit 13 is 2; bit 14 is 3.
	EXCP_CYCLE 
	21:16 
	When a float exception occurs, this tells the trap handler on which cycle the exception occurred on. 0-3 for normal float operations, 0-7 for double float add, and 0-15 for double float muladd or transcendentals. This register records the cycle number of the first occurrence of an enabled (unmasked) exception. EXCP_CYCLE[1:0] Phase: threads 0-15 are in phase 0, 48-63 in phase 3. 
EXCP_CYCLE[3:2] Multi-slot pass. 
EXCP_CYCLE[5:4] Hybrid pass: used for machines running at lower rates.
	DP_RATE 
	31:29 
	Determines how the shader interprets the TRAP_STS.cycle. Different Vector Shader Processors (VSP) process instructions at different rates.
	



3.11. Memory Violations 
A Memory Violation is reported from: 
3.11. Memory Violations 15 of 600
CDNA4 Instruction Set Architecture 
• LDS alignment error. 
• Memory read/write/atomic alignment error. 
• Flat access where the address is invalid (does not fall in any aperture). 
• Write to a read-only memory address. 
Memory violations are not reported for instruction or scalar-data accesses. 
Memory Buffer to LDS does NOT return a memory violation if the LDS address is out of range, but masks off EXEC bits of threads that would go out of range. 
When a memory access is in violation, the appropriate memory (LDS or TC) returns MEM_VIOL to the wave. This is stored in the wave’s TRAPSTS.mem_viol bit. This bit is sticky, so once set to 1, it remains at 1 until the user clears it. 
There is a corresponding exception enable bit (EXCP_EN.mem_viol). If this bit is set when the memory returns with a violation, the wave jumps to the trap handler. 
Memory violations are not precise. The violation is reported when the LDS or TC processes the address; during this time, the wave may have processed many more instructions. When a mem_viol is reported, the Program Counter saved is that of the next instruction to execute; it has no relationship the faulting instruction. 
3.12. Hardware ID Registers 
The values below indicate where a wave is currently execution. It is not safe to rely on these values as they may change over the lifetime of a wave. 
Table 6. Hardware ID (HW_ID) 
Field 
WAVE_ID 
	Bits 
3:0 
	Description 
Wave buffer slot number
	SIMD_ID 
	5:4 
	SIMD which the wave is assigned to within the CU
	PIPE_ID 
	7:6 
	Pipeline from which the wave was dispatched
	CU_ID 
	11:8 
	Compute Unit the wave is assigned to
	SH_ID 
	12 
	Shader Array (within an SE) the wave is assigned to. Is set to zero.
	SE_ID 
	15:13 
	Shader Engine the wave is assigned to
	TG_ID 
	19:16 
	Thread-group ID
	VM_ID 
	23:20 
	Virtual Memory ID
	QUEUE_ID 
	26:24 
	Queue from which this wave was dispatched
	STATE_ID 
	29:27 
	State ID (UNUSED)
	ME_ID 
	31:30 
	Micro-engine ID
	



Table 7. XCC ID (XCC_ID) 
Field 
XCC_ID 
	Bits 
3:0 
	Description 
ID of this XCC
	



3.13. GPR Initialization 
When a compute shader wave is launched VGPR0 and a number of SGPRs are initialized. 3.12. Hardware ID Registers 16 of 600
CDNA4 Instruction Set Architecture 
Compute shaders have VGPR0 initialized with the X, Y and Z index within the workgroup: { 2’b00, Z[9:0], Y[9:0], X[9:0] }. 
Table 8. CS SGPR Load 
SGPR Order 
First 0.. 16 of 
	Description 
User data registers 
	Enable 
COMPUTE_PGM_RSRC2.user_sgpr
	then 
	work_group_id0[31:0] 
	COMPUTE_PGM_RSRC2.tgid_x_en
	then 
	work_group_id1[31:0] 
	COMPUTE_PGM_RSRC2.tgid_y_en
	then 
	work_group_id2[31:0] 
	COMPUTE_PGM_RSRC2.tgid_z_en
	then 
	{first_wave, 6’h00, wave_id_in_group[4:0], 2’h0, 14’h0, work-group_size_in_waves[5:0]}
	COMPUTE_PGM_RSRC2.tg_size_en
	TTMP4,5 
	0
	

	TTMP6 
	dispatch packet addr lo
	

	TTMP7 
	dispatch packet addr hi
	

	TTMP8 
	dispatch grid X[31:0]
	

	TTMP9 
	dispatch grid Y[31:0]
	

	TTMP10 
	dispatch grid Z[31:0]
	

	TTMP11 
	{ 26’b0, wave_id_in_workgroup[5:0] }
	

	



Other TTMPs are not initialized. 
3.13. GPR Initialization 17 of 600
CDNA4 Instruction Set Architecture Chapter 4. Program Flow Control 
All program flow control is programmed using scalar ALU instructions. This includes loops, branches, subroutine calls, and traps. The program uses SGPRs to store branch conditions and loop counters. Constants can be fetched from the scalar constant cache directly into SGPRs. 
4.1. Program Control 
The instructions in the table below control the priority and termination of a shader program, as well as provide support for trap handlers. 
Table 9. Control Instructions 
Instructions 
S_ENDPGM 
	Description 
Terminates the wavefront. It can appear anywhere in the kernel and can appear multiple times.
	S_ENDPGM_SAVE D
	Terminates the wavefront due to context save. It can appear anywhere in the kernel and can appear multiple times.
	S_NOP 
	Does nothing; it can be repeated in hardware up to 16 times.
	S_TRAP 
	Jumps to the trap handler.
	S_RFE 
	Returns from the trap handler
	S_SETPRIO 
	Modifies the priority of this wavefront: 0=lowest, 3 = highest.
	S_SLEEP 
	Causes the wavefront to sleep for 64 - 8128 clock cycles.
	S_SENDMSG 
	Sends a message (typically an interrupt) to the host CPU.
	S_WAKEUP 
	Causes one wave in a work-group to signal all other waves in the same work-group to wake up from S_SLEEP early. If waves are not sleeping, they are not affected by this instruction.
	



4.2. Branching 
Branching is done using one of the following scalar ALU instructions. 
Table 10. Branch Instructions 
Instructions 
S_BRANCH 
	Description 
Unconditional branch.
	S_CBRANCH_<test> 
	Conditional branch. Branch only if <test> is true. Tests are VCCZ, VCCNZ, EXECZ, EXECNZ, SCCZ, and SCCNZ.
	S_CBRANCH_CDBGSYS 
	Conditional branch, taken if the COND_DBG_SYS status bit is set.
	S_CBRANCH_CDBGUSER 
	Conditional branch, taken if the COND_DBG_USER status bit is set.
	S_CBRANCH_CDBGSYS_AND_USER 
	Conditional branch, taken only if both COND_DBG_SYS and COND_DBG_USER are set.
	S_SETPC 
	Directly set the PC from an SGPR pair.
	S_SWAPPC 
	Swap the current PC with an address in an SGPR pair.
	S_GETPC 
	Retrieve the current PC value (does not cause a branch).
	S_CBRANCH_{G,I}_FORK and S_CBRANCH_JOIN
	Conditional branch for complex branching.
	S_SETVSKIP 
	Set a bit that causes all vector instructions to be ignored. Useful alternative to branching.
	



4.1. Program Control 18 of 600
CDNA4 Instruction Set Architecture 
Instructions 
	Description
	S_CALL_B64 
	Jump to a subroutine, and save return address. SGPR_pair = PC+4; PC = PC+4+SIMM16*4.
	



For conditional branches, the branch condition can be determined by either scalar or vector operations. A scalar compare operation sets the Scalar Condition Code (SCC), which then can be used as a conditional branch condition. Vector compare operations set the VCC mask, and VCCZ or VCCNZ then can be used to determine branching. 
4.3. Workgroups 
Work-groups are collections of wavefronts running on the same compute unit which can synchronize and share data. Up to 16 wavefronts (1024 work-items) can be combined into a work-group. When multiple wavefronts are in a workgroup, the S_BARRIER instruction can be used to force each wavefront to wait until all other wavefronts reach the same instruction; then, all wavefronts continue. Any wavefront can terminate early using S_ENDPGM, and the barrier is considered satisfied when the remaining live waves reach their barrier instruction. 
4.4. Data Dependency Resolution 
Shader hardware resolves most data dependencies, but a few cases must be explicitly handled by the shader program. In these cases, the program must insert S_WAITCNT instructions to ensure that previous operations have completed before continuing. 
The shader has three counters that track the progress of issued instructions. S_WAITCNT waits for the values of these counters to be at, or below, specified values before continuing. 
These allow the shader writer to schedule long-latency instructions, execute unrelated work, and specify when results of long-latency operations are needed. 
Instructions of a given type return in order, but instructions of different types can complete out-of-order. 
• VM_CNT: Vector memory count. 
Determines when memory reads have returned data to VGPRs, or memory writes have completed. ◦ Incremented every time a vector-memory read or write (MUBUF, MTBUF, or FLAT format) instruction is issued. 
◦ Decremented for reads when the data has been written back to the VGPRs, and for writes when the data has been written to the L2 cache. Ordering: Memory reads and writes return in the order they were issued, including mixing reads and writes. 
• LGKM_CNT: (LDS, (K)constant, (M)essage) Determines when one of these low-latency instructions have completed. 
◦ Incremented by 1 for every LDS instruction issued, as well as by Dword-count for scalar-memory reads (1 for 1-dword loads, 2 for 2-dword or larger loads). S_memtime counts the same as an s_load_dwordx2. 
◦ Incremented by 1 for every FLAT instruction issued. 
◦ Decremented by 1 for LDS reads or atomic-with-return when the data has been returned to VGPRs. ◦ Incremented by 1 for each S_SENDMSG issued. Decremented by 1 when message is sent out. ◦ 
4.3. Workgroups 19 of 600
CDNA4 Instruction Set Architecture 
Decremented by 1 for LDS writes when the data has been written to LDS. 
◦ Decremented by 1 for each Dword returned from the data-cache (SMEM). 
Ordering: 
▪ Instructions of different types are returned out-of-order. 
▪ Instructions of the same type are returned in the order they were issued, except scalar-memory reads, which can return out-of-order (in which case only S_WAITCNT 0 is the only legitimate value). 
• EXP_CNT: VGPR-export count: unused 
4.5. Manually Inserted Wait States (NOPs) 
The hardware does not check for the following dependencies; they must be resolved by inserting NOPs or independent instructions. 
Table 11. Required Software-inserted Wait States 
First Instruction 
S_SETREG <*> 
	Second Instruction 
S_GETREG <same reg> 
	Wait 
2
	Notes
	S_SETREG <*> 
	S_SETREG <same reg> 
	2
	

	SET_VSKIP 
	S_GETREG MODE 
	2 
	Reads VSKIP from MODE.
	S_SETREG MODE.vskip 
	any vector op 
	2 
	Requires two nops or non-vector instructions.
	VALU that sets VCC or EXEC 
	VALU that uses EXECZ or VCCZ as a data source
	5
	

	VALU writes SGPR/VCC (readlane, cmp, add/sub, div_scale)
	V_{READ,WRITE}LANE using that SGPR/VCC as the lane select
	4
	

	VALU writes VCC (including v_div_scale) V_DIV_FMAS 
	

	4
	

	FLAT_STORE_X3 
FLAT_STORE_X4 
FLAT_ATOMIC_{F}CMPSWAP_X2 (and global & scratch stores/atomics) BUFFER_STORE_DWORD_X3 
BUFFER_STORE_DWORD_X4 
BUFFER_STORE_FORMAT_XYZ 
BUFFER_STORE_FORMAT_XYZW BUFFER_ATOMIC_{F}CMPSWAP_X2
	Write VGPRs holding writedata from those instructions.
	1 
	BUFFER_STORE_* operations that use an SGPR for "offset" do not require any wait states.
	FLAT_STORE_X3 
FLAT_STORE_X4 
(and global & scratch stores/atomics) FLAT_ATOMIC_{F}CMPSWAP_X2 BUFFER_STORE_DWORD_X3 
BUFFER_STORE_DWORD_X4 
BUFFER_STORE_FORMAT_XYZ 
BUFFER_STORE_FORMAT_XYZW BUFFER_ATOMIC_{F}CMPSWAP_X2
	VALU writes VGPRs holding writedata from those instructions.
	2 
	BUFFER_STORE_* operations that use an SGPR for "offset" do not require any wait states.
	VALU writes SGPR 
	VMEM reads that SGPR 
	5 
	Hardware assumes that there is no dependency here. If the VALU writes the SGPR that is used by a VMEM, the user must add five wait states.
	SALU writes M0 
	S_SENDMSG 
	1
	

	



4.5. Manually Inserted Wait States (NOPs) 20 of 600
CDNA4 Instruction Set Architecture 
First Instruction 
	Second Instruction 
	Wait 
	Notes
	VALU writes VGPR 
	VALU DPP reads that VGPR 
	2
	

	VALU writes EXEC 
	VALU DPP op 
	5 
	ALU does not forward EXEC to DPP.
	Mixed use of VCC: alias vs 
SGPR# 
v_readlane, v_readfirstlane 
v_cmp 
v_add*i/u 
v_sub*_i/u 
v_div_scale* (writes vcc)
	VALU which reads VCC as a constant (not as a carry-in which is 0 wait states).
	1 
	VCC can be accessed by name or by the logical SGPR which holds VCC. The data dependency check logic does not understand that these are the same register and do not prevent races.
	S_SETREG TRAPSTS 
	RFE, RFE_restore 
	1
	

	SALU writes M0 
	LDS "add-TID" instruction, 
buffer_store_LDS_dword, scratch or global with LDS = 1
	1
	

	SALU writes M0 
	S_MOVEREL 
	1
	

	VALU writes SGPR/VCC: 
v_readlane, v_readfirstlane, v_cmp, v_add*_i/u, v_sub*_i/u, v_div_scale*
	VALU reads SGPR as constant 
	2
	

	VALU reads SGPR as carry-in 
	0
	

	v_readlane, v_writelane reads SGPR as lane-select
	4
	

	v_cmpx 
	VALU reads EXEC as constant 
	2
	

	V_readlane, v_readfirstlane, v_writelane
	4
	

	Other VALU 
	0
	

	VALU writes VGPRn 
	v_readlane vsrc0 reads VGPRn 
	1
	

	VALU op which uses OPSEL or SDWA with changes the result’s bit position
	VALU op consumes result of that op
	1
	

	VALU Trans op 
	Non-trans VALU op consumes result of that op
	1
	

	V_CMPX (writes exec) 
	V_PERMLANE* 
	4
	

	VALU* writes vdst 
	V_PERMLANE* reads vdst 
	2
	

	



Table 12. Trans Ops 
V_EXP_F32 
	V_LOG_F32 
	V_RCP_F32 
	V_RCP_IFLAG_F32
	V_RSQ_F32 
	V_RCP_F64 
	V_RSQ_F64 
	V_SQRT_F32
	V_SQRT_F64 
	V_SIN_F32 
	V_COS_F32 
	V_RCP_F16
	V_SQRT_F16 
	V_RSQ_F16 
	V_LOG_F16 
	V_EXP_F16
	V_SIN_F16 
	V_COS_F16 
	V_EXP_LEGACY_F32 
	V_LOG_LEGACY_F32
	



4.6. Arbitrary Divergent Control Flow 
In the CDNA architecture, conditional branches are handled in one of the following ways. 
1. S_CBRANCH This case is used for simple control flow, where the decision to take a branch is based on a previous compare operation. This is the most common method for conditional branching. 2. S_CBRANCH_I/G_FORK and S_CBRANCH_JOIN This method, intended for complex, irreducible control flow graphs, is described in the rest of this section. The performance of this method is lower than that for S_CBRANCH on simple flow control; use it only when necessary. 
4.6. Arbitrary Divergent Control Flow 21 of 600
CDNA4 Instruction Set Architecture 
Conditional Branch (CBR) graphs are grouped into self-contained code blocks, denoted by FORK at the entrance point, and JOIN and the exit point. The shader compiler must add these instructions into the code. This method uses a six-deep stack and requires three SGPRs for each fork/join block. Fork/Join blocks can be hierarchically nested to any depth (subject to SGPR requirements); they also can coexist with other conditional flow control or computed jumps. 

Figure 3. Example of Complex Control Flow Graph 
The register requirements per wavefront are: 
• CSP [2:0] - control stack pointer. 
• Six stack entries of 128-bits each, stored in SGPRS: { exec[63:0], PC[47:2] } 
This method compares how many of the 64 threads go down the PASS path instead of the FAIL path; then, it selects the path with the fewer number of threads first. This means at most 50% of the threads are active, and this limits the necessary stack depth to Log264 = 6. 
The following pseudo-code shows the details of CBRANCH Fork and Join operations. 
S_CBRANCH_G_FORK arg0, arg1 
  // arg1 is an sgpr-pair which holds 64bit (48bit) target address 
S_CBRANCH_I_FORK arg0, #target_addr_offset[17:2] 
  // target_addr_offset: 16b signed immediate offset 
// PC: in this pseudo-code is pointing to the cbranch_*_fork instruction 
mask_pass = SGPR[arg0] & exec 
mask_fail = ~SGPR[arg0] & exec 
if (mask_pass == exec) 
  I_FORK : PC += 4 + target_addr_offset 
  G_FORK: PC = SGPR[arg1] 
else if (mask_fail == exec) 
  PC += 4 
4.6. Arbitrary Divergent Control Flow 22 of 600
CDNA4 Instruction Set Architecture 
else if (bitcount(mask_fail) < bitcount(mask_pass)) 
  exec = mask_fail 
  I_FORK : SGPR[CSP*4] = { (pc + 4 + target_addr_offset), mask_pass } 
  G_FORK: SGPR[CSP*4] = { SGPR[arg1], mask_pass } 
  CSP++ 
  PC += 4 
else 
  exec = mask_pass 
  SGPR[CSP*4] = { (pc+4), mask_fail } 
  CSP++ 
  I_FORK : PC += 4 + target_addr_offset 
  G_FORK: PC = SGPR[arg1] 
S_CBRANCH_JOIN arg0 
if (CSP == SGPR[arg0]) // SGPR[arg0] holds the CSP value when the FORK started 
  PC += 4 // this is the 2nd time to JOIN: continue with pgm 
else 
  CSP -- // this is the 1st time to JOIN: jump to other FORK path 
  {PC, EXEC} = SGPR[CSP*4] // read 128-bits from 4 consecutive SGPRs 
4.6. Arbitrary Divergent Control Flow 23 of 600
CDNA4 Instruction Set Architecture Chapter 5. Scalar ALU Operations 
Scalar ALU (SALU) instructions operate on a single value per wavefront. These operations consist of 32-bit integer arithmetic and 32- or 64-bit bit-wise operations. The SALU also can perform operations directly on the Program Counter, allowing the program to create a call stack in SGPRs. Many operations also set the Scalar Condition Code bit (SCC) to indicate the result of a comparison, a carry-out, or whether the instruction result was zero. 
5.1. SALU Instruction Formats 
SALU instructions are encoded in one of five microcode formats, shown below: 
Each of these instruction formats uses some of these fields: 
Field 
OP 
	Description 
Opcode: instruction to be executed.
	SDST 
	Destination SGPR.
	SSRC0 
	First source operand.
	SSRC1 
	Second source operand.
	SIMM16 
	Signed immediate 16-bit integer constant.
	



The lists of similar instructions sometimes use a condensed form using curly braces { } to express a list of possible names. For example, S_AND_{B32, B64} defines two legal instructions: S_AND_B32 and S_AND_B64. 
5.2. Scalar ALU Operands 
Valid operands of SALU instructions are: 
• SGPRs, including trap temporary SGPRs. 
• Mode register. 
• Status register (read-only). 
• M0 register. 
• TrapSts register. 
5.1. SALU Instruction Formats 24 of 600
CDNA4 Instruction Set Architecture 
• EXEC mask. 
• VCC mask. 
• SCC. 
• PC. 
• Inline constants: integers from -16 to 64, and a some floating point values. 
• VCCZ, EXECZ, and SCC. 
• Hardware registers. 
• 32-bit literal constant. 
In the table below, 0-127 can be used as scalar sources or destinations; 128-255 can only be used as sources. 
Table 13. Scalar Operands 
Scalar 
Dest 
(7 bits)
	Code 
0 - 101 
	Meaning 
SGPR 0 to 101 
	Description 
Scalar GPRs
	102 
	FLAT_SCRATCH_LO 
	Holds the low Dword of the flat-scratch memory descriptor
	103 
	FLAT_SCRATCH_HI 
	Holds the high Dword of the flat-scratch memory descriptor
	104 
	XNACK_MASK_LO 
	Holds the low Dword of the XNACK mask.
	105 
	XNACK_MASK_HI 
	Holds the high Dword of the XNACK mask.
	106 
	VCC_LO 
	Holds the low Dword of the vector condition code
	107 
	VCC_HI 
	Holds the high Dword of the vector condition code
	108-123 
	TTMP0 to TTMP15 
	Trap temps (privileged)
	124 
	M0 
	Holds the low Dword of the flat-scratch memory descriptor
	125 
	reserved 
	reserved
	126 
	EXEC_LO 
	Execute mask, low Dword
	127 
	EXEC_HI 
	Execute mask, high Dword
	

	128 
	0 
	zero
	

	129-192 
	int 1 to 64 
	Positive integer values.
	

	193-208 
	int -1 to -16 
	Negative integer values.
	

	209-234 
	reserved 
	Unused.
	

	235 
	SHARED_BASE 
	Memory Aperture definition.
	

	236 
	SHARED_LIMIT
	

	237 
	PRIVATE_BASE
	

	238 
	PRIVATE_LIMIT
	

	239 
	Reserved 
	Reserved
	

	240 
	0.5 
	single or double floats
	

	241 
	-0.5
	

	242 
	1.0
	

	243 
	-1.0
	

	244 
	2.0
	

	245 
	-2.0
	

	246 
	4.0
	

	247 
	-4.0
	

	248 
	1.0 / (2 * PI)
	

	249-250 
	reserved 
	unused
	

	251 
	VCCZ 
	{ zeros, VCCZ }
	



5.2. Scalar ALU Operands 25 of 600
CDNA4 Instruction Set Architecture 


	Code 
	Meaning 
	Description
	

	252 
	EXECZ 
	{ zeros, EXECZ }
	

	253 
	SCC 
	{ zeros, SCC }
	

	254 
	reserved 
	unused
	

	255 
	Literal 
	constant 32-bit constant from instruction stream.
	



The SALU cannot use VGPRs or LDS. SALU instructions can use a 32-bit literal constant. This constant is part of the instruction stream and is available to all SALU microcode formats except SOPP and SOPK. Literal constants are used by setting the source instruction field to "literal" (255), and then the following instruction dword is used as the source value. 
If any source SGPR is out-of-range, the value of SGPR0 is used instead. 
If the destination SGPR is out-of-range, no SGPR is written with the result. However, SCC and EXEC (for saveexec) are written. 
If an instruction uses 64-bit data in SGPRs, the SGPR pair must be aligned to an even boundary. For example, it is legal to use SGPRs 2 and 3 or 8 and 9 (but not 11 and 12) to represent 64-bit data. 
5.3. Scalar Condition Code (SCC) 
The scalar condition code (SCC) is written as a result of executing most SALU instructions. The SCC is set by many instructions: 
• Compare operations: 1 = true. 
• Arithmetic operations: 1 = carry out. 
◦ SCC = overflow for signed add and subtract operations. For add, overflow = both operands are of the same sign, and the MSB (sign bit) of the result is different than the sign of the operands. For subtract (AB), overflow = A and B have opposite signs and the resulting sign is not the same as the sign of A. • Bit/logical operations: 1 = result was not zero. 
5.4. Integer Arithmetic Instructions 
This section describes the arithmetic operations supplied by the SALU. The table below shows the scalar integer arithmetic instructions: 
Table 14. Integer Arithmetic Instructions 
Instruction 
S_ADD_I32 
	Encoding 
SOP2 
	Sets SCC? 
y 
	Operation 
D = S0 + S1, SCC = overflow.
	S_ADD_U32 
	SOP2 
	y 
	D = S0 + S1, SCC = carry out.
	S_ADDC_U32 
	SOP2 
	y 
	D = S0 + S1 + SCC = overflow.
	S_SUB_I32 
	SOP2 
	y 
	D = S0 - S1, SCC = overflow.
	S_SUB_U32 
	SOP2 
	y 
	D = S0 - S1, SCC = carry out.
	S_SUBB_U32 
	SOP2 
	y 
	D = S0 - S1 - SCC = carry out.
	S_ABSDIFF_I32 
	SOP2 
	y 
	D = abs (s1 - s2), SCC = result not zero.
	



5.3. Scalar Condition Code (SCC) 26 of 600
CDNA4 Instruction Set Architecture 
Instruction 
	Encoding 
	Sets SCC? 
	Operation
	S_MIN_I32 
S_MIN_U32
	SOP2 
	y 
	D = (S0 < S1) ? S0 : S1. SCC = 1 if S0 was min.
	S_MAX_I32 
S_MAX_U32
	SOP2 
	y 
	D = (S0 > S1) ? S0 : S1. SCC = 1 if S0 was max.
	S_MUL_I32 
	SOP2 
	n 
	D = S0 * S1. Low 32 bits of result.
	S_ADDK_I32 
	SOPK 
	y 
	D = D + simm16, SCC = overflow. Sign extended version of simm16.
	S_MULK_I32 
	SOPK 
	n 
	D = D * simm16. Return low 32bits. Sign extended version of simm16.
	S_ABS_I32 
	SOP1 
	y 
	D.i = abs (S0.i). SCC=result not zero.
	S_SEXT_I32_I8 
	SOP1 
	n 
	D = { 24{S0[7]}, S0[7:0] }.
	S_SEXT_I32_I16 
	SOP1 
	n 
	D = { 16{S0[15]}, S0[15:0] }.
	



5.5. Conditional Instructions 
Conditional instructions use the SCC flag to determine whether to perform the operation, or (for CSELECT) which source operand to use. 
Table 15. Conditional Instructions 
Instruction 
S_CSELECT_{B32, B64} 
	Encoding SOP2 
	Sets SCC? n 
	Operation 
D = SCC ? S0 : S1.
	S_CMOVK_I32 
	SOPK 
	n 
	if (SCC) D = signext(simm16).
	S_CMOV_{B32,B64} 
	SOP1 
	n 
	if (SCC) D = S0, else NOP.
	



5.6. Comparison Instructions 
These instructions compare two values and set the SCC to 1 if the comparison yielded a TRUE result. 
Table 16. Conditional Instructions 
Instruction 
S_CMP_EQ_U64, S_CMP_NE_U64 
	Encoding 
SOPC 
	Sets SCC? 
y 
	Operation 
Compare two 64-bit source values. SCC = S0 <cond> S1.
	S_CMP_{EQ,NE,GT,GE,LE,LT}_{I3 2,U32}
	SOPC 
	y 
	Compare two source values. SCC = S0 <cond> S1.
	S_CMPK_{EQ,NE,GT,GE,LE,LT}_{I 32,U32}
	SOPK 
	y 
	Compare Dest SGPR to a constant. SCC = DST <cond> simm16. simm16 is zero-extended (U32) or sign-extended (I32).
	S_BITCMP0_{B32,B64} 
	SOPC 
	y 
	Test for "is a bit zero". SCC = !S0[S1].
	S_BITCMP1_{B32,B64} 
	SOPC 
	y 
	Test for "is a bit one". SCC = S0[S1].
	



5.7. Bit-Wise Instructions 
Bit-wise instructions operate on 32- or 64-bit data without interpreting it has having a type. For bit-wise operations if noted in the table below, SCC is set if the result is nonzero. 
Table 17. Bit-Wise Instructions 
5.5. Conditional Instructions 27 of 600
CDNA4 Instruction Set Architecture 
Instruction 
S_MOV_{B32,B64} 
	Encoding 
SOP1 
	Sets SCC? n 
	Operation 
D = S0
	S_MOVK_I32 
	SOPK 
	n 
	D = signext(simm16)
	{S_AND,S_OR,S_XOR}_{B32,B64} 
	SOP2 
	y 
	D = S0 & S1, S0 OR S1, S0 XOR S1
	{S_ANDN2,S_ORN2}_{B32,B64} 
	SOP2 
	y 
	D = S0 & ~S1, S0 OR ~S1, S0 XOR ~S1,
	{S_NAND,S_NOR,S_XNOR}_{B32,B64} 
	SOP2 
	y 
	D = ~(S0 & S1), ~(S0 OR S1), ~(S0 XOR S1)
	S_LSHL_{B32,B64} 
	SOP2 
	y 
	D = S0 << S1[4:0], [5:0] for B64.
	S_LSHR_{B32,B64} 
	SOP2 
	y 
	D = S0 >> S1[4:0], [5:0] for B64.
	S_ASHR_{I32,I64} 
	SOP2 
	y 
	D = sext(S0 >> S1[4:0]) ([5:0] for I64).
	S_BFM_{B32,B64} 
	SOP2 
	n 
	Bit field mask. D = ((1 << S0[4:0]) - 1) << S1[4:0].
	S_BFE_U32, S_BFE_U64 
S_BFE_I32, S_BFE_I64 
(signed/unsigned)
	SOP2 
	y 
	Bit Field Extract, then sign-extend result for I32/64 instructions. 
S0 = data, 
S1[5:0] = offset, S1[22:16]= width.
	S_NOT_{B32,B64} 
	SOP1 
	y 
	D = ~S0.
	S_WQM_{B32,B64} 
	SOP1 
	y 
	D = wholeQuadMode(S0). If any bit in a group of four is set to 1, set the resulting group of four bits all to 1.
	S_QUADMASK_{B32,B64} 
	SOP1 
	y 
	D[0] = OR(S0[3:0]), D[1]=OR(S0[7:4]), etc.
	S_BREV_{B32,B64} 
	SOP1 
	n 
	D = S0[0:31] are reverse bits.
	S_BCNT0_I32_{B32,B64} 
	SOP1 
	y 
	D = CountZeroBits(S0).
	S_BCNT1_I32_{B32,B64} 
	SOP1 
	y 
	D = CountOneBits(S0).
	S_FF0_I32_{B32,B64} 
	SOP1 
	n 
	D = Bit position of first zero in S0 starting from LSB. -1 if not found.
	S_FF1_I32_{B32,B64} 
	SOP1 
	n 
	D = Bit position of first one in S0 starting from LSB. -1 if not found.
	S_FLBIT_I32_{B32,B64} 
	SOP1 
	n 
	Find last bit. D = the number of zeros before the first one starting from the MSB. Returns -1 if none.
	S_FLBIT_I32 
S_FLBIT_I32_I64
	SOP1 
	n 
	Count how many bits in a row (from MSB to LSB) are the same as the sign bit. Return -1 if the input is zero or all 1’s (-1). 32-bit pseudo-code: 
if (S0 == 0 || S0 == -1) D = -1 
else 
D = 0 
for (I = 31 .. 0) 
if (S0[I] == S0[31]) 
D++ 
else break 
This opcode behaves the same as V_FFBH_I32.
	S_BITSET0_{B32,B64} 
	SOP1 
	n 
	D[S0[4:0], [5:0] for B64] = 0
	S_BITSET1_{B32,B64} 
	SOP1 
	n 
	D[S0[4:0], [5:0] for B64] = 1
	S_{and,or,xor,andn2,orn2,nand, 
nor,xnor}_SAVEEXEC_B64
	SOP1 
	y 
	Save the EXEC mask, then apply a bit-wise operation to it. 
D = EXEC 
EXEC = S0 <op> EXEC 
SCC = (exec != 0)
	S_{ANDN{1,2}_WREXEC_B64 
	SOP1 
	y 
	N1: EXEC, D = ~S0 & EXEC 
N2: EXEC, D = S0 & ~EXEC 
Both D and EXEC get the same result. SCC = (result != 0).
	



5.7. Bit-Wise Instructions 28 of 600
CDNA4 Instruction Set Architecture 
Instruction 
	Encoding 
	Sets SCC? 
	Operation
	S_MOVRELS_{B32,B64} 
S_MOVRELD_{B32,B64}
	SOP1 
	n 
	Move a value into an SGPR relative to the value in M0. MOVERELS: D = SGPR[S0+M0] 
MOVERELD: SGPR[D+M0] = S0  
Index must be even for 64. M0 is an unsigned index.
	



5.8. Access Instructions 
These instructions access hardware internal registers. 
Table 18. Hardware Internal Registers 
Instruction 
S_GETREG_B32 
	Encoding 
SOPK* 
	Sets 
SCC? 
n 
	Operation 
Read a hardware register into the LSBs of D.
	S_SETREG_B32 
	SOPK* 
	n 
	Write the LSBs of D into a hardware register. (Note that D is a source SGPR.) Must add an S_NOP between two consecutive S_SETREG to the same register.
	S_SETREG_IMM32_B32 
	SOPK* 
	n 
	S_SETREG where 32-bit data comes from a literal constant (so this is a 64-bit instruction format).
	



The hardware register is specified in the DEST field of the instruction, using the values in the table above. Some bits of the DEST specify which register to read/write, but additional bits specify which bits in the register to read/write: 
SIMM16 = {size[4:0], offset[4:0], hwRegId[5:0]}; offset is 0..31, size is 1..32. 
Table 19. Hardware Register Values 
Code 0 
	Register 
reserved
	Description
	1 
	MODE 
	R/W.
	2 
	STATUS 
	Read only.
	3 
	TRAPSTS 
	R/W.
	4 
	HW_ID 
	Read only. Debug only.
	5 
	GPR_ALLOC 
	Read only. {sgpr_size, sgpr_base, vgpr_size, vgpr_base }.
	6 
	LDS_ALLOC 
	Read only. {lds_size, lds_base}.
	7 
	IB_STS 
	Read only. {lgkm_cnt, exp_cnt, vm_cnt}.
	8 - 15 
	

	reserved.
	16 
	TBA_LO 
	Trap base address register [31:0].
	17 
	TBA_HI 
	Trap base address register [47:32].
	18 
	TMA_LO 
	Trap memory address register [31:0].
	19 
	TMA_HI 
	Trap memory address register [47:32].
	20 
	XCC_ID 
	ID of the XCC this wave is running on
	21 
	PERF_SNAPSHOT_DATA 
	Stochastic Performance sampling data
	22 
	PERF_SNAPSHOT_DATA1 
	Stochastic Performance sampling data1
	23 
	PERF_SNAPSHOT_PC_LO 
	Stochastic Performance sampling program counter
	24 
	PERF_SNAPSHOT_PC_HI 
	Stochastic Performance sampling program counter
	



5.8. Access Instructions 29 of 600
CDNA4 Instruction Set Architecture Table 20. IB_STS 
Code 
VM_CNT 
	Register 23:22, 
3:0
	Description 
Number of VMEM instructions issued but not yet returned.
	LGKM_CNT 
	11:8 
	LDS, Constant-memory and Message instructions issued-but-not-completed count.
	



Table 21. GPR_ALLOC 
Code 
VGPR_BASE 
	Register 5:0 
	Description 
Physical address of first VGPR assigned to this wavefront, as [7:2]
	VGPR_SIZE 
	11:6 
	Number of VGPRs assigned to this wavefront, as [7:2]. 0=4 VGPRs, 1=8 VGPRs, etc.
	ACCV_OFF 
	17:12 
	Accumulation VGPR offset from VGPR_BASE, in units of 4 VGPRs.
	SGPR_BASE 
	23:18 
	Physical address of first SGPR assigned to this wavefront, as [8:3].
	SGPR_SIZE 
	27:24 
	Number of SGPRs assigned to this wave, as [7:4]. 0=16 SGPRs, 1=32 SGPRs, etc.
	



Table 22. LDS_ALLOC 
Code 
LDS_BASE 
	Register 8:0 
	Description 
Physical address of first LDS location assigned to this wavefront, in units of 64 Dwords.
	LDS_SIZE 
	21:12 
	Amount of LDS space assigned to this wavefront, in units of 64 Dwords.
	



5.8. Access Instructions 30 of 600
CDNA4 Instruction Set Architecture Chapter 6. Vector ALU Operations 
Vector ALU instructions (VALU) perform an arithmetic or logical operation on data for each of 64 threads and write results back to VGPRs, SGPRs or the EXEC mask. 
6.1. Microcode Encodings 
Most VALU instructions are available in two encodings: VOP3 which uses 64-bits of instruction and has the full range of capabilities, and one of three 32-bit encodings that offer a restricted set of capabilities. A few instructions are only available in the VOP3 encoding. 
When an instruction is available in two microcode formats, it is up to the user to decide which to use. It is recommended to use the 32-bit encoding whenever possible. 
The microcode encodings are shown below. 
VOP2 is for instructions with two inputs and a single vector destination. Instructions that have a carry-out implicitly write the carry-out to the VCC register. 
VOP1 is for instructions with no inputs or a single input and one destination. 
VOPC is for comparison instructions. 

VOP3 is for instructions with up to three inputs, input modifiers (negate and absolute value), and output modifiers. There are two forms of VOP3: one which uses a scalar destination field (used only for div_scale, integer add and subtract); this is designated VOP3b. All other instructions use the common form, designated VOP3a. 
Any of the 32-bit microcode formats may use a 32-bit literal constant, but not VOP3. 
VOP3P is for instructions that use "packed math": They perform the operation on a pair of input values that are packed into the high and low 16-bits of each operand; the two 16-bit results are written to a single VGPR as two packed values. 
6.1. Microcode Encodings 31 of 600
CDNA4 Instruction Set Architecture VOP3P-MAI is a variation of the VOP3P format for use with the Matrix Arithmetic Instructions (MAI). 

6.2. Operands 
All VALU instructions take at least one input operand (except V_NOP and V_CLREXCP). The data-size of the operands is explicitly defined in the name of the instruction. For example, V_MUL_F32 operates on 32-bit floating point data. 
6.2.1. Instruction Inputs 
VALU instructions can use any of the following sources for input, subject to restrictions listed below: 
• VGPRs. 
• SGPRs. 
• Inline constants - constant selected by a specific VSRC value. 
• Literal constant - 32-bit value in the instruction stream. When a literal constant is used with a 64bit instruction, the literal is expanded to 64 bits by: padding the LSBs with zeros for floats, padding the MSBs with zeros for unsigned ints, and by sign-extending signed ints. 
• M0. 
• EXEC mask. 
Limitations 
• At most one SGPR can be read per instruction, but the value can be used for more than one operand. • At most one literal constant can be used, and only when an SGPR or M0 is not used as a source. 
Limitations for Constants 
VALU "ADDC", "SUBB" and CNDMASK all implicitly use an 
SGPR value (VCC), so these instructions cannot use an additional SGPR or literal constant. 
Instructions using the VOP3 form and also using floating-point inputs have the option of applying absolute value (ABS field) or negate (NEG field) to any of the input operands. 
Limitations for SDWA and OPSEL 
DOT instructions must not use SDWA or OPSEL. 
VALU ops which use SDWA or OPSEL must not consume the result of that instruction in the next VALU instruction - there must be at least on independent instruction or V_NOP between them. 
6.2. Operands 32 of 600
CDNA4 Instruction Set Architecture 6.2.1.1. Literal Expansion to 64 bits 
Literal constants are 32-bits, but they can be used as sources which normally require 64-bit data: 
• 64 bit float: the lower 32-bit are padded with zero. 
• 64-bit unsigned integer: zero extended to 64 bits 
• 64-bit signed integer: sign extended to 64 bits 
6.2.2. Instruction Outputs 
VALU instructions typically write their results to VGPRs specified in the VDST field of the microcode word. A thread only writes a result if the associated bit in the EXEC mask is set to 1. 
All V_CMPX instructions write the result of their comparison (one bit per thread) to both an SGPR (or VCC) and the EXEC mask. 
Instructions producing a carry-out (integer add and subtract) write their result to VCC when used in the VOP2 form, and to an arbitrary SGPR-pair when used in the VOP3 form. 
When the VOP3 form is used, instructions with a floating-point result can apply an output modifier (OMOD field) that multiplies the result by: 0.5, 1.0, 2.0 or 4.0. Optionally, the result can be clamped (CLAMP field) to the range [0.0, +1.0]. 
Output modifiers apply only to floating point results and are ignored for integer or bit results. Output modifiers are not compatible with output denormals: if output denormals are enabled, then output modifiers are ignored. If output denormals are disabled, then the output modifier is applied and denormals are flushed to zero. Output modifiers are not IEEE compatible: -0 is flushed to +0. Output modifiers are ignored if the IEEE mode bit is set to 1. 
In the table below, all codes can be used when the vector source is nine bits; codes 0 to 255 can be the scalar source if it is eight bits; codes 0 to 127 can be the scalar source if it is seven bits; and codes 256 to 511 can be the vector source or destination. 
Table 23. Instruction Operands 
Value 
0-101 
	Name 
SGPR 
	Description 
0 .. 101
	102 
	FLAT_SCRATCH_LO 
	Flat Scratch[31:0].
	103 
	FLAT_SCRATCH_HI 
	Flat Scratch[63:32].
	104 
	XNACK_MASK_LO
	

	105 
	XNACK_MASK_HI
	

	106 
	VCC_LO 
	vcc[31:0].
	107 
	VCC_HI 
	vcc[63:32].
	108-123 
	TTMP0 to TTMP 15 
	Trap handler temps (privileged).
	124 
	M0
	

	125 
	reserved
	

	126 
	EXEC_LO 
	exec[31:0].
	127 
	EXEC_HI 
	exec[63:32].
	128 
	0
	

	



6.2. Operands 33 of 600
CDNA4 Instruction Set Architecture 
Value 
	Name 
	Description
	129-192 
	int 1.. 64 
	Integer inline constants.
	193-208 
	int -1 .. -16
	209-234 
	reserved 
	Unused.
	235 
	SHARED_BASE 
	Memory Aperture definition.
	236 
	SHARED_LIMIT
	237 
	PRIVATE_BASE
	238 
	PRIVATE_LIMIT
	239 
	Reserved 
	Reserved
	240 
	0.5 
	Single, double, or half-precision inline floats. 
1/(2*PI) is 0.15915494. 
The exact value used is: 
half: 0x3118 
single: 0x3e22f983 
double: 0x3fc45f306dc9c882
	241 
	-0.5
	242 
	1.0
	243 
	-1.0
	244 
	2.0
	245 
	-2.0
	246 
	4.0
	247 
	-4.0
	248 
	1/(2*PI)
	249 
	SDWA 
	Sub Dword Address (only valid as Source-0)
	250 
	DPP 
	DPP over 16 lanes (only valid as Source-0)
	251 
	VCCZ 
	{ zeros, VCCZ }
	252 
	EXECZ 
	{ zeros, EXECZ }
	253 
	SCC 
	{ zeros, SCC }
	254 
	Reserved 
	Reserved
	255 
	Literal 
	constant 32-bit constant from instruction stream.
	256-511 
	VGPR 
	0 .. 255
	



6.2.3. Out-of-Range GPRs 
When a source VGPR is out-of-range, the instruction uses as input the value from VGPR0. When the destination GPR is out-of-range, the instruction executes but does not write the results. 
6.3. Instructions 
The table below lists the complete VALU instruction set by microcode encoding, except for VOP3P instructions which are listed in a later section. 
Table 24. VALU Instruction Set 
VOP3 
V_ADD3_U32 
	VOP3 - 2 operands 
V_ADD_F64 
	VOP2 
V_ADDC_CO_U32 
	VOP1 
V_ACCVGPR_MOV_B32
	V_ADD_LSHL_U32 
	V_ADD_I16 
	V_ADD_CO_U32 
	V_BFREV_B32
	V_ALIGNBIT_B32 
	V_ADD_I32 
	V_ADD_F16 
	V_CEIL_F16
	V_ALIGNBYTE_B32 
	V_ASHRREV_I64 
	V_ADD_F32 
	V_CEIL_F32
	V_AND_OR_B32 
	V_BCNT_U32_B32 
	V_ADD_U16 
	V_CEIL_F64
	V_ASHR_PK_I8_I32 
	V_BFM_B32 
	V_ADD_U32 
	V_CLREXCP
	V_ASHR_PK_U8_I32 
	V_CVT_PKACCUM_U8_F32 
	V_AND_B32 
	V_COS_F16
	



6.3. Instructions 34 of 600
CDNA4 Instruction Set Architecture 
VOP3 
	VOP3 - 2 operands 
	VOP2 
	VOP1
	V_BFE_I32 
	V_CVT_PKNORM_I16_F16 
	V_ASHRREV_I16 
	V_COS_F32
	V_BFE_U32 
	V_CVT_PKNORM_I16_F32 
	V_ASHRREV_I32 
	V_CVT_F16_F32
	V_BFI_B32 
	V_CVT_PKNORM_U16_F16 
	V_CNDMASK_B32 
	V_CVT_F16_I16
	V_BITOP3_B16 
	V_CVT_PKNORM_U16_F32 
	V_DOT2C_F32_BF16 
	V_CVT_F16_U16
	V_BITOP3_B32 
	V_CVT_PKRTZ_F16_F32 
	V_DOT2C_F32_F16 
	V_CVT_F32_BF16
	V_CUBEID_F32 
	V_CVT_PK_BF16_F32 
	V_DOT2C_I32_I16 
	V_CVT_F32_BF8
	V_CUBEMA_F32 
	V_CVT_PK_BF8_F32 
	V_DOT4C_I32_I8 
	V_CVT_F32_F16
	V_CUBESC_F32 
	V_CVT_PK_F16_F32 
	V_DOT8C_I32_I4 
	V_CVT_F32_F64
	V_CUBETC_F32 
	V_CVT_PK_FP8_F32 
	V_FMAAK_F32 
	V_CVT_F32_FP8
	V_CVT_PK_U8_F32 
	V_CVT_PK_I16_I32 
	V_FMAC_F32 
	V_CVT_F32_I32
	V_CVT_SCALEF32_2XPK16_BF6_F32 
	V_CVT_PK_U16_U32 
	V_FMAC_F64 
	V_CVT_F32_U32
	V_CVT_SCALEF32_2XPK16_FP6_F32 
	V_CVT_SCALEF32_F16_BF8 
	V_FMAMK_F32 
	V_CVT_F32_UBYTE0
	V_CVT_SCALEF32_PK_BF8_F32 
	V_CVT_SCALEF32_F16_FP8 
	V_LDEXP_F16 
	V_CVT_F32_UBYTE1
	V_CVT_SCALEF32_PK_FP4_F32 
	V_CVT_SCALEF32_F32_BF8 
	V_LSHLREV_B16 
	V_CVT_F32_UBYTE2
	V_CVT_SCALEF32_PK_FP8_F32 
	V_CVT_SCALEF32_F32_FP8 
	V_LSHLREV_B32 
	V_CVT_F32_UBYTE3
	V_CVT_SCALEF32_SR_BF8_BF16 
	V_CVT_SCALEF32_PK32_BF16_BF6 V_LSHRREV_B16 
	

	V_CVT_F64_F32
	V_CVT_SCALEF32_SR_BF8_F16 
	V_CVT_SCALEF32_PK32_BF16_FP6 V_LSHRREV_B32 
	

	V_CVT_F64_I32
	V_CVT_SCALEF32_SR_BF8_F32 
	V_CVT_SCALEF32_PK32_BF6_BF16 V_MAC_F16 
	

	V_CVT_F64_U32
	V_CVT_SCALEF32_SR_FP8_BF16 
	V_CVT_SCALEF32_PK32_BF6_F16 
	V_MADAK_F16 
	V_CVT_FLR_I32_F32
	V_CVT_SCALEF32_SR_FP8_F16 
	V_CVT_SCALEF32_PK32_F16_BF6 
	V_MADMK_F16 
	V_CVT_I16_F16
	V_CVT_SCALEF32_SR_FP8_F32 
	V_CVT_SCALEF32_PK32_F16_FP6 
	V_MAX_F16 
	V_CVT_I32_F32
	V_CVT_SCALEF32_SR_PK32_BF6_BF16 
	V_CVT_SCALEF32_PK32_F32_BF6 
	V_MAX_F32 
	V_CVT_I32_F64
	V_CVT_SCALEF32_SR_PK32_BF6_F16 
	V_CVT_SCALEF32_PK32_F32_FP6 
	V_MAX_I16 
	V_CVT_NORM_I16_F16
	V_CVT_SCALEF32_SR_PK32_BF6_F32 
	V_CVT_SCALEF32_PK32_FP6_BF16 V_MAX_I32 
	

	V_CVT_NORM_U16_F16
	V_CVT_SCALEF32_SR_PK32_FP6_BF16 
	V_CVT_SCALEF32_PK32_FP6_F16 
	V_MAX_U16 
	V_CVT_OFF_F32_I4
	V_CVT_SCALEF32_SR_PK32_FP6_F16 
	V_CVT_SCALEF32_PK_BF16_BF8 
	V_MAX_U32 
	V_CVT_PK_F32_BF8
	V_CVT_SCALEF32_SR_PK32_FP6_F32 
	V_CVT_SCALEF32_PK_BF16_FP4 
	V_MIN_F16 
	V_CVT_PK_F32_FP8
	V_CVT_SCALEF32_SR_PK_FP4_BF16 
	V_CVT_SCALEF32_PK_BF16_FP8 
	V_MIN_F32 
	V_CVT_RPI_I32_F32
	V_CVT_SCALEF32_SR_PK_FP4_F16 
	V_CVT_SCALEF32_PK_BF8_BF16 
	V_MIN_I16 
	V_CVT_U16_F16
	V_CVT_SCALEF32_SR_PK_FP4_F32 
	V_CVT_SCALEF32_PK_BF8_F16 
	V_MIN_I32 
	V_CVT_U32_F32
	V_DIV_FIXUP_F16 
	V_CVT_SCALEF32_PK_F16_BF8 
	V_MIN_U16 
	V_CVT_U32_F64
	V_DIV_FIXUP_F32 
	V_CVT_SCALEF32_PK_F16_FP4 
	V_MIN_U32 
	V_EXP_F16
	V_DIV_FIXUP_F64 
	V_CVT_SCALEF32_PK_F16_FP8 
	V_MUL_F16 
	V_EXP_F32
	V_DIV_FIXUP_LEGACY_F16 
	V_CVT_SCALEF32_PK_F32_BF8 
	V_MUL_F32 
	V_FFBH_I32
	V_DIV_FMAS_F32 
	V_CVT_SCALEF32_PK_F32_FP4 
	V_MUL_HI_I32_I24 
	V_FFBH_U32
	V_DIV_FMAS_F64 
	V_CVT_SCALEF32_PK_F32_FP8 
	V_MUL_HI_U32_U24 
	V_FFBL_B32
	V_DIV_SCALE_F32 
	V_CVT_SCALEF32_PK_FP4_BF16 
	V_MUL_I32_I24 
	V_FLOOR_F16
	V_DIV_SCALE_F64 
	V_CVT_SCALEF32_PK_FP4_F16 
	V_MUL_LO_U16 
	V_FLOOR_F32
	V_FMA_F16 
	V_CVT_SCALEF32_PK_FP8_BF16 
	V_MUL_U32_U24 
	V_FLOOR_F64
	V_FMA_F32 
	V_CVT_SCALEF32_PK_FP8_F16 
	V_OR_B32 
	V_FRACT_F16
	V_FMA_F64 
	V_CVT_SR_BF16_F32 
	V_PK_FMAC_F16 
	V_FRACT_F32
	V_FMA_LEGACY_F16 
	V_CVT_SR_BF8_F32 
	V_SUBBREV_CO_U32 
	V_FRACT_F64
	V_LERP_U8 
	V_CVT_SR_F16_F32 
	V_SUBB_CO_U32 
	V_FREXP_EXP_I16_F16
	V_LSHL_ADD_U32 
	V_CVT_SR_FP8_F32 
	V_SUBREV_CO_U32 
	V_FREXP_EXP_I32_F32
	V_LSHL_ADD_U64 
	V_LDEXP_F32 
	V_SUBREV_F16 
	V_FREXP_EXP_I32_F64
	V_LSHL_OR_B32 
	V_LDEXP_F64 
	V_SUBREV_F32 
	V_FREXP_MANT_F16
	V_MAD_F16 
	V_LSHLREV_B64 
	V_SUBREV_U16 
	V_FREXP_MANT_F32
	V_MAD_I16 
	V_LSHRREV_B64 
	V_SUBREV_U32 
	V_FREXP_MANT_F64
	V_MAD_I32_I16 
	V_MAX_F64 
	V_SUB_CO_U32 
	V_LOG_F16
	V_MAD_I32_I24 
	V_MBCNT_HI_U32_B32 
	V_SUB_F16 
	V_LOG_F32
	V_MAD_I64_I32 
	V_MBCNT_LO_U32_B32 
	V_SUB_F32 
	V_MOV_B32
	V_MAD_LEGACY_F16 
	V_MIN_F64 
	V_SUB_U16 
	V_MOV_B64
	V_MAD_LEGACY_I16 
	V_MUL_F64 
	V_SUB_U32 
	V_NOP
	



6.3. Instructions 35 of 600
CDNA4 Instruction Set Architecture 
VOP3 
	VOP3 - 2 operands 
	VOP2 
	VOP1
	V_MAD_LEGACY_U16 
	V_MUL_HI_I32 
	V_XNOR_B32 
	V_NOT_B32
	V_MAD_U16 
	V_MUL_HI_U32 
	V_XOR_B32 
	V_PERMLANE16_SWAP_B32
	V_MAD_U32_U16 
	V_MUL_LEGACY_F32 
	

	V_PERMLANE32_SWAP_B32
	V_MAD_U32_U24 
	V_MUL_LO_U32 
	

	V_PRNG_B32
	V_MAD_U64_U32 
	V_PACK_B32_F16 
	

	V_RCP_F16
	V_MAX3_F16 
	V_READLANE_B32 
	

	V_RCP_F32
	V_MAX3_F32 
	V_SUB_I16 
	

	V_RCP_F64
	V_MAX3_I16 
	V_SUB_I32 
	

	V_RCP_IFLAG_F32
	V_MAX3_I32 
	V_TRIG_PREOP_F64 
	

	V_READFIRSTLANE_B32
	V_MAX3_U16 
	V_WRITELANE_B32 
	

	V_RNDNE_F16
	V_MAX3_U32 
	

	

	V_RNDNE_F32
	V_MAXIMUM3_F32 
	

	

	V_RNDNE_F64
	V_MED3_F16 
	

	

	V_RSQ_F16
	V_MED3_F32 
	

	

	V_RSQ_F32
	V_MED3_I16 
	

	

	V_RSQ_F64
	V_MED3_I32 
	

	

	V_SAT_PK_U8_I16
	V_MED3_U16 
	

	

	V_SIN_F16
	V_MED3_U32 
	

	

	V_SIN_F32
	V_MIN3_F16 
	

	

	V_SQRT_F16
	V_MIN3_F32 
	

	

	V_SQRT_F32
	V_MIN3_I16 
	

	

	V_SQRT_F64
	V_MIN3_I32 
	

	

	V_SWAP_B32
	V_MIN3_U16 
	

	

	V_TRUNC_F16
	V_MIN3_U32 
	

	

	V_TRUNC_F32
	V_MINIMUM3_F32 
	

	

	V_TRUNC_F64
	V_MQSAD_PK_U16_U8
	

	

	

	V_MQSAD_U32_U8
	

	

	

	V_MSAD_U8
	

	

	

	V_OR3_B32
	

	

	

	V_PERM_B32
	

	

	

	V_QSAD_PK_U16_U8
	

	

	

	V_SAD_HI_U8
	

	

	

	V_SAD_U16
	

	

	

	V_SAD_U32
	

	

	

	V_SAD_U8
	

	

	

	V_XAD_U32
	

	

	

	



The next table lists the compare instructions. 
Table 25. VALU Instruction Set 
Op 
V_CMP 
	Formats 
	Functions 
I16, I32, I64, U16, U32, U64 F, LT, EQ, LE, GT, LG, GE, T 
	Result 
Write VCC..
	V_CMPX 
	Write VCC and exec.
	V_CMP 
	F16, F32, F64 
	F, LT, EQ, LE, GT, LG, GE, T, 
O, U, NGE, NLG, NGT, NLE, NEQ, NLT 
(o = total order, u = unordered, 
N = NaN or normal compare)
	Write VCC.
	V_CMPX 
	Write VCC and exec.
	V_CMP_CLASS 
	F16, F32, F64 
	Test for one of: signaling-NaN, quiet-NaN, 
positive or negative: infinity, normal, subnormal, zero.
	Write VCC.
	V_CMPX_CLASS 
	Write VCC and exec.
	



6.3. Instructions 36 of 600
CDNA4 Instruction Set Architecture 6.4. Denormalized and Rounding Modes 
The shader program has explicit control over the rounding mode applied and the handling of denormalized inputs and results. The MODE register is set using the S_SETREG instruction; it has separate bits for controlling the behavior of single and double-precision floating-point numbers. 
Note: that V_DOT2 instructions operating on floating point data do not support denormal and rounding modes. They flush input and output denorms. 
Table 26. Round and Denormal Modes 
Field 
FP_ROUND 
	Bit Position 
3:0 
	Description 
[1:0] Single-precision round mode. 
[3:2] Double/Half-precision round mode. 
Round Modes: 0=nearest even; 1= +infinity; 2= -infinity, 3= toward zero.
	FP_DENORM 
	7:4 
	[5:4] Single-precision denormal mode. 
[7:6] Double/Half-precision denormal mode. 
Denormal modes: 
0 = Flush input and output denorms. 
1 = Allow input denorms, flush output denorms. 
2 = Flush input denorms, allow output denorms. 
3 = Allow input and output denorms.
	



6.5. ALU Clamp Bit Usage 
When using V_CMP instructions, setting the clamp bit to 1 indicates that the compare signals if a floating point exception occurs. For integer operations, it clamps the result to the largest and smallest representable value. For floating point operations, it clamps the result to the range: [0.0, 1.0]. 
6.6. VGPR Indexing 
VGPR Indexing allows a value stored in the M0 register to act as an index into the VGPRs either for the source or destination registers in VALU instructions. 
6.6.1. Indexing Instructions 
The table below describes the instructions which enable, disable and control VGPR indexing. 
Table 27. VGPR Indexing Instructions 
Instruction 
S_SET_GPR_IDX_OFF 
	Encoding 
SOPP 
	Sets SCC? N 
	Operation 
Disable VGPR indexing mode. Sets: mode.gpr_idx_en = 0.
	S_SET_GPR_IDX_ON 
	SOPC 
	N 
	Enable VGPR indexing, and set the index value and mode from an SGPR. mode.gpr_idx_en = 1 
M0[7:0] = S0.u[7:0] 
M0[15:12] = SIMM4
	S_SET_GPR_IDX_IDX 
	SOP1 
	N 
	Set the VGPR index value: 
M0[7:0] = S0.u[7:0]
	



6.4. Denormalized and Rounding Modes 37 of 600
CDNA4 Instruction Set Architecture 
Instruction 
	Encoding 
	Sets SCC? 
	Operation
	S_SET_GPR_IDX_MODE 
	SOPP 
	N 
	Change the VGPR indexing mode, which is stored in M0[15:12]. 
M0[15:12] = SIMM4
	



Indexing is enabled and disabled by a bit in the MODE register: gpr_idx_en. When enabled, two fields from M0 are used to determine the index value and what it applies to: 
• M0[7:0] holds the unsigned index value, added to selected source or destination VGPR addresses. • M0[15:12] holds a four-bit mask indicating to which source or destination the index is applied. ◦ M0[15] = dest_enable. 
◦ M0[14] = src2_enable. 
◦ M0[13] = src1_enable. 
◦ M0[12] = src0_enable. 
Indexing only works on VGPR source and destinations, not on inline constants or SGPRs. It is illegal for the index attempt to address VGPRs that are out of range. 
6.6.2. VGPR Indexing Details 
This section describes how VGPR indexing is applied to instructions that use source and destination registers in unusual ways. The table below shows which M0 bits control indexing of the sources and destination registers for these specific instructions. 
Instruction 
v_readlane 
	Microcode Encodes sdst = src0, SS1 
	VALU Receives 
	M0[15] 
(dst) 
x 
	M0[15] 
(s2) 
x 
	M0[15] 
(s1) 
x 
	M0[12] 
(s0) 
src0
	v_readfirstlane 
	sdst = func(src0) 
	

	x 
	x 
	x 
	src0
	v_writelane 
	dst = func(ss0, ss1) 
	

	dst 
	x 
	x 
	x
	v_mac_* 
	dst = src0 * src1 + dst 
	mad: dst, src0, src1, src2 
	dst, s2 
	x 
	src1 
	src0
	v_madak 
	dst = src0 * src1 + imm 
	mad: dst, src0, src1, src2 
	dst 
	x 
	src1 
	src0
	v_madmk 
	dst = S0 * imm + src1 
	mad: dst, src0, src1, src2 
	dst 
	src2 
	x 
	src0
	v_*sh*_rev 
	dst = S1 << S0 
	<shift> (src1, src0) 
	dst 
	x 
	src1 
	src0
	v_cvt_pkaccum 
	uses dst as src2 
	

	dst, s2 
	x 
	src1 
	src0
	SDWA (dest preserve, sub-Dword mask)
	uses dst as src2 for read mod-write
	

	

	dst, s2
	

	

	



where: 
src= vector source 
SS = scalar source 
dst = vector destination 
sdst = scalar destination 
6.7. Packed Math 
CDNA supports packed math, which performs operations on two 16-bit values within a Dword as if they were separate elements. For example, a packed add of V0=V1+V2 is really two separate adds: adding the low 16 bits of each Dword and storing the result in the low 16 bits of V0, and adding the high halves. 
6.7. Packed Math 38 of 600
CDNA4 Instruction Set Architecture 
Packed math uses the instructions below and the microcode format "VOP3P". This format adds op_sel and neg fields for both the low and high operands, and removes ABS and OMOD. 
Packed Math Opcodes: 
V_PK_MAD_I16 
	V_PK_MUL_LO_U16 
	V_PK_ADD_I16 
	V_PK_SUB_I16
	V_PK_LSHLREV_B16 
	V_PK_LSHRREV_B16 
	V_PK_ASHRREV_I16 
	V_PK_MAX_I16
	V_PK_MIN_I16 
	V_PK_MAD_U16 
	V_PK_ADD_U16 
	V_PK_SUB_U16
	V_PK_MAX_U16 
	V_PK_MIN_U16 
	V_PK_FMA_F16 
	V_PK_ADD_F16
	V_PK_MUL_F16 
	V_PK_MIN_F16 
	V_PK_MAX_F16
	

	V_MAD_MIX_F32 
	V_MAD_MIXLO_F16 
	V_MAD_MIXHI_F16
	

	V_PK_FMA_F32 
	V_PK_MUL_F32 
	V_PK_ADD_F32 
	V_PK_MOV_B32
	



V_MAD_MIX_* are not packed math, but perform a single Multiply-Add operation on a 
mixture of 16- and 32-bit inputs. The Multiply-add is performed as an FMA - fused multiply add. They are listed here because they use the VOP3P encoding. 
Packed 32-bit instructions operate on 2 dwords at a time and those operands must be two 
 
dword aligned (i.e. an even VGPR address). Output modifiers are not supported for these instructions. OPSEL and OPSEL_HI work to select the first or second DWORD for each source. 
6.7.1. Packed Convert 
All convert opcodes operating on FP6/BF6/FP4 data must use VGPR sources for any operand slots providing more than 32-bits of data. 
4-bit 
	6-bit 
	8-bit
	CVT_SCALE_PK_FP4_F32 
CVT_SCALE_SR_PK_FP4_F32 
CVT_SCALE_PK_F32_FP4
	CVT_SCALE_PK_FP6_F32 
CVT_SCALE_PK_BF6_F32 
CVT_SCALE_SR_PK_FP6_F32 
CVT_SCALE_SR_PK_BF6_F32 
CVT_SCALE_PK_F32_FP6 
CVT_SCALE_PK_F32_BF6
	CVT_SCALE_PK_FP8_F32 
CVT_SCALE_PK_BF8_F32 
CVT_SCALE_SR_FP8_F32 
CVT_SCALE_SR_BF8_F32 
CVT_SCALE_PK_F32_FP8 
CVT_SCALE_PK_F32_BF8 
CVT_SCALE_F32_FP8 
CVT_SCALE_F32_BF8
	CVT_SCALE_PK_FP4_F16 
CVT_SCALE_PK_FP4_BF16 
CVT_SCALE_SR_PK_FP4_F16 
CVT_SCALE_SR_PK_FP4_BF16 
CVT_SCALE_PK_F16_FP4 
CVT_SCALE_PK_BF16_FP4
	CVT_SCALE_PK_FP6_F16 
CVT_SCALE_PK_FP6_FB16 
CVT_SCALE_PK_BF6_F16 
CVT_SCALE_PK_BF6_BF16 
CVT_SCALE_SR_PK_FP6_F16 
CVT_SCALE_SR_PK_FP6_BF16 
CVT_SCALE_SR_PK_BF6_F16 
CVT_SCALE_SR_PK_BF6_BF16 
CVT_SCALE_PK_F16_FP6 
CVT_SCALE_PK_F16_BF6 
CVT_SCALE_PK_BF16_FP6 
CVT_SCALE_PK_BF16_BF6
	CVT_SCALE_PK_FP8_F16 
CVT_SCALE_PK_BF8_F16 
CVT_SCALE_PK_FP8_BF16 
CVT_SCALE_PK_BF8_BF16 
CVT_SCALE_SR_FP8_F16 
CVT_SCALE_SR_BF8_F16 
CVT_SCALE_SR_FP8_BF16 
CVT_SCALE_SR_BF8_BF16 
CVT_SCALE_PK_F16_FP8 
CVT_SCALE_PK_F16_BF8 
CVT_SCALE_F16_FP8 
CVT_SCALE_F16_BF8
	16-bit 
	

	Integer 8-bit
	



6.7. Packed Math 39 of 600
CDNA4 Instruction Set Architecture 
CVT_PK_F16_F32 
CVT_PK_BF16_F32 
CVT_F32_BF16
	

	ASHR_PK_I8_I32 
ASHR_PK_U8_I32
	



Convert instructions with SCALE add an 8-bit exponent bias (E8M0, bias of 127) to each F4/F6/F8 value. Each exponent bias is shared by a block of 32 values along the K dimension. 
For example, conversion from FP32 to FP6 (16x16x128): 
• Source data is in VGPRs 0..31, with K=0..31 for M=0 in lane0, M=1 in lane1 etc up to M=15 in lane 15; then K=32..63 in lanes 16..31; K=64..96 in lanes 32..48; and K=96..127 in lanes 48..63. 
• Result data is in VGPRs 0..5, with K and M distributed similarly (lanes0..15 has K=0..31 and M=0..15). • Exponent biases: the VGPR holds one set of exponent biases in bits [30:23] (typical float32 exponent position). 
6.7. Packed Math 40 of 600
CDNA4 Instruction Set Architecture Chapter 7. Matrix Arithmetic Instructions 
Matrix core is an extension to CDNA architecture shader instruction set supporting the Machine Intelligence SIMD. The matrix core has its own VGPR file: the Accumulation ("Acc") GPRs. This is separate from the normal (Architectural, or "Arch") VGPRs in the original SIMD. Shader I/O can only use both types of VGPRs. Instructions have an ACC bit to indicate if data is transferred to/from architectural or accumulation VGPRs. Data can be moved between the ACC and ARCH VGPRs via the V_ACCVGPR_READ and V_ACCVGPR_WRITE instructions. 
The core operation implemented inside the matrix core is the 4 × 1 times 1 × 4 outer matrix product, yielding 16 output values. The outer product can be performed both on dense inputs and on 2,4 sparse ones (where two of each set of four values is zero). The matrix core unit uses combinations of these operations, both in parallel and in series, to implement the dense matrix-fused-multiply-add (MFMA) instructions described in Subsection Matrix fused-multiply-add (MFMA) and their 2,4-sparse variants described in Subsection Sparse Matrices. 
Because these matrix instructions do not produce their output in a single cycle, and since their partially written results may be observable, a certain amount of independent instructions must sometimes be present between the issuance of a matrix core instruction and accesses to its results or modification of the registers that hold its inputs, as described in Subsection Dependency Resolution: Required Independent Instructions. 
Additional information can be found on the GPUOpen blog: https://gpuopen.com/learn/amd-lab-notes/ amd-lab-notes-matrix-cores-README/ 
This blog post relates to CDNA2 but may be helpful with understanding CDNA4. 
The AMD Matrix Instruction Calculator (https://github.com/RadeonOpenCompute/ 
amd_matrix_instruction_calculator) contains a helper tool that allows developers to view detailed information about the MFMA instructions in the CDNA architecture. It allows users to query instruction level information such as computational throughput and register usage. It also allows users to generate mappings between matrix element and hardware registers for each MFMA instruction and their modifiers. 
7.1. Matrix fused-multiply-add (MFMA) 
The matrix fused-multiply-add (MFMA) instructions use the matrix core to perform one or more matrix multiplications. Note that the matrix core unit, which executes these instructions, has the 4 × 1 by 1 × 4 outer product as its fundamental computational primitive, and so the MFMA instructions implement outer-product like operations. 
These instructions all have names of the form V_MFMA_[output type]_[M]X[N]X[K][_[B]B]_[input type] where B (which is 1 if not specified) is the number of matrices (or blocks) that are multiplied, and M, N, and K, are the multiplication dimensions for each block. For example, the instruction V_MFMA_F32_32x32x1_2B_F32 perform the operations 
D[0,:,:] = C[0,:,:] + A[0,:,:] * B[0,:,:] 
D[1,:,:] = C[1,:,:] + A[1,:,:] * B[1,:,:] 
where the D[b,:,:] and C[b,:,:] are 32 × 32 matrices of 32-bit floats, the A[b,:,:] are 32 × 1 matrices of floats, 7.1. Matrix fused-multiply-add (MFMA) 41 of 600
CDNA4 Instruction Set Architecture 
and the B[b,:,:] are 1 × 32 matrices of floats. 
The input and output values for an MFMA can be stored either in the standard architectural vector registers (VGPRs) or accumulation VGPRs (AccVGPRs), which are additional registers exclusive to the matrix core unit. The register file that the registers holding matrices A and B are controlled by the low and high bits, respectively, of the ACC field of a MFMA instruction (0 for VGPRs, 1 for AccVGPRs), while the ACC_CD bit determines if the C and D matrices are stored in VGPRs (0) or AccVGPRs (1). Data can be moved to and from AGPRs using the V_ACCVGPR_* instructions. 
Note that the registers holding input or output data for a MFMA instruction must be contiguous, and that the first register must be aligned to the number of registers required as the input or output. For instance, if an instruction requires four input registers for matrix A, registers 4 through 7 may be used (by setting SRC0 to 4) but not registers 5 through 8. 
7.1.1. Notation 
When indexing values that have multiple blocks, M[b,i,j] is the value in block b, row i, and column j of the value M, where matrices are zero-indexed. 
When describing the inputs and outputs of an MFMA operation, they are written as matrices with each column representing a different lane in a wavefront and each row representing a different register (or logical item) that a lane has. 
For example, consider the following pair of matrices A 
A[0,0,0] A[0,0,1] 
A[0,1,0] A[0,1,1] 
… … 
A[0,31,0] A[0,31,1] 
---- ---- 
A[1,0,0] A[0,0,1] 
… … 
A[1,31,0] A[1,31,1]
	



When the value is written as: 
Lane 0 Lane 1 … Lane 31 Lane 32 … Lane 63 
Register 0 A[0,0,0] A[0,1,0] … A[0,31,0] A[1,0,0] … A[1,31,0] Register 1 A[0,0,1] A[0,1,1] … A[0,31,1] A[1,0,1] … A[1,31,1]
	



this means that each lane of a wavefront holds two values across two contiguous registers, which are the two values of a row of one of the blocks of A, with the first 32 lanes holding a different row from block 0 and the second 32 lanes holding successive rows of block 1. 
This specification writes matrices in their storage layout. 
When showing register layouts, this spec assumes the first register is 0. 
Unless otherwise specified, the division operator rounds down (takes the floor). 
7.1. Matrix fused-multiply-add (MFMA) 42 of 600
CDNA4 Instruction Set Architecture 7.1.2. List of Dense MFMA instructions 
Table 28. MFMA VALU Opcodes: 
Instruction 
V_MFMA_F32_{*}_F32 
	Variants 
32x32x1_2B 
	Blocks 
2 
	Cycles 
64 
	Description 
Matrix multiply, using FMA with F32 A & B matrices.
	16x16x1_4B 
	4 
	32 
	4x4x1_16B 
	16 
	8
	32x32x2 
	1 
	64
	16x16x4 
	1 
	32
	V_MFMA_F32_{*}_F16 
	32x32x4_2B 
	2 
	64 
	Matrix multiply, using FMA with F16 A & B matrices.
	16x16x4_4B 
	4 
	32 
	4x4x4_16B 
	16 
	8
	32x32x8 
	1 
	32
	16x16x16 
	1 
	16
	V_MFMA_F32_{*}_BF16 
	32x32x4_2B 
	2 
	64 
	Matrix multiply, using FMA with BF16 A & B matrices.
	16x16x4_4B 
	4 
	32 
	4x4x4_16B 
	16 
	8
	32x32x8 
	1 
	32
	16x16x16 
	1 
	16
	V_MFMA_I32_{*}_I8 
	32x32x4_2B 
	2 
	64 
	Matrix multiply, using FMA with I8 A & B matrices
	16x16x4_4B 
	4 
	32
	4x4x4_16B 
	16 
	8
	32x32x16 
	1 
	32
	16x16x32 
	1 
	16
	V_MFMA_F64_{*}_F64 
	16x16x4 
	1 
	64 
	Matrix Multiply on F64 data.
	4x4x4_4B 
	4 
	32
	V_MFMA_F32_{*}_BF8_BF8 V_MFMA_F32_{*}_BF8_FP8 V_MFMA_F32_{*}_FP8_BF8 V_MFMA_F32_{*}_FP8_FP8
	16x16x32 
	1 
	16 
	Matrix Multiply on FP8 or BF8 data.
	32x32x16 
	1 
	32
	V_MFMA_F32_{*}_BF16 
V_MFMA_F32_{*}_F16
	16x16x32 
	1 
	16 
	Matrix Multiply on FP16 or BF16 data
	32x32x16 
	1 
	32
	V_MFMA_I32_16X16X64_I8 
	16x16x64 
	1 
	16 
	Matrix Multiply on Int8 data
	V_MFMA_I32_32X32X32_I8 
	32x32x32 
	1 
	32 
	Matrix Multiply on Int8 data
	V_MFMA_F32_16x16x128_F8F6F4 
	16x16x128 
	1 
	16 or 32 
	Matrix Multiply using FP4, FP6 or FP8 
independently for Matrix-A and Matrix-B. Larger cycle count if either matrix A or B is F8. 
	V_MFMA_F32_32x32x64_F8F6F4 
	32x32x64 
	1 
	32 or 64
	V_MFMA_SCALE_F32_16X16X128_ F8F6F4
	16x16x128 
	1 
	16 or 32 
	Matrix Multiply using FP4, FP6 or FP8 
independently for Matrix-A and Matrix-B. Larger cycle count if either matrix A or B is F8.
	V_MFMA_SCALE_F32_32X32X64_F 8F6F4
	32x32x64 
	1 
	32 or 64
	



Rules for the MFMA instructions listed above, except F8F6F4: 
Control 
Denorm Control 
	Behavior 
Ignores Denorm Control from MODE and keep Input/Output Denorms.
	



7.1. Matrix fused-multiply-add (MFMA) 43 of 600
CDNA4 Instruction Set Architecture 
Control 
	Behavior
	Clamp 
	Supported. uses the FP16_OVFL bit from MODE 
If set, F32 Result on overflow is clamped to +/- MAX, otherwise the overflow result is normalized to +/-INF. 
If set, I32 Result is clamped to +/-MAX on overflow/underflow, otherwise the carry out bits are dropped.
	Round Mode 
	ignores Round Mode from MODE and forces it to RNE.
	Exceptions 
	Not Supported
	Execution Mask 
	ignores exec mask from MODE and forces it to 1 for all threads
	Sources 
	Src0/1/2/VDST if VGPR need to be even aligned. 
Src0/1 can be only VGPR, SRC2 can be inline/constant
	Scale 
	No support for FP16, BF16,I8 MFMA Opcodes
	



7.1.3. Usage examples 
7.1.3.1. V_MFMA_F32_32X32X1_2B_F32 
The first examples show MFMA usage in order to build an intuition for the general semantics of these instructions. 
Suppose the user wants do two matrix multiplications of 32 × 1 matrices A[b,:,:] by 1 × 32 matrices B[b,:,:], accumulating the results into 32 × 32 matrices D[b,:,:]. 
The input register for A stores columns of A across successive lanes (that is, the i coordinate is the fastest moving) and has the form 
Lane 0 Lane 1 … Lane 31 Lane 32 … Lane 63 
Register 0 A[0,0,0] A[0,1,0] … A[0,31,0] A[1,0,0] … A[1,31,0]
	



that is, lane l holds the value 
A[l / 32, l % 32, 0] 
The layout for B holds rows of B in the same way that the A layout stores its columns. That is, B is stored with lane l holding 
B[l / 32, 0, l % 32] 
Lane 0 Lane 1 … Lane 31 Lane 32 … Lane 63 
Register 0 B[0,0,0] B[0,0,1] … B[0,0,31] B[1,0,0] … B[1,0,31]
	



The core component of the output layout is the 4 × N (where N is 32 here) tile of values. (The use of 4 × N tiles, as opposed to a simpler layout, is a consequence of the matrix core’s internal structure). As many of these tiles as possible (here 2 of them) are packed into the lanes of each group of registers, going by row and then by 
7.1. Matrix fused-multiply-add (MFMA) 44 of 600
CDNA4 Instruction Set Architecture 
block. 
That is, the layout of D (and the corresponding layout of C) is: 
Lane 0 Lane 1 … Lane 31 Lane 32 … Lane 63 
Register 0 D[0,0,0] D[0,0,1] … D[0,0,31] D[0,4,0] … D[0,4,31] Register 1 D[0,1,0] D[0,1,1] … D[0,1,31] D[0,5,0] … D[0,5,31] … … … … … … … … Register 3 D[0,3,0] D[0,3,1] … D[0,3,31] D[0,7,0] … D[0,7,31] Register 4 D[0,8,0] D[0,8,1] … D[0,8,31] D[0,12,0] … D[0,12,31] … … … … … … … … Register 15 D[0,27,0] D[0,27,1] … D[0,27,31] D[0,31,0] … D[0,31,31] Register 16 D[1,0,0] D[1,0,1] … D[1,0,31] D[1,4,0] … D[1,4,31] … … … … … … … … Register 31 D[1,27,0] D[1,27,1] … D[1,27,31] D[1,31,0] … D[1,31,31]
	



In other words, the output value D[b, i, j] is located in lane 
l = j + 32 * ((i/4) % 2) 
of output register 
r = 16b + 4(i / 8) + (i % 4) 
In order to produce these results, the broadcast fields (CBSZ, ABID, and BLGP) must all be set to 0. The usage of these fields is shown in Subsection Broadcasting values. 
7.1.3.2. V_MFMA_F32_32X32X2_F32 
As another example, consider the instruction V_MFMA_F32_32X32X2_F32. For this instruction, there is only one block being multiplied, and the matrices A[0,:,:] and B[0,:,:] are 32 × 2 and 2 × 32 respectively. 
This instruction takes one input register for each of A and B, which the same format as above, except that lanes 32-63 contain the second column of A (row of B) instead of the second block. The output layout is the same as above, except that there is only one block and so there are only 16 output registers. More concretely, the input and output layouts for V_MFMA_F32_32X32X2_F32 are 
Lane 0 Lane 1 … Lane 31 Lane 32 … Lane 63 
Register 0 A[0,0,0] A[0,1,0] … A[0,31,0] A[0,0,1] … A[0,31,1]
	



Lane 0 Lane 1 … Lane 31 Lane 32 … Lane 63 
Register 0 B[0,0,0] B[0,0,1] … B[0,0,31] B[0,1,0] … B[0,1,31]
	



Lane 0 Lane 1 … Lane 31 Lane 32 … Lane 63 
Register 0 D[0,0,0] D[0,0,1] … D[0,0,31] D[0,4,0] … D[0,4,31] Register 1 D[0,1,0] D[0,1,1] … D[0,1,31] D[0,5,0] … D[0,5,31] … … … … … … … … 
7.1. Matrix fused-multiply-add (MFMA) 45 of 600
CDNA4 Instruction Set Architecture 
Lane 0 Lane 1 … Lane 31 Lane 32 … Lane 63 
Register 3 D[0,3,0] D[0,3,1] … D[0,3,31] D[0,7,0] … D[0,7,31] Register 4 D[0,8,0] D[0,8,1] … D[0,8,31] D[0,12,0] … D[0,12,31] … … … … … … … … Register 15 D[0,27,0] D[0,27,1] … D[0,27,31] D[0,31,0] … D[0,31,31]
	



7.1.3.3. V_MFMA_F32_4X4X4_16B_F16 
This example demonstrates how values that are not 32 bits long are packed into registers and how the output format changes in the case where an entire matrix cannot fill all lanes in an output register group. 
The V_MFMA_F32_4X4X4_16B_F16 instruction performs 16 block multiplications of the form D[b,:,:] = C[b,:,:] + A[b,:,:] * B[b,:,:] 
where each block of A and B is a 4 × 4 block of half-precision floating point values and each block of C and D holds 4 × 4 single-precision floats. 
The instruction uses 2 registers to hold each of A and B, even though, following the input format principles from the previous section, each lane needs to hold four values. This is because each input register holds two half-precision values, with the second of those values in the upper bits (16—31) of the register. 
That is, the input layout of A is 
Lane 0 Lane 1 … Lane 3 Lane 4 … Lane 63 
Register 0[15:0] A[0,0,0] A[0,1,0] … A[0,3,0] A[1,0,0] … A[15,3,0] Register 0[31:16] A[0,0,1] A[0,1,1] … A[0,3,1] A[1,0,1] … A[15,3,1] Register 1[15:0] A[0,0,2] A[0,1,2] … A[0,3,2] A[1,0,2] … A[15,3,2] Register 1[31:16] A[0,0,3] A[0,1,3] … A[0,3,3] A[1,0,3] … A[15,3,3]
	



and for B is 
Lane 0 Lane 1 … Lane 3 Lane 4 … Lane 63 
Register 0[15:0] B[0,0,0] B[0,0,1] … B[0,0,3] B[1,0,0] … B[15,0,3] Register 0[31:16] B[0,1,0] B[0,1,1] … B[0,1,3] B[1,1,0] … B[15,1,3] Register 1[15:0] B[0,2,0] B[0,2,1] … B[0,2,3] B[1,2,0] … B[15,2,3] Register 1[31:16] B[0,3,0] B[0,3,1] … B[0,3,3] B[1,3,0] … B[15,3,3]
	



The 16 4 × 4 output blocks of this instruction are arranged into four output registers as follows. 
Lane 0 Lane 1 … Lane 3 Lane 4 … Lane 63 
Register 0 D[0,0,0] D[0,0,1] … D[0,0,3] D[1,0,0] … D[15,0,3] … … … … … … … … Register 3 D[0,3,0] D[0,3,1] … D[0,3,3] D[1,3,0] … D[15,3,3]
	



That is, because there are not enough groups of 4 rows available in a block to fill 64 lanes of output in each register, successive blocks are used instead. Note that these outputs are 32-bit floats and so are not packed into registers. 
7.1. Matrix fused-multiply-add (MFMA) 46 of 600
CDNA4 Instruction Set Architecture 7.1.3.4. V_MFMA_F64_16X16X4_F64 
This demonstrates how double-precision values are handled using the example of V_MFMA_F64_16X16X4_F64. This instruction follows the same input layout patterns as the previous examples and operates most similarly to V_MFMA_F32_32X32X2_F32. However, each input is spread across multiple registers in order to accommodate the full 64-bit value. 
The output of this instruction, and the other double-precision MFMA instructions, does not follow the 4 × N block layout of other MFMA instructions. Instead, the output rows are packed contiguously across the lanes of each wavefront, and then packed into pairs (to account for the 64 bits needed to store the output) of registers, as shown below. 
The input and output formats for V_MFMA_F64_16X16X4_F64 are 
Lane 0 Lane 1 … Lane 15 Lane 16 … Lane 63 Reg. 0 A[0,0,0][31:0] A[0,1,0][31:0] … A[0,15,0][31:0] A[0,0,1][31:0] … A[0,15,3][31:0] Reg. 1 A[0,0,0][63:32] A[0,1,0][63:32] … A[0,15,0][63:0] A[0,0,1][63:32] … A[0,15,3][63:32]
	



Lane 0 Lane 1 … Lane 15 Lane 16 … Lane 63 Reg. 0 B[0,0,0][31:0] B[0,0,1][31:0] … B[0,0,15][31:0] B[0,1,0][31:0] … B[0,3,15][31:0] Reg. 1 B[0,0,0][63:32] B[0,0,1][63:32] … B[0,0,15][63:32] B[0,1,0][63:32] … B[0,3,15][63:32]
	



Lane 0 Lane 1 … Lane 15 Lane 16 … Lane 63 Reg. 0 D[0,0,0][31:0] D[0,0,1][31:0] … D[0,0,15][31:0] D[0,1,0][31:0] … D[0,3,15][31:0] Reg. 1 D[0,0,0][63:32] D[0,0,1][63:32] … D[0,0,15][63:32] D[0,1,0][63:32] … D[0,3,15][63:32] Reg. 2 D[0,4,0][31:0] D[0,4,1][31:0] … D[0,4,15][31:0] D[0,5,0][31:0] … D[0,7,15][31:0] … … … … … … … … Reg. 7 D[0,12,0][63:32] D[0,12,1][63:32] … D[0,12,15][63:32] D[0,13,0][63:32] … D[0,15,15][63:32]
	



7.1.4. General input and output layout 
In general, an MFMA instruction is parameterized by its input and output datatypes, the sizes M, N, and K of each matrix block and the number of blocks it operates on B. 
Semantically, for each 0 <= b < B, 0 <= i < M, and 0 <= j < N, it computes 
D[b,i,j] = C[b,i,j] + sum_{0 <= k < K} A[b,i,k] * B[b,k,j] 
where each A[b,:,:] is M × K, each B[b,:,:] is K × N, and each D[b,:,:] and corresponding C[b,:,:] is M × N. 
The values of the inputs and outputs are placed into the arguments to the instruction according to a fixed layout. For simplicity, this layout is defined in terms of the lanes of a wavefront and of the sequence of items for each lane: these items are arranged into the 32-bit registers that are the true arguments to an MFMA instruction in little-endian form. 
More specifically, 
• For 64-bit quantities, each item corresponds to a pair of registers, with the low bits of the quantity in the first of those registers and high bits in the second one 
7.1. Matrix fused-multiply-add (MFMA) 47 of 600
CDNA4 Instruction Set Architecture 
• For 32-bit quantities, each item corresponds to a distinct register 
• For 16-bit quantities, an item is half of a register, with the odd-numbered items taking up bits 31-16 and the even ones in bits 0-15 
• For 8-bit quantities, four items are packed into a register, analogously to the 16-bit case • For 4-bit quantities, eight items are packed into a register analogously to the 16- and 8-bit cases. That is, item 0 lives in bits 3-0 of the first register, item 1 is in bits 7-4 of the same register, and so on, until item 8 is placed in bits 3-0 of the following register.” 
• 6-bit quantities, like fp6 and bf6 values, are also densely packed into the registers that contain them. Because 6-bit quantities cannot be evenly packed into one 32-bit register, all instructions that take 6-bit inputs from lanes require them to be placed into six contiguous registers, and thus will require each lane to provide 32 values across those registers. If that group of 6 registers is treated as one 192-bit register, we can then describe item 0 as residing in bits 5-0 of that register, item 1 as being stored in bits 11-6, item 4 in bits 35-30 (note the crossing of the 32-bit register boundary) and so on.” 
7.1.4.1. Input layout 
To define the input layout for the matrix A, first define the auxiliary constant 
K_L = K / (64 / (M * B)) 
which is the number of consecutive values of K that each lane holds in its registers. 
For example, for the instruction V_MFMA_F32_32X32X1_2B_F32: 
K_L = 1 / (64 / (32 * 2)) = 1 / 1 = 1 
and for V_MFMA_F32_32X32X2_F32: 
K_L = 2 / (64 / (32 * 1)) = 2 / 2 = 1 
These both show that the one input register holds one value in the K dimension, but for V_MFMA_F32_4X4X4_16B_F16: 
K_L = 4 / (64 / (4 * 16)) = 4 / 1 = 4 
representing the fact that, for each lane, the two input registers hold four values in the K dimension. These input values are packed little-endian. For example, the third value in each row (which has k = 2 zero-indexed), is in bits 15:0 of the second input register for both A and B across all lanes. That 16-bit region is "item 2" in the layout computed below for V_MFMA_F32_4X4X4_16B_F16. 
Note that, in all MFMA instructions, the products M * B and N * B are less than 64, that is, the values of a single column of A or row of B, considered over all blocks, fit within a single input item. 
With this layout defined, a given input value A[b,i,k] is placed in the item 
7.1. Matrix fused-multiply-add (MFMA) 48 of 600
CDNA4 Instruction Set Architecture 
k % K_L 
of lane 
i + M * (b + B * (k / K_L)) 
The layout for B is the analogous function that places B[b,k,j] in item 
k % K_L 
of lane 
j + N * (b + B * (k / K_L)) 
7.1.4.2. Output layout 
The output values D[b,i,j] of an MFMA instruction, as well as the corresponding values C[b,i,j] of the matrix to add to the result, are stored in a fixed layout that is a function of the MFMA instruction being used. 
To define this layout, first define the following constants: 
• H, the group height, which indicates how many consecutive rows of output are placed in each row group and which are therefore stored in consecutive items on a single lane. For f64 instructions, H = 1, but for all other MFMA instructions, H = 4. 
• B_I = ceil(64 / (N * M / H)), the number of blocks stored in each output item (an item within the storage for D or C) across all lanes 
• M_I = (64 / B_I) / N, the number of rows of D stored in each output item across all lanes • G = M / (H * M_I), the number of row groups needed to store B_I blocks of output. 
For example, using the instruction V_MFMA_F32_32X32X1_2B_F32 gives 
H = 4 
B_I = ceil(64 / (32 * 32 / H)) = ceil(64 / 256) = 1 
M_I = (64 / 1) / 32 = 2 
G = 32 / (4 * 2) = 4 
while the instruction V_MFMA_F32_4X4X4_16B_F16 yields the values 
H = 4 
B_I = ceil(64 / (H * 4 / 4)) = ceil(64 / 4) = 16 
M_I = (64 / 16) / 4 = 4 / 4 = 1 
G = 4 / (H * 1) = 4 / 4 = 1 
7.1. Matrix fused-multiply-add (MFMA) 49 of 600
CDNA4 Instruction Set Architecture With these constants defined, the value D[b,i,j] of matrix D is located in item 
(i % H) + H * (i/(H * M_I) + G * (b / B_I)) 
on lane 
j + N * ((i / H) % M_I + M_I * (b % B_I)) 
7.1.5. 8-bit and Smaller Matrix Operations and Layouts There are two MFMA instructions which can independently select FP4, FP6 or FP6 for the A and B matrices: 
V_MFMA_F32_16x16x128_F8F6F4 
	A & B Matrix 
16x128 
F8: 8 VGPRs 
F6: 6 VGPRs 
F4: 4 VGPRs
	C & D Matrices 16x16 F32 
4 VGPRs
	Notes 
If either matrix = F8 → 32 cycles 
Else → 16 cycles
	V_MFMA_F32_32x32x64_F8F6F4 
	32x64 
F8: 8 VGPRs 
F6: 6 VGPRs 
F4: 4 VGPRs
	32x32 F32 
16 VGPRs
	If either matrix = F8 → 64 cycles 
Else → 32 cycles
	



Rules for the F8F6F4 MFMA instructions: 
Control 
Matrix Format 
	Behavior 
CBSZ[2:0] defines the matrix A format, BLGP[2:0] defines the matrix B format. Matrix op supports mixed types (i.e., any combination of the formats defined). 
BLGP[2:0] / 
CBSZ[2:0] 
3’b000 E4M3 (FP8) 
3’b001 E5M2 (BF8) 
3’b010 E2M3 (FP6) 
3’b011 E3M2 (BF6) 
3’b100 E2M1 (FP4)
	Denorm Control 
	Ignores Denorm Control from MODE and keep Input/Output Denorms.
	Clamp 
	Supported and uses the FP16_OVFL bit. 
If set, F32 Result on overflow is clamped to +/- MAX, otherwise the overflow result is normalized to +/-INF. 
If set, I32 Result is clamped to +/-MAX on overflow/underflow, otherwise the carry out bits are dropped.
	Round Mode 
	ignores Round Mode from MODE and forces it to RNE.
	Imod/Omod 
	Not Supported
	Exceptions 
	Not Supported
	Execution Mask 
	ignores exec mask from MODE and forces it to 1 for all threads
	Operand 
Alignment/Sources
	Src0/1/2/VDST if VGPR need to be even aligned. 
Src0/1 can be only VGPR/ACC_VGPR. 
SRC2 can be VGPR/ACC_VGPR/Constant
	



7.1. Matrix fused-multiply-add (MFMA) 50 of 600
CDNA4 Instruction Set Architecture 
Control 
	Behavior
	Scale 
	Format is E8M0. 
ABID[0] = 1’b1 : Must be set for V_MFMA_SCALE_F32_16X16X128_F8F6F4 and V_MFMA_SCALE_F32_32X32X64_F8F6F4 instructions. 
ABID[0] = 1’b0 : forces all scales into the ALU as 1.0f (exponent = 0x7f Biased – MFMA Runs without scale source). 
Hardware adjusts this scale value in its calculation: d_exp = (a0_exp+b0_exp) + (a1_exp+ b1_exp) + … + c_exp + scale_a + scale_b.
	



7.1.5.1. Dense Matrix Layouts: 8-bit and Smaller 
Matrix A[M][k] and B[k][N] Layouts are shown below. 
For 32x32x64 FP4: 
 K_L = 64 / (64 / 32) = 32 
 A[I, k] goes in “item” k % 32 of lane I + 32 * (k / 32) 
FP4: 
16x16x128 
A[16][128] 
B[128][16] 
v0
	row0 
thr 0-15 
M/N = [0-15] 
k=0-31 
	row1 
thr 16-31 
M/N = [0-15] 
k=32-63 
	row2 thr 32-47 
M/N = [0-15] 
k = 64 - 95 
	row3 
thr48-63 
M/N = [0-15] 
K = 96 - 127
	v1
	v2
	v3
	



32x32x64 
A[32][64] 
B[64][32] 
v0
	row0 
thr 0-15 
M/N = [0-15] 
k=0-31 
	row1 
thr 16-31 
M/N = [16-31] 
k=0-31 
	row2 
thr 32-47 
M/N = [0-15] 
k =32-63 
	row3 
thr48-63 
M/N = [16-31] 
K = 32-63
	v1
	v2
	v3
	



FP6: 
16x16x128 
A[16][128] 
B[128][16] 
v0
	row0 
thr 0-15 
M/N = [0-15] 
k=0-15 
	row1 
thr 16-31 
M/N = [0-15] 
k = 32-47 
	row2 
thr 32-47 
M/N = [0-15] 
k=64-79 
	row3 
thr48-63 
M/N = [0-15] 
k = 96-111
	v1 
	v2
	v3
	k = 16-31 
	k = 48 - 63 
	k = 80-95 
	k = 112-127
	v4 
	v5
	



7.1. Matrix fused-multiply-add (MFMA) 51 of 600
CDNA4 Instruction Set Architecture 
32x32x64 
A[32][64] 
B[64][32] 
v0
	row0 
thr 0-15 
M/N = [0-15] 
k=0-15 
	row1 
thr 16-31 
M/N = [16-31] 
k = 0-15 
	row2 
thr 32-47 
M/N = [0-15] 
k = 32-47 
	row3 
thr48-63 
M/N = [16-31] 
k = 32-47
	v1 
	v2
	v3
	k = 16-31 
	k = 16-31 
	k = 48 - 63 
	k = 48 - 63
	v4 
	v5
	



FP8: 
16x16x128 
A[16][128] 
B[128][16] 
v0
	row0 
thr 0-15 
M/N = [0-15] 
k=0-15 
	row1 
thr 16-31 
M/N = [0-15] 
k=16-31 
	row2 
thr 32-47 
M/N = [0-15] 
k = 32-47 
	row3 
thr48-63 
M/N = [0-15] 
k = 48 - 63
	v1
	v2
	v3
	v4
	k=64-79 
	k = 80-95 
	k = 96-111 
	k = 112-127
	v5
	v6
	v7
	



32x32x64 
A[32][64] 
B[64][32] 
v0
	row0 
thr 0-15 
M/N = [0-15] 
k = 0 – 15 
	row1 
thr 16-31 
M/N = [16-31] 
k = 0-15 
	row2 
thr 32-47 
M/N = [0-15] 
k = 16-31 
	row3 
thr48-63 
M/N = [16-31] 
k = 16-31
	v1
	v2
	v3
	v4
	k = 32-47 
	k = 32-47 
	k = 48-63 
	k = 48-63
	v5
	v6
	v7
	



A[m][k]/B[k][n] Layout BF16 16x16x32 
A[16][32] 
B[32][16]
	row0 
thr 0-15 
M/N = [0-15]
	row1 
thr 16-31 
M/N = [0-15]
	row2 
thr 32-47 
M/N = [0-15]
	row3 
thr48-63 
M/N = [0-15]
	v0 
	k=0-1 
	k=8-9 
	k=16-17 
	k=24-25
	v1 
	k=2-3 
	k=10-11 
	k=18-19 
	k=26-27
	v2 
	k=4-5 
	k=12-13 
	k=20-21 
	k=28-29
	v3 
	k=6-7 
	k=14-15 
	k=22-23 
	k=30-31
	



32x32x16 
A[32][16] 
B[16][32] 
v0 
	row0 
thr 0-15 
M/N = [0-15] 
k=0-1 
	row1 
thr 16-31 
M/N = [16-31] 
k=0-1 
	row2 
thr 32-47 
M/N = [0-15] 
k=8-9 
	row3 
thr48-63 
M/N = [16-31] 
k=8-9
	v1 
	k=2-3 
	k=2-3 
	k=10-11 
	k=10-11
	



7.1. Matrix fused-multiply-add (MFMA) 52 of 600
CDNA4 Instruction Set Architecture 
32x32x16 
A[32][16] 
B[16][32]
	row0 
thr 0-15 
M/N = [0-15]
	row1 
thr 16-31 
M/N = [16-31]
	row2 
thr 32-47 
M/N = [0-15]
	row3 
thr48-63 
M/N = [16-31]
	v2 
	k=4-5 
	k=4-5 
	k=12-13 
	k=12-13
	v3 
	k=6-7 
	k=6-7 
	k=14-15 
	k=14-15
	



A[m][k]/B[k][n] Layout IU8 
16x16x64 
A[16][64] 
B[64][16] 
v0 
	row0 
thr 0-15 
M/N = [0-15] 
k=0-3 
	row1 
thr 16-31 
M/N = [0-15] 
k=16-19 
	row2 
thr 32-47 
M/N = [0-15] 
k=32-35 
	row3 
thr48-63 
M/N = [0-15] 
k=48-51
	v1 
	k=4-7 
	k=20-23 
	k=36-39 
	k=52-55
	v2 
	k=8-11 
	k=24-27 
	k=40-43 
	k=56-59
	v3 
	k=12-15 
	k=28-31 
	k=44-47 
	k=60-63
	



32x32x32 
A[32][32] 
B[32][32] 
v0 
	row0 
thr 0-15 
M/N = [0-15] 
k=0-3 
	row1 
thr 16-31 
M/N = [16-31] 
k=0-3 
	row2 
thr 32-47 
M/N = [0-15] 
k=16-19 
	row3 
thr48-63 
M/N = [16-31] 
k=16-19
	v1 
	k=4-7 
	k=4-7 
	k=20-23 
	k=20-23
	v2 
	k=8-11 
	k=8-11 
	k=24-27 
	k=24-27
	v3 
	k=12-15 
	k=12-15 
	k=28-31 
	k=28-31
	



7.1.6. Broadcasting values 
While the operation of multiplying a 32 × 1 matrix of floats A by a 1 × 64 matrix B is not available natively, one can emulate this multiplication using the broadcast controls Control Broadcast SiZe (CBSZ), A Block ID (ABID), and B Lane-Group Permutation (BLGP). 
These controls impact the retrieval of values from lanes: after the lane l_a in which a particular element of A would reside is computed, that value is permuted as defined by the CBSZ and ABID fields in order to determine the lane that is accessed during the computation. Similarly, l_b, the lane to be used when retrieving any particular value of B, is permuted in the manner specified by the BLGP field. 
7.1.6.1. CBSZ and ABID 
Together, the CBSZ and ABID fields control the broadcasting of the blocks of matrix A. 
When the 3-bit CBSZ field is non-zero, one block of lanes broadcasts the values it holds for matrix A to the other blocks of lanes, superseding the values those other lanes hold for the A matrix. Setting CBSZ such that (1 << CBSZ) exceeds the number of blocks the MFMA instruction processes is undefined. 
The broadcast block size is 
S = 64 / (1 << CBSZ) 
7.1. Matrix fused-multiply-add (MFMA) 53 of 600
CDNA4 Instruction Set Architecture 
For example, if CBSZ is 1, then one block of 32 lanes provides the inputs to both groups of 32 lanes in the wavefront, while CBSZ being 3 means that a the values from a block of 8 lanes are replicated. 
The largest legal value of CBSZ is 4. 
The 4-bit ABID field controls which block of S lanes is used as the broadcast source. The possible blocks are numbered in order, with lanes S-1:0 being selected by ABID=0, 2S-1:S corresponding to ABID=1, and so on. For example, if CBSZ=2, then ABID=1 means the values from lanes 16 to 31 are broadcast, to the three other blocks of lanes, while ABID=3 means that lanes 48 to 63 serve as the source of their inputs. 
It is not legal to set ABID such that ABID >= (1 << CBSZ), as such values do not refer to a potential source block. 
Put differently, the CBSZ and ABID bits cause lane l_a to read thir inputs from the lane given by the permutation 
p_a(l_a) = (l_a % S) + (S * ABID) 
As a full example, if CBSZ=1 and ABID=1 when using the instruction V_MFMA_F32_32X32X1_2B_F32, both 1 × 32 blocks of B are multiplied by the values in the second 32 × 1 block of A, which is stored by the first 32 lanes. That is, the operation becomes: 
D[b,i,j] = C[b,i,j] + A[1,i,0] * B[b,0,j] 
which is a 32 × 1 by 1 × 64 matrix multiplication if the two blocks of B are treated as one matrix with 64 rows. 
7.1.6.2. Alternate meaning of CBSZ field for F8F6F4 instructions 
V_MFMA_F32_*_F8F6F4 use CBSZ to indicate the data type, and behave as if BLGP==0 in terms of data broadcasting. 
7.1.6.3. BLGP 
The 3-bit BLGP field selects how the lane from which values in matrix B are read is permuted. Once it is determined that some value B[b,k,j] is in item r on lane l_b, using the defined input layout, the lane to be accessed l_b is permuted depending on the BLGP field as shown in Table Permutations corresponding to BLGP values. 
Table 29. Permutations corresponding to BLGP values 
Value 
0 
	Description 
No broadcast 
	Expression 
l_b
	1 
	Broadcast first 32 lanes 
	l_b % 32
	2 
	Broadcast second 32 lanes 
	l_b % 32 + 32
	3 
	Rotate 16 lanes left 
	(l_b + 16) % 64
	4 
	Broadcast first 16 lanes 
	l_b % 16
	5 
	Broadcast second 16 lanes 
	l_b %16 + 16
	6 
	Broadcast third 16 lanes 
	l_b % 16 + 32
	



7.1. Matrix fused-multiply-add (MFMA) 54 of 600
CDNA4 Instruction Set Architecture 
Value 
	Description 
	Expression
	7 
	Broadcast fourth 16 lanes 
	l_b % 16 + 48
	



7.1.6.4. Alternate meaning of broadcast fields for F64 instructions 
The MFMA instructions that operate on double-precision floats (f64) do not support the broadcasting methods described above. 
These instructions ignore CBSZ and ABID. 
The BLGP field is repurposed for signaling the negation of the matrices A, B, and C. 
• BLGP[0] causes values from matrix A to be implicitly negated if set 
• BLGP[1] causes values from matrix B to be implicitly negated if set 
• BLGP[2] causes values from matrix C to be implicitly negated if set 
7.1.6.5. Alternate meaning of broadcast fields for F8F6F4 instructions 
V_MFMA_F32_*_F8F6F4 use BLGP to indicate the data type, and behave as if BLGP==0 in terms of data broadcasting. 
7.2. Block Scaled Matrices 
Matrix block scaling associates a unique scale factor with a block of matrix values in the K dimension. For the operations describe here, the block size is 32. Block scaled data format are: F4, F6, and F8. 
The scale factor is an exponent-offset, encoded as an 8-bit exponent (bias 127) with valid values in: -127, 127 (0xFF is NaN). 
The figure below gives an illustrative example of block scaling, assuming a tiled matrix multiplication operation of D = A x B + C, where A is of shape (M x K), B is of shape (K x N), and C/D is of shape M x N. The row and column scale factors, Ax and Bx, are vectors of dimension M and N, respectively, that store the scale factors that are needed for computing D. In the example below where M=K=N=S=4, scale factors are provided with every 1x4 row of matrix A and every 4x1 column of matrix B. During dot product operations, the scales are applied after the normal dot product prior to output/accumulation. 
7.2. Block Scaled Matrices 55 of 600
CDNA4 Instruction Set Architecture 
7.2.1. MFMA with Block Exponent Scaling 
Scale values are set for MFMA with 4-dword instructions that combine a "Load-Scale factors" and MFMA functions into one instruction: 
V_MFMA_SCALE_F32_16X16X128_F8F6F4, V_MFMA_SCALE_F32_32X32X64_F8F6F4. The scale value is used just for one instruction and does not carry forward into non-"scale" MFMA ops. 
The 4-DWORD instruction is constructed in a manner that looks like two back-to-back VOP3P’s, where the first holds has the constant 0xD3AC across what is normally the ENCODING through OPCODE fields, and the second VOP3P has OP = V_MFMA_SCALE_F32_16X16X128_F8F6F4 or V_MFMA_SCALE_F32_32X32X64_F8F6F4. 
Operands of Load-Scale (first 2 DWORDs of "SCALE" ops): 
ENCODING 
	0xCC35 in bits [31:16]
	SRC0 
	Matrix A scale 
{OP_SEL_HI [0], OP_SEL[0]} defines which part of scale is used by the Matrix A of MFMA instruction.
	SRC1 
	Matrix B scale 
{OP_SEL_HI [1], OP_SEL[1]} defines which part of scale is used by the Matrix B of MFMA instruction.
	Scale for F4/6/8 matrix (2-bit OPSEL codes): 
00: Src[7:0] Lane 0-63 is the scale to be used 
01: Src[15:8] Lane 0-63 is the scale to be used 
10: Src[23:16] Lane 0-63 is the scale to be used 
11: Src[31:24] Lane 0-63 is the scale to be used 
Scale values (SRC0 and SRC1) can be either VGPRs or Inline constants (floats, using only the exponent portion).
	



For the V_MFMA_F32_16x16x128_F8F6F4 op, the K dimension is 128. There is one scale value for every 32 K dimension values: 128/32 = 4 scale values per matrix row. The M and N dimensions are 16, so there are 16 rows. This means in total the matrix needs 16 * 4 = 64 8-bit scale values. This comes from one-quarter of one VGPR across 64 lanes. 
See the next section for the list of MFMA operations which support SCALE. 
7.2. Block Scaled Matrices 56 of 600
CDNA4 Instruction Set Architecture 
Scale data layout for 16x16 Output Matrices (K=128): 
Lane 0 
M=0, K=0..31 
	Lane 1 
M=1, K=0..31 
	… 
… 
	Lane 15 
M=15, K=0..31 
	Lane 16 
M=0, K=32..63 
	… 
… 
	Lane 32 
M=0, K=64..95 
	… 
… 
	Lane 63 
M=15, K=96..127
	



Scale data layout for 32x32 Output Matrices (K=64): 
Lane 0 
M=0, K=0..31 
	Lane 1 
M=1, K=0..31 
	… 
… 
	Lane 15 
M=15, K=0..31 
	Lane 16 
M=16, K=0..31 
	… 
… 
	Lane 32 
M=0, K=32..63 
	… 
… 
	Lane 63 
M=31, K=32..63
	



7.3. BF8 / FP8 and Smaller Formats and Conversions 
Table 30. Small Float Data Formats 
Fmt 
FP16 
	Sign-Exp 
Mant 
E5M10 
	Bias 
15 
	+0 
-0 
0x0000 
0x8000
	INF, 
-INF 
0x7C00 
0xFC00
	NaN, 
-NaN 
(normal) 
	Max 
65504 
	Min 
(norm) 
6.10352E-05 
	Min (denorm) 5.96046E-08
	FP8 
	E4M3 
	7 
	+: 0x00 
-: 0x80
	N/A 
	+: 0x7F 
-: 0xFF
	448 
	+/-2.0^(-6) 
	+/-2.0^(-9)
	BF8 
	E5M2 
	15 
	+: 0x00 
-: 0x80
	+: 0x7C 
-: 0xFC
	+: 0x7D-7F 
-: 0xFD-FF
	57344 
	+/-2^(-14) 
	2.0^(-16)
	FP6 
	E2M3 
	1 
	0 
	N/A 
	N/A 
	S.11.111 = +/- 7.5
	S.01.000 = +/- 1.0
	S.00.001 = +/- 0.125
	BF6 
	E3M2 
	3 
	0 
	N/A 
	N/A 
	S.111.11 = +/- 28.0
	S.001.00 = +/- 0.25
	S.000.11 = +/- 0.0675
	FP4 
	E2M1 
	1 
	0 
	N/A 
	N/A 
	S.11.1 = +/- 6.0
	S01.0 = +/-1.0 
	S.0.01 = +/- 0.5
	



Table 31. Small Float Data Format Conversion ops 
Instruction 
CVT_PK_FP8_F32 
	Dst 
FP8 
	Src0 
FP32 
	Src1 
FP32 
	Encoding 
VOP3 
	Control 
Op_Sel[3] 
ignores: clamp, omod supports: neg, abs
	Notes 
RNE
	CVT_PK_BF8_F32 
	BF8 
	FP32 
	FP32 
	VOP3 
	Op_Sel[3] 
ignores: clamp, omod supports: neg, abs
	RNE
	CVT_SR_FP8_F32 
	FP8 
	FP32 
	U32 
	VOP3 
	Op_Sel[3:2] 
ignores: clamp, omod supports: neg, abs
	Stochastic Rounding
	CVT_SR_BF8_F32 
	BF8 
	FP32 
	U32 
	VOP3 
	Op_Sel[3:2] 
ignores: clamp, omod supports: neg, abs
	Stochastic Rounding
	CVT_SR_FP16_F32 
	FP16 
	FP32 
	U32 
	VOP3 
	Op_Sel[3] 
ignores: clamp, omod supports: neg, abs
	Stochastic Rounding
	CVT_SR_BF16_F32 
	BF16 
	FP32 
	U32 
	VOP3 
	Op_Sel[3] 
ignores: clamp, omod supports: neg, abs
	Stochastic Rounding
	



7.3. BF8 / FP8 and Smaller Formats and Conversions 57 of 600
CDNA4 Instruction Set Architecture 
Instruction 
	Dst 
	Src0 
	Src1 
	Encoding 
	Control 
	Notes
	CVT_PK_F32_FP8 
	F32 
	FP8 
	- 
	VOP1 
	SDWA, Op_Sel[0], 
dst,dst+1 
ignores: abs, neg, sext
	dst must be even
	CVT_PK_F32_BF8 
	F32 
	BF8 
	- 
	VOP1 
	SDWA, Op_Sel[0], 
dst,dst+1 
ignores: abs, neg, sext
	dst must be even
	CVT_F32_FP8 
	F32 
	FP8 
	- 
	VOP1 
	SDWA, Op_Sel 
ignores: abs, neg, sext
	-
	CVT_F32_BF8 
	F32 
	BF8 
	- 
	VOP1 
	SDWA, Op_Sel 
ignores: abs, neg, sext
	-
	



Table 32. Small Float Data Format Conversion ops with SCALE 
4-Bit 
CVT_SCALE_PK_FP4_F32 
CVT_SCALE_SR_PK_FP4_F32 
CVT_SCALE_PK_F32_FP4
	6-Bit 
CVT_SCALE_PK_FP6_F32 
CVT_SCALE_PK_BF6_F32 
CVT_SCALE_SR_PK_FP6_F32 
CVT_SCALE_SR_PK_BF6_F32 
CVT_SCALE_PK_F32_FP6 
CVT_SCALE_PK_F32_BF6
	8-Bit 
CVT_SCALE_PK_FP8_F32 
CVT_SCALE_PK_BF8_F32 
CVT_SCALE_SR_FP8_F32 
CVT_SCALE_SR_BF8_F32 
CVT_SCALE_PK_F32_FP8 
CVT_SCALE_PK_F32_BF8 
CVT_SCALE_F32_FP8 
CVT_SCALE_F32_BF8
	CVT_SCALE_PK_FP4_F16 
CVT_SCALE_PK_FP4_BF16 
CVT_SCALE_SR_PK_FP4_F16 
CVT_SCALE_SR_PK_FP4_BF16 
CVT_SCALE_PK_F16_FP4 
CVT_SCALE_PK_BF16_FP4
	CVT_SCALE_PK_FP6_F16 
CVT_SCALE_PK_FP6_FB16 
CVT_SCALE_PK_BF6_F16 
CVT_SCALE_PK_BF6_BF16 
CVT_SCALE_SR_PK_FP6_F16 
CVT_SCALE_SR_PK_FP6_BF16 
CVT_SCALE_SR_PK_BF6_F16 
CVT_SCALE_SR_PK_BF6_BF16  CVT_SCALE_PK_F16_FP6 
CVT_SCALE_PK_F16_BF6 
CVT_SCALE_PK_BF16_FP6 
CVT_SCALE_PK_BF16_BF6
	CVT_SCALE_PK_FP8_F16 
CVT_SCALE_PK_BF8_F16 
CVT_SCALE_PK_FP8_BF16 
CVT_SCALE_PK_BF8_BF16 
CVT_SCALE_SR_FP8_F16 
CVT_SCALE_SR_BF8_F16 
CVT_SCALE_SR_FP8_BF16 
CVT_SCALE_SR_BF8_BF16  
CVT_SCALE_PK_F16_FP8 
CVT_SCALE_PK_F16_BF8 
CVT_SCALE_F16_FP8 
CVT_SCALE_F16_BF8
	16-Bit 
	Integer-8
	

	CVT_PK_F16_F32 
CVT_PK_BF16_F32 
CVT_F32_BF16
	ASHR_PK_I8_I32 
ASHR_PK_U8_I32
	

	



All of the instructions in the table above use VOP3. 
Note: VOP3 instructions may not use SDWA. 
In the above table, the CVT_*_F32 instructions do not support 4-cycle forwarding on these operations. The user must insert a NOP or instruction writing some other destination VREG between the conversions writing the low/high half or bytes of the same destination register. 
Convert instructions come in two types: 
• Packed – convert two 8-bit values into 32-bit values per instruction 
• Stochastic Round – one source has the number to convert and the other has a random number used in rounding 
◦ These ops add a random value from the specified VGPR and then truncate to the smaller result data 
7.3. BF8 / FP8 and Smaller Formats and Conversions 58 of 600
CDNA4 Instruction Set Architecture 
type 
◦ For convert ops requiring a Stochastic Rounding Value over multiple passes, an updated random number is generated every pass. 
▪ Multipass: FP32, FP16 or BF16 to FP4 or FP6 
▪ The VGPR holding the PRNG is not updated; the new pseudo-random value is created internally via V_PRNG_B32 but not written 
Converts from 8-bit formats and SDWA in VOP1: 
• SRC0 must be set to “SDWA”, and the SRC0 VGPR is specified in the SDWA word as is the SRC0_SELECT which specifies which bytes to be converted. The other SDWA fields are ignored. 
Converts with Scale: 
• Conversion from F4/F6/F8 do not support input modifiers 
• Conversion from F32 supports MODE-based denormal control; F4/F6/F8 allows denorms regardless of MODE 
• Conversion to F4/F6 does not support FP16_OVFL, while to F8 does 
• Convert ops do not support OMOD or DPP 
• The scale is an E8M0 exponent with a bias of 127 
• The scale can come from a VGPR or an inline-constant (float exponent portion is used). 
If a value exceeds the FP4/FP6 representable range after rounding, the value is clamped/saturated to the maximum FP4/FP6 magnitude, preserving the sign. During conversion to FP4/FP6, if a value has magnitude less than the minimum subnormal magnitude of FP4/FP6 after rounding, the value is converted to zero. 
CVT_SR_FP8_F32 and CVT_SR_BF8_F32 OP_SEL usage: 
Op_sel[3:2] == 00: dest_vgpr[31:0] = {prev_dst_vgpr[31:8], result[7:0]} 
Op_sel[3:2] == 01: dest_vgpr[31:0] = {prev_dst_vgpr[31:16], result[7:0], prev_dst_vgpr[7:0]} Op_sel[3:2] == 10: dest_vgpr[31:0] = {prev_dst_vgpr[31:24], result[7:0], prev_dst_vgpr[15:0]} Op_sel[3:2] == 11: dest_vgpr[31:0] = {result[7:0], prev_dst_vgpr[23:0]} 
CVT_SR_FP8_F32 OP_SEL usage: 
Uses Src0 and Src1 as inputs supplied by the VOP3 encoding, it adds the two operands with attention to not use msbs of src1 mantissa based on opcode and dependent on the F8 data type for the stochastic round before converting to F8 type.  Then OP_SEL bits 3 and 2 are repurposed for this 8b write op and used to steer the resulting 8 bits into the correct byte lane of the 32b output preserving the remaining 24b of the destination.  
CVT_*FP8_F32 and CVT*_BF8_F32 FP16_OVFL rule 
The FP16_OVFL flag is applied to data conversions from F32 to FP8/BF8 formats. The overflow behaviour is specified in the table below: 
CVT_SR_* and CVT_PK_* support only VGPRs as inputs, not SGPRs, literal or inline constants. 
Source Value
	Destination Value
	FP8 
	BF8
	FP16_OVFL=1 
	FP16_OVFL=0 
	FP16_OVFL=1 
	FP16_OVFL=0
	NaN 
	NaN 
	NaN 
	NaN 
	NaN
	±Inf 
	±max_E4M3 
	NaN 
	±max_E5M2 
	±Inf
	



7.3. BF8 / FP8 and Smaller Formats and Conversions 59 of 600
CDNA4 Instruction Set Architecture 
Greater than max FP8 magnitude 
	±max_E4M3 
	NaN 
	±max_E5M2 
	±Inf
	



The register SH_MEM_CONFIG, bit[8] must be set to 1 to produce the correct results for BF8 and FP8 operations. 
7.4. Floating-point handling details and formats 
The handling of denormal numbers varies depending on the datatypes the instruction takes and, in some cases, the MODE flags. 
• V_MFMA_F32_*_F32 instructions, which take 32-bit inputs, honor the denormal-handling flags in MODE • Matrix-C input and result-matrix output ignore MODE.denorm and do not flush denormals • All instructions that take floats whose size is less than 32-bits (F16, BF16, BF8, FP8) ignore MODE.denorm and do not flush denormals 
• The V_MFMA_F64_*_F64 instructions, which take 64-bit inputs and outputs ignores MODE and rounds to nearest even and allows denorms in the input and output 
• The V_MFMA_I32_*_I8 perform integer multiply-add and thus do not respect the MODE bits. The 16-bit results of multiplying the I8 input values are sign-extended to 32 bits before multiplication, and the 16-bit result of the multiplication is sign extended to 32 bits prior to being added to the 32-bit result 
The matrix core unit does not support arithmetic exceptions, except for DGEMM matrix operation which does support exceptions. 
7.5. Sparse Matrices 
The V_SMFMAC family of instructions perform matrix multiply-accumulate operations on a 4:2 structurally sparse matrix A and dense matrices B, C, and D: D = C + A × B. 
The A matrix is represented using 4:2 structured sparsity which means that two out of every 4 elements along the matrix K-dimension are zero. These zeros are not stored directly but instead are described in a separate VGPR which holds pairs of 2-bit index values. The index values indicate which two values out of each group of 4 are non-zero and are used to reconstruct full A-matrix. Non-zero samples are tightly packed resulting in 2:1 compression. Only the A-matrix may be sparse. 
These SMFMAC instructions are all "accumulate" ops, where the C and D matrices are identical, referenced by the instruction’s VDST field (D-matrix). The C operand input is repurposed to hold the index data offset. 
Table 33. SMFMA VALU Opcodes: 
Instruction 
V_SMFMAC_F32_{*}_F16 
	Variants 
16x16x32 
	Blocks 
1 
	Cycles 
16 
	Description 
Sparse Matrix multiply of F16 data
	32x32x16 
	1 
	32
	V_SMFMAC_F32_{*}_BF16 
	16x16x32 
	1 
	16 
	Sparse Matrix multiply of BF16 data
	32x32x16 
	1 
	32
	V_SMFMAC_I32_{*}_I8 
	16x16x64 
	1 
	16 
	Sparse Matrix multiply of I8 data
	32x32x32 
	1 
	32
	



7.4. Floating-point handling details and formats 60 of 600
CDNA4 Instruction Set Architecture 
Instruction 
	Variants 
	Blocks 
	Cycles 
	Description
	V_SMFMAC_F32_{*}_BF8_BF8 
V_SMFMAC_F32_{*}_BF8_FP8 
V_SMFMAC_F32_{*}_FP8_BF8 
V_SMFMAC_F32_{*}_FP8_FP8
	16x16x64 
	1 
	16 
	Sparse Matrix multiply of BF8 or FP8 data
	32x32x32 
	1 
	32
	V_SMFMAC_F32_16X16X64_BF16 V_SMFMAC_F32_16X16X64_F16
	16x16x64 
	1 
	16 
	Sparse Matrix Multiply of FP16/BF16 data
	V_SMFMAC_I32_16X16X128_I8 
	16x16x128 
	1 
	16 
	Sparse Matrix Multiply of Int8 data
	V_SMFMAC_F32_16x16x128_BF8_BF8 V_SMFMAC_F32_16x16x128_BF8_FP8 V_SMFMAC_F32_16x16x128_FP8_BF8 V_SMFMAC_F32_16x16x128_FP8_FP8
	16x16x128 
	1 
	16 
	Sparse Matrix Multiply of FP8/BF8 data
	V_SMFMAC_F32_32X32X32_BF16 V_SMFMAC_F32_32X32X32_F16
	32x32x32 
	1 
	32 
	Sparse Matrix Multiply of FP16/BF16 data
	V_SMFMAC_I32_32X32X64_I8 
	32x32x64 
	1 
	32 
	Sparse Matrix Multiply of Int8 data
	V_SMFMAC_F32_32x32x64_BF8_BF8 V_SMFMAC_F32_32x32x64_BF8_FP8 V_SMFMAC_F32_32x32x64_FP8_BF8 V_SMFMAC_F32_32x32x64_FP8_FP8
	32x32x64 
	1 
	32 
	Sparse Matrix Multiply of FP8/BF8 data
	



Matrix A is structurally sparse and occupies two VGPRs per lane at srcA offset. Matrix B is dense and occupies four VGPRs per lane at srcB offset. Matrix C shares VGPR offset with destination argument and occupies 16 VGPRs. 
16-bit source data 
If CBSZ[1:0] =0, ABID[1:0] selects one of four 8-bit sets of sparse-indices within a VGPR starting at srcC containing 8-bits of index information for a lane. If CBSZ[1:0] !=0; the very first is selected (VGPR[srcC][7..0]). 
8-bit source data 
If CBSZ[1:0] =0, ABID[0] selects one of two 16-bit sets of sparse-indices within a VGPR starting at srcC containing 16-bits of index information for a lane. If CBSZ[1:0] !=0; the very first is selected (VGPR[srcC][15..0]). 
All SMFMAC instructions must follow these restrictions: 
1. The Matrix A is sparse matrix and matrix B is the dense matrix. The ALU loads twice more data from VGPR for matrix B comparing with matrix A. 
2. Matrix C is the same as the result Matrix. The ALU uses the VDST VGPR to load matrix C. All instructions are encoded as accumulation opcodes. 
3. Src2 has the index encoded (all of the indexes are in one VGPR) and it can only be VGPR. Index Data provides information about which 2 out 4 SRCA are non-zero. For this index pair, index 0 < index 1 & index0 != index1. 
4. The VGPR address of Src0, src1 and VDST must be even aligned. 
5. CBSZ and ABID controls are ONLY used to pick the index from the VGPR read and don’t affect SRCA matrix broadcast etc. as defined for other MFMA opcodes that use CBSZ and ABID controls. 
SMFMAC instructions interpret the ACC_CD differently from other instructions: For SMFMAC the ACC_CD bit control only the DEST vgpr (arch vs accum), not the SRC2 location. The SRC2 argument provides the index data for sparse data supplied by the SRC0 argument which must reside in the ARCH-vgprs along with the A and B matrix data. In other words SRC2 acts as if ACC_CD==0. 
7.5. Sparse Matrices 61 of 600
CDNA4 Instruction Set Architecture 
Denorm Control 
	ignores Denorm Control from MODE and keep Input/Output Denorms.
	Clamp 
	Supported. uses the FP16_OVFL bit from MODE. 
If set, F32 Result on overflow is clamped to +/- MAX, otherwise the overflow result is normalized to +/-INF. 
If set, I32 Result is clamped to +/-MAX on overflow/underflow, otherwise the carry out bits are dropped.
	FP16_Ovfl 
	Once the FP16_OVFL is set, F32 overflow result is clamped to +/- MAX, otherwise the overflow result is normalized to +/-INF.
	Round Mode 
	ignores Round Mode from MODE and forces it to RNE.
	Exceptions 
	Not Supported
	Execution Mask 
	ignores exec mask from MODE and forces it to 1 for all threads
	

	Operand Alignment/Sources Src0/1/VDST if VGPR needs to be even aligned. Src0/1/VDST can be only VGPR/ACCVGPR 
Src2 can only be VGPR (No even alignment req)
	Scale 
	No support for FP16, BF16,I8 MFMA Opcodes
	Sparse Index Select 
	16x16x64_BF16, 32x32x32_BF16, 16x16x64_F16, 32x32x32_F16 : 
If CBSZ[1:0] =0, ABID[0] selects one of two, 8 bit sets within a VGPR starting at srcC containing 8 bits of index information for a lane. If CBSZ[1:0] !=0; the very first set is selected (VGPR[srcC][7..0]). 
16x16x128_IU8/*F8, 32x32x64_IU8/*F8: 
CBSZ[1:0] ,ABID[1:0] fields ignored. One single defined set within a VGPR.
	



7.5.1. Details of Sparsity Structure 
Every index for the matrix B selection is a 2-bit number which identifies one of K=4 is selected. Depending on the matrix B layout, SRC2 may hold multiple sets of indices. 
7.5.1.1. 16-bit A/B Matrix 
When the A and B matrices consist of 16-bit data (FP16, BF16), the rules below apply. 
Table 34. Matrix B Layout 
16x16x32 v0 
	Row0 Row1 k=0,1 
	k=8,9 
	Row2 
k=16,17 
	Row3 
k=24,25
	v1 
	k=2,3 
	k=10,11 
	k=18,19 
	k=26,27
	v2 
	k=4,5 
	k=12,13 
	k=20,21 
	k=28,29
	v3 
	k=6,7 
	k=14,15 
	k=22,23 
	k=30,31
	32x32x16 
	Row0 
	Row1 
	Row2 
	Row3
	v0 
	k=0,1 
	k=0,1 
	k=8,9 
	k=8,9
	v1 
	k=2,3 
	k=2,3 
	k=10,11 
	k=10,11
	v2 
	k=4,5 
	k=4,5 
	k=12,13 
	k=12,13
	v3 
	k=6,7 
	k=6,7 
	k=14,15 
	k=14,15
	



Each lane has K=8 values which requires 4 indices per lane (8 bits), so each SRC2 VGPR holds 4 sets of indices. 
Table 35. Index Layout 
Lane ID 
Vn[31:30] 
	0 1 … 3 4 … 31 32 … 63 set3, V1[31:16]
	



7.5. Sparse Matrices 62 of 600
CDNA4 Instruction Set Architecture 
Lane ID 
	0 
	1 
	… 3 
	

	4 
	

	… 31 
	32 
	

	… 63
	Vn[29:28] 
	set3, V1[15:0]
	Vn[27:26] 
	set3, V0[31:16]
	Vn[25:24] 
	set3, V0[15:0]
	… 
	…
	Vn[9:8] 
	set1, V0[15:0]
	Vn[7:6] 
	set0, V1[31:16]
	Vn[5:4] 
	set0, V1[15:0]
	Vn[3:2] 
	set0, V0[31:16]
	Vn[1:0] 
	set0, V0[15:0]
	



"Vn" is the SRC-C VGPR holding the index values. 
7.5.1.2. 8-bit A/B Matrix 
When the A and B matrices consist of 8-bit data (I8, FP8, BF8), the rules below apply. 
Table 36. Matrix B Layout 
16x16x64 v0 
	Row0 
k=0,1,2,3 
	Row1 
k=16,17,18,19 
	Row2 
k=32,33,34,35 
	Row3 
k=48,49,50,51
	v1 
	k=4,5,6,7 
	k=20,21,22,23 
	k=36,37,38,39 
	k=52,53,54,55
	v2 
	k=8,9.10,11 
	k=24,25,26,27 
	k=40,41,42,43 
	k=56,57,58,59
	v3 
	k=12,13,14,15 
	k=28,29,30,31 
	k=44,45,46,47 
	k=60,61,62,63
	32x32x32 
	Row0 
	Row1 
	Row2 
	Row3
	v0 
	k=0,1,2,3 
	k=0,1,2,3 
	k=16,17,18,19 
	k=16,17,18,19
	v1 
	k=4,5,6,7 
	k=4,5,6,7 
	k=20,21,23,23 
	k=20,21,23,23
	v2 
	k=8,9,10,11 
	k=8,9,10,11 
	k=24,25,26,27 
	k=24,25,26,27
	v3 
	k=12,13,14,15 
	k=12,13,14,15 
	k=28,29,30,31 
	k=28,29,30,31
	



Each lane has K=16 values which requires 8 indices per lane (16 bits), so each SRC2 VGPR holds 2 sets of indices. 
Table 37. Index Layout 
Lane ID 
Vn[31:30] 
	0 1 … 3 4 … 31 32 … 63 set1, V1[31:24]
	Vn[29:28] 
	set1, V1[23:16]
	Vn[27:26] 
	set1, V1[15:8]
	Vn[25:24] 
	set1, V1[7:0]
	Vn[23:22] 
	set1, V0[31:24]
	Vn[21:20] 
	set1, V0[23:16]
	Vn[19:19] 
	set1, V0[15:8]
	Vn[17:16] 
	set1, V0[7:0]
	Vn[15:14] 
	set0, V1[31:24]
	… 
	…
	Vn[3:2] 
	set0, V0[15:8]
	Vn[1:0] 
	set0, V0[7:0]
	



7.5. Sparse Matrices 63 of 600
CDNA4 Instruction Set Architecture 7.5.1.3. Sparse Matrix Index Layout 
BF16 Layouts for Matrix A : A is a sparse matrix (2 out of every 4k = 0) and packed as A[16][32] (for SMFMAC_F32_16x16x64_BF16) or A[32][16] (SMFMAC_F32_32x32x32_BF16). 
F16/BF16 Layout for Matrix B 
B[64][16] 
	row0 
thr 0-15 
N = [0-15]
	row1 
thr 16-31 
N = [0-15]
	row2 
thr 32-47 
N = [0-15]
	row3 
thr48-63 
N = [0-15]
	v0 
	k=0-1 
	k=8-9 
	k=16-17 
	k=24-25
	v1 
	k=2-3 
	k=10-11 
	k=18-19 
	k=26-27
	v2 
	k=4-5 
	k=12-13 
	k=20-21 
	k=28-29
	v3 
	k=6-7 
	k=14-15 
	k=22-23 
	k=30-31
	v4 
	k=32-33 
	k=40-41 
	k=48-49 
	k=56-57
	v5 
	k=34-35 
	k=42-43 
	k=50-51 
	k=58-59
	v6 
	k=36-37 
	k=44-45 
	k=52-53 
	k=60-61
	v7 
	k=38-39 
	k=46-47 
	k=54-55 
	k=62-63
	



B[32][32] 
v0 
	row0 
thr 0-15 
N = [0-15] 
k=0-1 
	row1 
thr 16-31 
N = [16-31] 
k=0-1 
	row2 
thr 32-47 
N = [0-15] 
k=8-9 
	row3 
thr48-63 
N = [16-31] 
k=8-9
	v1 
	k=2-3 
	k=2-3 
	k=10-11 
	k=10-11
	v2 
	k=4-5 
	k=4-5 
	k=12-13 
	k=12-13
	v3 
	k=6-7 
	k=6-7 
	k=14-15 
	k=14-15
	v4 
	k=16-17 
	k=16-17 
	k=24-25 
	k=24-25
	v5 
	k=18-19 
	k=18-19 
	k=26-27 
	k=26-27
	v6 
	k=20-21 
	k=20-21 
	k=28-29 
	k=28-29
	v7 
	k=22-23 
	k=22-23 
	k=30-31 
	k=30-31
	



F16/BF16 Index Layout : 
Index layouts map SRCA Matrix elements that are not sparse to indicate which 2/4 k values are non-zero. Layout below maps directly to SRCA Matrix (opcode reads 4 VGPRS shown as V0-V3 as an example) 
[31:30] 
	Row0 
Set1 V3[31:16] 
	Row1 
Set1 V3[31:16] 
	Row2 
Set1 V3[31:16] 
	Row3 
Set1 V3[31:16]
	[29:28] 
	Set1 V3[15:0] 
	Set1 V3[15:0] 
	Set1 V3[15:0] 
	Set1 V3[15:0]
	[27:26] 
	Set1 V2[31:16] 
	Set1 V2[31:16] 
	Set1 V2[31:16] 
	Set1 V2[31:16]
	[25:24] 
	Set1 V2[15:0] 
	Set1 V2[15:0] 
	Set1 V2[15:0] 
	Set1 V2[15:0]
	[23:22] 
	Set1 V1[31:16] 
	Set1 V1[31:16] 
	Set1 V1[31:16] 
	Set1 V1[31:16]
	[21:20] 
	Set1 V1[15:0] 
	Set1 V1[15:0] 
	Set1 V1[15:0] 
	Set1 V1[15:0]
	[19:18] 
	Set1 V0[31:16] 
	Set1 V0[31:16] 
	Set1 V0[31:16] 
	Set1 V0[31:16]
	[17:16] 
	Set1 V0[15:0] 
	Set1 V0[15:0] 
	Set1 V0[15:0] 
	Set1 V0[15:0]
	[15:14] 
	Set0 V3[31:16] 
	Set0 V3[31:16] 
	Set0 V3[31:16] 
	Set0 V3[31:16]
	[13:12] 
	Set0 V3[15:0] 
	Set0 V3[15:0] 
	Set0 V3[15:0] 
	Set0 V3[15:0]
	[11:10] 
	Set0 V2[31:16] 
	Set0 V2[31:16] 
	Set0 V2[31:16] 
	Set0 V2[31:16]
	[9:8] 
	Set0 V2[15:0] 
	Set0 V2[15:0] 
	Set0 V2[15:0] 
	Set0 V2[15:0]
	[7:6] 
	Set0 V1[31:16] 
	Set0 V1[31:16] 
	Set0 V1[31:16] 
	Set0 V1[31:16]
	



7.5. Sparse Matrices 64 of 600
CDNA4 Instruction Set Architecture 


	Row0 
	Row1 
	Row2 
	Row3
	[5:4] 
	Set0 V1[15:0] 
	Set0 V1[15:0] 
	Set0 V1[15:0] 
	Set0 V1[15:0]
	[3:2] 
	Set0 V0[31:16] 
	Set0 V0[31:16] 
	Set0 V0[31:16] 
	Set0 V0[31:16]
	[1:0] 
	Set0 V0[15:0] 
	Set0 V0[15:0] 
	Set0 V0[15:0] 
	Set0 V0[15:0]
	



IU8/F8 Layouts for Matrix A : 
A is a sparse matrix (2 out of every 4k = 0) and packed as A[16][64] (for SMFMAC_*32_16x16x128_*) or A[32][32] (SMFMAC_*32_32x32x64_*). 
IU8/F*8 Layout for Matrix B B[128][16] 
	row0 
	row1 
	row2 
	row3
	v0 
	k=0-3 
	k=16-19 
	k=32-35 
	k=48-51
	v1 
	k=4-7 
	k=20-23 
	k=36-39 
	k=52-55
	v2 
	k=8-11 
	k=24-27 
	k=40-43 
	k=56-59
	v3 
	k=12-15 
	k=28-31 
	k=44-47 
	k=60-63
	v4 
	k=64-67 
	k=80-83 
	k=96-99 
	k=112-115
	v5 
	k=68-71 
	k=84-87 
	k=100-103 
	k=116-119
	v6 
	k=72-75 
	k=88-91 
	k=104-107 
	k=120-123
	v7 
	k=76-79 
	k=92-95 
	k=108-111 
	k=124-127
	



B[64][32] 
v0 
	row0 
k=0-3 
	row1 
k=0-3 
	row2 
k=16-19 
	row3 
k=16-19
	v1 
	k=4-7 
	k=4-7 
	k=20-23 
	k=20-23
	v2 
	k=8-11 
	k=8-11 
	k=24-27 
	k=24-27
	v3 
	k=12-15 
	k=12-15 
	k=28-31 
	k=28-31
	v4 
	k=32-35 
	k=32-35 
	k=48-51 
	k=48-51
	v5 
	k=36-39 
	k=36-39 
	k=52-55 
	k=52-55
	v6 
	k=40-43 
	k=40-43 
	k=56-59 
	k=56-59
	v7 
	k=44-47 
	k=44-47 
	k=60-63 
	k=60-63
	



IU8/F*8 Index Layout : 
Index layouts map SRCA Matrix elements that are not sparse to indicate which 2/4 k values are non-zero. Layout below maps directly to SRCA Matrix (opcode reads 4 VGPRS shown as V0-V3 as an example) 
[31:30] 
	Row0 
Set0 V3[31:24] 
	Row1 
Set0 V3[31:24] 
	Row2 
Set0 V3[31:24] 
	Row3 
Set0 V3[31:24]
	[29:28] 
	Set0 V3[23:16] 
	Set0 V3[23:16] 
	Set0 V3[23:16] 
	Set0 V3[23:16]
	[27:26] 
	Set0 V3[16:8] 
	Set0 V3[16:8] 
	Set0 V3[16:8] 
	Set0 V3[16:8]
	[25:24] 
	Set0 V3[7:0] 
	Set0 V3[7:0] 
	Set0 V3[7:0] 
	Set0 V3[7:0]
	[23:22] 
	Set0 V2[31:24] 
	Set0 V2[31:24] 
	Set0 V2[31:24] 
	Set0 V2[31:24]
	[21:20] 
	Set0 V2[23:16] 
	Set0 V2[23:16] 
	Set0 V2[23:16] 
	Set0 V2[23:16]
	[19:18] 
	Set0 V2[16:8] 
	Set0 V2[16:8] 
	Set0 V2[16:8] 
	Set0 V2[16:8]
	[17:16] 
	Set0 V2[7:0] 
	Set0 V2[7:0] 
	Set0 V2[7:0] 
	Set0 V2[7:0]
	[15:14] 
	Set0 V1[31:24] 
	Set0 V1[31:24] 
	Set0 V1[31:24] 
	Set0 V1[31:24]
	[13:12] 
	Set0 V1[23:16] 
	Set0 V1[23:16] 
	Set0 V1[23:16] 
	Set0 V1[23:16]
	[11:10] 
	Set0 V1[16:8] 
	Set0 V1[16:8] 
	Set0 V1[16:8] 
	Set0 V1[16:8]
	[9:8] 
	Set0 V1[7:0] 
	Set0 V1[7:0] 
	Set0 V1[7:0] 
	Set0 V1[7:0]
	[7:6] 
	Set0 V0[31:24] 
	Set0 V0[31:24] 
	Set0 V0[31:24] 
	Set0 V0[31:24]
	



7.5. Sparse Matrices 65 of 600
CDNA4 Instruction Set Architecture 


	Row0 
	Row1 
	Row2 
	Row3
	[5:4] 
	Set0 V0[23:16] 
	Set0 V0[23:16] 
	Set0 V0[23:16] 
	Set0 V0[23:16]
	[3:2] 
	Set0 V0[16:8] 
	Set0 V0[16:8] 
	Set0 V0[16:8] 
	Set0 V0[16:8]
	[1:0] 
	Set0 V0[7:0] 
	Set0 V0[7:0] 
	Set0 V0[7:0] 
	Set0 V0[7:0]
	



7.6. Dependency Resolution: Required Independent Instructions 
The table below indicates timing conditions which require the user to insert NOPs (or independent VALU instructions). 
DLop Dot products 
XDLOP Matrix math on {I8, F16, BF16} 
DGEMM V_MFMA…F64 
PASS 4 clock cycles 
Table 38. VOP3P-Matrix Opcodes Required NOPs 
First Instruction 
Non-DLops VALU Write VGPR 
	Second Instruction 
V_MFMA* read VGPR OR V_SMFMA* read VGPR
	Required Waits 
2 
	Comments 
No internal 4 & 8 cycle forwarding path.
	DL ops Write VGPR 
	DLops read VGPR as SrcC, and the opcode is exactly the same as 1st DLops
	0 
	We can only support same opcode of DLops back-to-back SrcC forwarding which is used for accumulation.
	DLops read VGPR as 
SrcA/B, and the opcode is exactly the same as 1st DLops
	3 
	We don’t support SrcA/B forwarding in DLops
	Any opcode read/write VGPR that is not exactly the same as 1st DLops 
opcode (RAW + WAW)
	3 
	Disable all of the forwarding path from DL ops to normal VALU/VM/LDS/FLAT ops
	



7.6. Dependency Resolution: Required Independent Instructions 66 of 600
CDNA4 Instruction Set Architecture 
First Instruction 
	Second Instruction 
	Required Waits
	Comments
	XDL Write VGPR or V_SMFMA* Write VGPR
	XDL read VGPR as Source C exactly same with 1st vDst OR V_SMFMA* read VGPR for Matrix C exactly same with 1st vDst
	2 
	the two V_MFMA must be the same number passes and vDst and vSrc start from the same offset and same VGPR size. V_MFMA & V_SMFMA must be the same number passes and both vDst start from the same offset and same VGPR size. Note: V_SMFMA reads vdst for Matrix C.
	0
	0
	0
	XDL read VGPR as Source C overlapped with 1st vDst OR V_SMFMA* read VGPR 
for Matrix C overlapped with 1st vDst
	4 
	overlapped with XDL 
Note: V_SMFMA reads vdst for Matrix C.
	6
	10
	18
	S/DGEMM read VGPR as Source C
	3 
	Overlapped with S/DGEMM
	6
	10
	18
	V_MFMA read VGPR as SrcA or SrcB OR 
V_SMFMA* read VGPR as SrcA or SrcB or Index SrcC
	5 
	No internal forwarding path, need to wait previous V_MFMA/V_SMFMA* commit result to VGPR V_SMFMA uses srcC address for extra Index C Reads
	8
	12
	20
	1) VM, LDS, FLAT, Export Read VGPR overlapped with 1st vDst 2) VALU 
read/write VGPR (RAW + WAW)
	5
	

	8
	

	12
	

	20
	

	SGEMM Write VGPR 
	SGEMM read VGPR as Source C exactly same with 1st vDst
	2 
	the two SGEMM must be the same number passes and vDst and vSrc start from the same offset and same VGPR size.
	0
	0
	0
	S/DGEMM read VGPR as Source C overlapped with 1st vDst
	2 
	Overlapped with S/DGEMM
	4
	8
	16
	XDL read VGPR as Source C overlapped with 1st vDst or V_SMFMA* read VGPR for Matrix C overlapped 
with 1st vDst
	0 
	XDL 2x opcodes read SRCC Faster
	S/DGEMM, V_MFMA read VGPR as SrcA or SrcB OR V_SMFMA* read VGPR as SrcA or SrcB or Index SrcC
	4 
	No internal forwarding path, need to wait previous V_MFMA commit result to VGPR Note: V_SMFMA used SrcC for Index reads.
	6
	10
	18
	1) VM, LDS, FLAT, Export Read VGPR overlapped with 1st vDst 2) VALU 
read/write VGPR (RAW + WAW)
	4 
	V_MFMA_F32_4X4X4F16
	6 
	V_MFMA_F32_16X16X16F16
	10 
	V_MFMA_F32_32X32X8F16
	18 
	V_MFMA_F32_32X32X4F16
	



7.6. Dependency Resolution: Required Independent Instructions 67 of 600
CDNA4 Instruction Set Architecture 
First Instruction 
	Second Instruction 
	Required Waits
	Comments
	V_MFMA_16x16x4_F64 Write VGPR V_MFMA_16x16x4_F64
	read VGPR as Source C exactly same with 1st vDst
	0 
	the two V_MFMA must be the same number passes and vDst and vSrc start from the same offset.
	S/DGEMM read VGPR as Source C overlapped with 1st vDst
	17 
	overlapped, different VGPR access sequence
	XDL read VGPR as Source C overlapped with 1st vDst
	0 
	2 HW Waits
	V_SMFMA* read VGPR for Matrix C overlapped with 1st vDst
	0 
	V_SMFMA reads vdst for Matrix C.
	S/DGEMM read VGPR as SrcA or SrcB
	19 
	No internal forwarding path, need to wait previous V_MFMA commit result to VGPR
	XDL read VGPR as SrcA or SrcB
	19
	

	V_SMFMA* read VGPR as SrcA or SrcB or Index SrcC
	19 
	V_SMFMA uses srcC address for extra Index C Reads
	VALU read/write VGPR (RAW + WAW)
	19
	

	VM, LDS, FLAT and Export Read VGPR overlapped with 1st vDst
	18 
	No internal forwarding path, need to wait previous V_MFMA commit result to VGPR
	V_MFMA_4x4x4_F64 Write VGPR 
	V_MFMA_4x4x4_F64, read VGPR as Source C exactly same with 1st vDst
	4 
	4x4x4 needs to do accumulation, so the write VGPR is later than normal XDL 4x4x4, so needs extra wait
	S/DGEMM read VGPR as Source C overlapped with 1st vDst
	4 
	overlapped, different VGPR access sequence
	XDL read VGPR as Source C overlapped with 1st vDst
	0 
	2 HW Waits
	V_SMFMA read VGPR for Matrix C overlapped with 1st vDst
	0 
	V_SMFMA reads vdst for Matrix C.
	S/DGEMM read VGPR as SrcA or SrcB
	6 
	No internal forwarding path, need to wait previous V_MFMA commit result to VGPR
	XDL read VGPR as SrcA or SrcB
	6 
	Already have 2 hardware waits, so only needs 1 software waits.
	V_SMFMA* read VGPR as SrcA or SrcB or Index SrcC
	6 
	V_SMFMA uses srcC address for extra Index C Reads
	VALU read/write VGPR (RAW + WAW)
	6 
	Already have 2 hardware waits, so only needs 1 software wait.
	VM, LDS, FLAT and Export Read VGPR overlapped with 1st vDst
	9 
	No internal forwarding path, need to wait previous V_MFMA commit result to VGPR
	V_CMPX* write EXEC MASK 
	V_MFMA* 
	4 
	Doesn’t support execution mask forwarding with XDL/DGEMM
	



7.6. Dependency Resolution: Required Independent Instructions 68 of 600
CDNA4 Instruction Set Architecture 
First Instruction 
	Second Instruction 
	Required Waits
	Comments
	XDL/SMFMA Read VGPR SrcC 
	VALU write VGPR (WAR), Co-Execution Anti 
Dependency for over lapping with 1st SrcC
	1 
	XDL and VALU could access arch VGPR, need to resolve WAR, XDL read at S11, VALU write at S11, so needs 1 wait for this case
	3
	7
	15
	



7.6. Dependency Resolution: Required Independent Instructions 69 of 600
CDNA4 Instruction Set Architecture Chapter 8. Scalar Memory Operations 
Scalar Memory Read (SMEM) instructions allow a shader program to load data from memory into SGPRs through the Scalar Data Cache, or write data from SGPRs to memory through the Scalar Data Cache. Instructions can read from 1 to 16 Dwords, or write 1 to 4 Dwords at a time. Data is read directly into SGPRs without any format conversion. 
The scalar unit reads and writes consecutive Dwords between memory and the SGPRs. This is intended primarily for loading ALU constants. No data formatting is supported, nor is byte or short data. 
8.1. Microcode Encoding 
Scalar memory read, write and atomic instructions are encoded using the SMEM microcode format. The fields are described in the table below: 
Table 39. SMEM Encoding Field Descriptions 
Field 
OP 
	Size 8 
	Description 
Opcode.
	IMM 
	1 
	Determines how the OFFSET field is interpreted. 
IMM=1 : Offset is a 21-bit unsigned byte offset to the address. 
IMM=0 : Offset[6:0] specifies an SGPR or M0 which provides an unsigned byte offset (for stores, must be M0). STORE and ATOMIC instructions cannot use an SGPR: only imm or M0.
	GLC 
	1 
	Globally Coherent. 
For loads, controls L1 cache policy: 0=hit_lru, 1=miss_evict. 
For stores, controls L1 cache bypass: 0=write-combine, 1=write-thru. 
For atomics, "1" indicates that the atomic returns the pre-op value.
	SDATA 
	7 
	SGPRs to return read data to, or to source write-data from. 
Reads of two Dwords must have an even SDST-sgpr. 
Reads of four or more Dwords must have their DST-gpr aligned to a multiple of 4. 
SDATA must be: SGPR or VCC. Not: exec or m0.
	SBASE 
	6 
	SGPR-pair (SBASE has an implied LSB of zero) which provides a base address, or for BUFFER instructions, a set of 4 SGPRs (4-sgpr aligned) which hold the resource constant. For BUFFER instructions, the only resource fields used are: base, stride, num_records.
	OFFSET 21 
	

	An unsigned byte offset, or the address of an SGPR holding the offset. Writes and atomics: M0 or immediate only, not SGPR.
	NV 
	1 
	Non-volatile.
	SOE 
	1 
	Scalar Offset Enable.
	



See Memory Scope and Temporal Control for more information on the GLC bit. 
8.1. Microcode Encoding 70 of 600
CDNA4 Instruction Set Architecture 8.2. Operations 
8.2.1. S_LOAD_DWORD, S_STORE_DWORD 
These instructions load 1-16 Dwords or store 1-4 Dwords between SGPRs and memory. The data in SGPRs is specified in SDATA, and the address is composed of the SBASE, OFFSET, and SOFFSET fields. 
8.2.1.1. Scalar Memory Addressing 
S_LOAD / S_STORE / S_DCACHE_DISCARD: 
ADDR = SGPR[base] + inst_offset + { M0 or SGPR[offset] or zero } 
S_SCRATCH_LOAD / S_SCRATCH_STORE: 
ADDR = SGPR[base] + inst_offset + { M0 or SGPR[offset] or zero } * 64 
Use of offset fields: 
0 
	IMM SOFFSET_EN (SOE) 0 
	Address 
SGPR[base] + (SGPR[offset] or M0)
	0 
	1 
	SGPR[base] + (SGPR[soffset] or M0)
	1 
	0 
	SGPR[base] + inst_offset
	1 
	1 
	SGPR[base] + inst_offset + (SGPR[soffset] or M0)
	



All components of the address (base, offset, inst_offset, M0) are in bytes, but the two LSBs are ignored and treated as if they were zero. S_DCACHE_DISCARD ignores the six LSBs to make the address 64-byte-aligned. 
It is illegal and undefined if the inst_offset is negative and the resulting 
(inst_offset + (M0 or SGPR[offset])) is negative. 
Scalar access to private space must either use a buffer constant or manually convert the address: Addr = Addr - private_base + private_base_addr + scratch_baseOffset_for_this_wave 
"Hidden private base" is not available to the shader through hardware: It must be preloaded into an SGPR or made available through a constant buffer. This is equivalent to what the driver must do to calculate the base address from scratch for buffer constants. 
A scalar instruction must not overwrite its own source registers because the possibility of the instruction being replayed due to an ATC XNACK. Similarly, instructions in scalar memory clauses must not overwrite the sources of any of the instructions in the clause. A clause is defined as a string of memory instructions of the same type. A clause is broken by any non-memory instruction. One exception to this rule is a single SMEM instruction in a clause by itself which loads a single DWORD may legally overwrite its own source SGPRs. (This 
8.2. Operations 71 of 600
CDNA4 Instruction Set Architecture 
instruction either completely succeeds to execute and continue, or completely fail; it does not overwrite just part of one DWORD). 
Atomics are unusual because they are naturally aligned and they must be in a single-instruction clause. By definition, an atomic that returns the pre-op value overwrites its data source, which is acceptable. 
Reads/Writes/Atomics using Buffer Constant 
Buffer constant fields used: base_address, stride, num_records, NV. Other fields are ignored. 
Scalar memory read/write does not support "swizzled" buffers. Stride is used only for memory address bounds checking, not for computing the address to access. 
The SMEM supplies only a SBASE address (byte) and an offset (byte or Dword). Any "index * stride" must be calculated manually in shader code and added to the offset prior to the SMEM. 
The two LSBs of V#.base and of the final address are ignored to force Dword alignment. 
"m_*" components come from the buffer constant (V#): 
  offset = IMM ? OFFSET : SGPR[OFFSET] 
  m_base = { SGPR[SBASE * 2 +1][15:0], SGPR[SBASE] } 
  m_stride = SGPR[SBASE * 2 +1][31:16] 
  m_num_records = SGPR[SBASE * 2 + 2] 
  m_size = (m_stride == 0) ? 1 : m_num_records 
  m_addr = (SGPR[SBASE * 2] + offset) & ~0x3 
  SGPR[SDST] = read_Dword_from_dcache(m_base, offset, m_size) 
  If more than 1 dword is being read, it is returned to SDST+1, SDST+2, etc, 
  and the offset is incremented by 4 bytes per DWORD. 
8.2.2. Scalar Atomic Operations 
The scalar memory unit supports the same set of memory atomics as the vector memory unit. Addressing is the same as for scalar memory loads and stores. Like the vector memory atomics, scalar atomic operations can return the "pre-operation value" to the SDATA SGPRs. This is enabled by setting the microcode GLC bit to 1. 
8.2.3. S_DCACHE_INV, S_DCACHE_WB 
This instruction invalidates, or does a "write back" of dirty data, for the entire scalar data cache. It does not return anything to SDST. 
8.2.4. S_MEMTIME 
This instruction reads a 64-bit clock counter into a pair of SGPRs: SDST and SDST+1. 8.2. Operations 72 of 600