# FP8 Flash Attention - Performance Benchmarks

## Benchmark Command

```bash
# BF16 baseline (reference)
python /sgl-workspace/sglang/benchmark/kernels/bench_mi350_fmha_asm.py --seq-len 4096

# FP8 kernel (when K-loop implemented)
cd /sgl-workspace/aiter/hsa/gfx950/fmha_v3_fwd_fp8
python bench_fp8_attention.py
```

---

## Target Performance

| Seq Len | BF16 TF/s | FP8 Target | Speedup |
|---------|-----------|------------|---------|
| 1024 | ~400 | >520 | >30% |
| 4096 | ~780 | >1000 | >30% |
| 32130 | ~1000 | >1300 | >30% |

---

## Current Status

| Metric | Value |
|--------|-------|
| Shape | 32×128 (single tile) |
| Correctness | ✅ All tests pass |
| K-loop | ⬜ Not implemented |
| Benchmark | ⬜ Pending K-loop |

---

## Compute Analysis

### FP8 vs BF16 MFMA Throughput

| Instruction | Throughput (ops/cycle) |
|-------------|------------------------|
| v_mfma_f32_32x32x16_bf16 | 512 |
| v_mfma_f32_32x32x16_fp8_fp8 | 1024 |

FP8 has 2× MFMA throughput → theoretical 2× speedup.
Practical target: 30-50% due to memory/LDS overhead.

### Memory Bandwidth

| Data | Size/tile | Access |
|------|-----------|--------|
| Q | 32×128×1B = 4KB | Once, reused across K-tiles |
| K | 32×128×1B = 4KB | Once per K-tile |
| V | 32×128×1B = 4KB | Once per K-tile |
| O | 32×128×4B = 16KB | Write once |

---

## Next Steps

1. Implement K-tile loop (seq > 32)
2. Run `bench_mi350_fmha_asm.py` with FP8 kernel
3. Profile with rocprof for bottleneck analysis
