# FP8 FMHA Benchmark Results

## QK MFMA Performance (pipelined, pitch-136)

| Shape | Blocks | Time | TF/s | vs BF16 QK |
|-------|--------|------|------|------------|
| H=40, Seq=4096 | 5120 | 0.70 ms | 244 | 84% |
| H=40, Seq=8192 | 10240 | 2.37 ms | 290 | 100% |

## BF16 Reference (full attention)

| Shape | Time | TF/s |
|-------|------|------|
| H=40, Seq=4096 | 0.40 ms | 868 |
| H=40, Seq=32130 | 20.8 ms | 1016 |

## Analysis

- QK is ~1/3 of full attention FLOPS
- FP8 QK matches BF16 QK throughput at seq=8192
- To beat BF16: need >400 TF/s for QK (theoretical 2x with FP8)
- Current bottleneck: not using buffer_load...lds (direct LDS load)

## Next Steps

1. Add softmax to QK kernel
2. Add PV MFMA (complete attention)
3. Optimize with buffer_load...lds for higher throughput
4. Target: >1300 TF/s full attention (30% faster than BF16)
