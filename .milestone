# FP8 Flash Attention - Milestones

## Goal
FP8 kernel with >30% TF/s improvement over BF16 (~1000 → >1300 TF/s)

---

## Status: Phase 2 Complete ✅

### Completed

| Phase | Description | Status |
|-------|-------------|--------|
| 1 | HD=32 foundation | ✅ |
| 2 | HD=128 correctness | ✅ |

### Phase 2 Details
- S^T = K @ Q^T computation
- Row-wise softmax (VGPR sum + permlane32_swap)
- P transpose store
- 7 rigorous tests passing

---

## Next: Phase 3 (K-loop)

| Task | Status |
|------|--------|
| Kernel arg for seq_len | ⬜ |
| K-tile loop structure | ⬜ |
| Online softmax (running max/sum) | ⬜ |
| Output rescaling between tiles | ⬜ |

---

## Phase 4 (Optimization)

| Task | Status |
|------|--------|
| LDS bank conflict reduction | ⬜ |
| Instruction scheduling | ⬜ |
| Performance benchmarking | ⬜ |

---

## Files

```
hsa/gfx950/fmha_v3_fwd_fp8/
├── test_full_hd128.s       # Production kernel
├── test_rigorous.py        # Main test suite
├── test_softmax_check.py   # Quick check
├── test_rowwise_softmax.py # Row-wise verify
└── test_transpose_theory.py # S^T proof
```
