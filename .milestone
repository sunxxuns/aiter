# FP8 Flash Attention - Milestones

## Goal
FP8 kernel with >30% TF/s improvement over BF16 (~1000 → >1300 TF/s)

---

## Current Status: Phase 2 Complete ✅

### Phase 2: HD=128 Single-tile Correctness ✅
- S^T = K @ Q^T computation (matches BF16 reference)
- Row-wise softmax via VGPR sum + permlane32_swap
- P transpose store for correct PV input
- All 7 rigorous tests passing
- Shape: 32 queries × 32 keys × 128 head_dim

### Phase 3: K-tile Loop (TODO)

| Task | Status |
|------|--------|
| Study BF16 buffer_load patterns | ⬜ |
| Add seq_len kernel argument | ⬜ |
| Implement K-tile loop | ⬜ |
| Online softmax (running max/sum) | ⬜ |
| Output rescaling between tiles | ⬜ |
| Test seq_len > 32 | ⬜ |

---

## Test Command

```bash
cd /sgl-workspace/aiter/hsa/gfx950/fmha_v3_fwd_fp8
python test_rigorous.py
```

---

## Files

```
hsa/gfx950/fmha_v3_fwd_fp8/
├── test_full_hd128.s       # Production kernel (HD=128, SEQ=32)
├── test_rigorous.py        # Main test suite (7 tests)
├── test_softmax_check.py   # Quick sanity check
├── test_rowwise_softmax.py # Row-wise verification
├── test_transpose_theory.py # S^T math proof
```

---

## Performance Targets

| Seq Len | BF16 TF/s | FP8 Target | Status |
|---------|-----------|------------|--------|
| 32 | N/A | Correctness | ✅ |
| 1024 | ~400 | >520 | Phase 3 |
| 32130 | ~1000 | >1300 | Phase 3 |
